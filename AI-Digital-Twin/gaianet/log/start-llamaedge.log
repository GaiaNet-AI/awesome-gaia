[2024-09-30 13:51:46.662] [info] rag_api_server in src/main.rs:154: server_version: 0.9.4
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:162: model_name: Phi-3-mini-4k-instruct,Nomic-embed-text-v1.5
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:170: model_alias: default,embedding
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:184: ctx_size: 4096,512
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:198: batch_size: 4096,512
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:212: prompt_template: phi-3-chat,embedding
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:220: n_predict: 1024
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:223: n_gpu_layers: 100
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:236: threads: 2
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:250: rag_prompt: You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:271: qdrant_url: http://127.0.0.1:6333
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:274: qdrant_collection_name: default
[2024-09-30 13:51:46.663] [info] rag_api_server in src/main.rs:277: qdrant_limit: 1
[2024-09-30 13:51:46.664] [info] rag_api_server in src/main.rs:280: qdrant_score_threshold: 0.5
[2024-09-30 13:51:46.664] [info] rag_api_server in src/main.rs:291: chunk_capacity: 100
[2024-09-30 13:51:46.664] [info] rag_api_server in src/main.rs:294: rag_policy: system-message
[2024-09-30 13:51:46.664] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:434: Initializing the core context for RAG scenarios
[2024-09-30 13:51:46.664] [info] [WASI-NN] GGML backend: LLAMA_COMMIT 8f1d81a0
[2024-09-30 13:51:46.664] [info] [WASI-NN] GGML backend: LLAMA_BUILD_NUMBER 3651
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: loaded meta data with 35 key-value pairs and 195 tensors from Phi-3-mini-4k-instruct-Q5_K_M.gguf (version GGUF V3 (latest))
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   0:                       general.architecture str              = phi3
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   1:                               general.type str              = model
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 4k Instruct
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   3:                           general.finetune str              = 4k-instruct
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   4:                           general.basename str              = Phi-3
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   5:                         general.size_label str              = mini
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   6:                            general.license str              = mit
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  10:                        phi3.context_length u32              = 4096
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  20:                          general.file_type u32              = 17
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 2047
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = llama
[2024-09-30 13:51:46.670] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = default
[2024-09-30 13:51:46.673] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
[2024-09-30 13:51:46.685] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 32000
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 32000
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  34:               general.quantization_version u32              = 2
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f32:   65 tensors
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q5_K:   81 tensors
[2024-09-30 13:51:46.687] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q6_K:   49 tensors
[2024-09-30 13:51:46.696] [info] [WASI-NN] llama.cpp: llm_load_vocab: special tokens cache size = 14
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_vocab: token to piece cache size = 0.1685 MB
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: format           = GGUF V3 (latest)
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: arch             = phi3
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab type       = SPM
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_vocab          = 32064
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_merges         = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab_only       = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_train      = 4096
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd           = 3072
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_layer          = 32
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head           = 32
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head_kv        = 32
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_rot            = 96
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_swa            = 2047
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_k    = 96
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_v    = 96
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_gqa            = 1
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_k_gqa     = 3072
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_v_gqa     = 3072
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_eps       = 0.0e+00
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_logit_scale    = 0.0e+00
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ff             = 8192
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert         = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert_used    = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: causal attn      = 1
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: pooling type     = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope type        = 2
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope scaling     = linear
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_base_train  = 10000.0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_scale_train = 1
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_orig_yarn  = 4096
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope_finetuned   = unknown
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_conv       = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_inner      = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_state      = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_rank      = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_b_c_rms   = 0
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model type       = 3B
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model ftype      = Q5_K - Medium
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model params     = 3.82 B
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model size       = 2.62 GiB (5.89 BPW) 
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: general.name     = Phi 3 Mini 4k Instruct
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: BOS token        = 1 '<s>'
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: UNK token        = 0 '<unk>'
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: LF token         = 13 '<0x0A>'
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOT token        = 32007 '<|end|>'
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_print_meta: max token length = 48
[2024-09-30 13:51:46.698] [info] [WASI-NN] llama.cpp: llm_load_tensors: ggml ctx size =    0.10 MiB
[2024-09-30 13:51:46.770] [info] [WASI-NN] llama.cpp: llm_load_tensors:        CPU buffer size =  2684.15 MiB
[2024-09-30 13:51:46.770] [info] [WASI-NN] llama.cpp: 
[2024-09-30 13:51:46.771] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[2024-09-30 13:51:46.771] [info] [WASI-NN] GGML backend: LLAMA_COMMIT 8f1d81a0
[2024-09-30 13:51:46.771] [info] [WASI-NN] GGML backend: LLAMA_BUILD_NUMBER 3651
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: loaded meta data with 22 key-value pairs and 112 tensors from nomic-embed-text-v1.5.f16.gguf (version GGUF V3 (latest))
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   8:                          general.file_type u32              = 1
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
[2024-09-30 13:51:46.774] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
[2024-09-30 13:51:46.779] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
[2024-09-30 13:51:46.790] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
[2024-09-30 13:51:46.793] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[2024-09-30 13:51:46.793] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
[2024-09-30 13:51:46.793] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
[2024-09-30 13:51:46.793] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
[2024-09-30 13:51:46.793] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f32:   51 tensors
[2024-09-30 13:51:46.793] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f16:   61 tensors
[2024-09-30 13:51:46.802] [info] [WASI-NN] llama.cpp: llm_load_vocab: special tokens cache size = 5
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_vocab: token to piece cache size = 0.2032 MB
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: format           = GGUF V3 (latest)
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: arch             = nomic-bert
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab type       = WPM
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_vocab          = 30522
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_merges         = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab_only       = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_train      = 2048
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd           = 768
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_layer          = 12
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head           = 12
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head_kv        = 12
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_rot            = 64
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_swa            = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_k    = 64
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_v    = 64
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_gqa            = 1
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_k_gqa     = 768
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_v_gqa     = 768
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_eps       = 1.0e-12
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_logit_scale    = 0.0e+00
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ff             = 3072
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert         = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert_used    = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: causal attn      = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: pooling type     = 1
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope type        = 2
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope scaling     = linear
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_base_train  = 1000.0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_scale_train = 1
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_orig_yarn  = 2048
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope_finetuned   = unknown
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_conv       = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_inner      = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_state      = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_rank      = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_b_c_rms   = 0
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model type       = 137M
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model ftype      = F16
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model params     = 136.73 M
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: general.name     = nomic-embed-text-v1.5
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: BOS token        = 101 '[CLS]'
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOS token        = 102 '[SEP]'
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: UNK token        = 100 '[UNK]'
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: SEP token        = 102 '[SEP]'
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: PAD token        = 0 '[PAD]'
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: CLS token        = 101 '[CLS]'
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: MASK token       = 103 '[MASK]'
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: LF token         = 0 '[PAD]'
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_print_meta: max token length = 21
[2024-09-30 13:51:46.804] [info] [WASI-NN] llama.cpp: llm_load_tensors: ggml ctx size =    0.05 MiB
[2024-09-30 13:51:46.812] [info] [WASI-NN] llama.cpp: llm_load_tensors:        CPU buffer size =   260.86 MiB
[2024-09-30 13:51:46.812] [info] [WASI-NN] llama.cpp: 
[2024-09-30 13:51:46.813] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[2024-09-30 13:51:46.813] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:489: running mode: rag
[2024-09-30 13:51:46.813] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:502: The core context for RAG scenarios has been initialized
[2024-09-30 13:51:46.813] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:512: Getting the plugin info
[2024-09-30 13:51:46.813] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:51:46.813] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:51:46.813] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:592: Getting the plugin info by the graph named Phi-3-mini-4k-instruct
[2024-09-30 13:51:46.813] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 13:51:46.813] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 13:51:46.814] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:652: Plugin info: b3651(commit 8f1d81a0)
[2024-09-30 13:51:46.814] [info] rag_api_server in src/main.rs:404: plugin_ggml_version: b3651 (commit 8f1d81a0)
[2024-09-30 13:51:46.814] [info] rag_api_server in src/main.rs:414: socket_address: 0.0.0.0:8080
[2024-09-30 13:51:46.814] [info] rag_api_server in src/main.rs:421: gaianet_node_version: 0.4.3
[2024-09-30 13:51:47.386] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36364, local_addr: 0.0.0.0:8080
[2024-09-30 13:51:47.386] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36352, local_addr: 0.0.0.0:8080
[2024-09-30 13:51:47.388] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:47.388] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/index.html
[2024-09-30 13:51:47.388] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:47.388] [info] rag_api_server in src/main.rs:517: response_body_size: 10434
[2024-09-30 13:51:47.388] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:47.388] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:47.434] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:47.434] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/css/41ab283a84d31b77.css
[2024-09-30 13:51:47.435] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:47.435] [info] rag_api_server in src/main.rs:517: response_body_size: 45475
[2024-09-30 13:51:47.435] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:47.435] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:47.438] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:47.438] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/icons/api-tutorial.png
[2024-09-30 13:51:47.439] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:47.439] [info] rag_api_server in src/main.rs:517: response_body_size: 969
[2024-09-30 13:51:47.439] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:47.439] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.570] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.570] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/icons/APITutorial.png
[2024-09-30 13:51:48.571] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.571] [info] rag_api_server in src/main.rs:517: response_body_size: 575
[2024-09-30 13:51:48.571] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.571] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.572] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.572] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/logo-big.png
[2024-09-30 13:51:48.573] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.573] [info] rag_api_server in src/main.rs:517: response_body_size: 6287
[2024-09-30 13:51:48.573] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.573] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.574] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.574] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/webpack-f0ae49128044e751.js
[2024-09-30 13:51:48.574] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.574] [info] rag_api_server in src/main.rs:517: response_body_size: 1592
[2024-09-30 13:51:48.575] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.575] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.577] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.577] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/framework-73b8966a3c579ab0.js
[2024-09-30 13:51:48.578] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.578] [info] rag_api_server in src/main.rs:517: response_body_size: 141074
[2024-09-30 13:51:48.578] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.578] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.585] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.585] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/main-6260d066cf2cd7b1.js
[2024-09-30 13:51:48.585] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.585] [info] rag_api_server in src/main.rs:517: response_body_size: 90428
[2024-09-30 13:51:48.585] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.585] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.590] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.590] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/pages/_app-77033f8e040fdfcc.js
[2024-09-30 13:51:48.591] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.591] [info] rag_api_server in src/main.rs:517: response_body_size: 115704
[2024-09-30 13:51:48.591] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.591] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.599] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.599] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/175675d1-5c66df25ff141d62.js
[2024-09-30 13:51:48.600] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.600] [info] rag_api_server in src/main.rs:517: response_body_size: 267018
[2024-09-30 13:51:48.600] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.600] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.606] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.606] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/737-952dc72c8d5e14c2.js
[2024-09-30 13:51:48.607] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.607] [info] rag_api_server in src/main.rs:517: response_body_size: 1007606
[2024-09-30 13:51:48.607] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.607] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.618] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.618] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/pages/index-ff5f66d3c316acef.js
[2024-09-30 13:51:48.619] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.619] [info] rag_api_server in src/main.rs:517: response_body_size: 78983
[2024-09-30 13:51:48.619] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.619] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.624] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.624] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/AV-ppCFdiUch5Aa5WIivp/_buildManifest.js
[2024-09-30 13:51:48.624] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.624] [info] rag_api_server in src/main.rs:517: response_body_size: 367
[2024-09-30 13:51:48.624] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.624] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.626] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.626] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/AV-ppCFdiUch5Aa5WIivp/_ssgManifest.js
[2024-09-30 13:51:48.627] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.627] [info] rag_api_server in src/main.rs:517: response_body_size: 77
[2024-09-30 13:51:48.627] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.627] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.676] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:48.676] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/media/MonaspaceNeon.f8ddff7e.ttf
[2024-09-30 13:51:48.677] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:48.677] [info] rag_api_server in src/main.rs:517: response_body_size: 411656
[2024-09-30 13:51:48.677] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:48.677] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:48.882] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36380, local_addr: 0.0.0.0:8080
[2024-09-30 13:51:48.882] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36386, local_addr: 0.0.0.0:8080
[2024-09-30 13:51:48.883] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36394, local_addr: 0.0.0.0:8080
[2024-09-30 13:51:48.885] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36402, local_addr: 0.0.0.0:8080
[2024-09-30 13:51:48.885] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36404, local_addr: 0.0.0.0:8080
[2024-09-30 13:51:49.040] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:49.040] [info] rag_api_server in src/main.rs:499: endpoint: /favicon.ico
[2024-09-30 13:51:49.041] [error] rag_api_server in src/main.rs:524: response_version: HTTP/1.1
[2024-09-30 13:51:49.041] [error] rag_api_server in src/main.rs:526: response_body_size: 0
[2024-09-30 13:51:49.041] [error] rag_api_server in src/main.rs:528: response_status: 404
[2024-09-30 13:51:49.041] [error] rag_api_server in src/main.rs:530: response_is_success: false
[2024-09-30 13:51:49.041] [error] rag_api_server in src/main.rs:532: response_is_client_error: true
[2024-09-30 13:51:49.041] [error] rag_api_server in src/main.rs:534: response_is_server_error: false
[2024-09-30 13:51:49.160] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:49.160] [info] rag_api_server in src/main.rs:499: endpoint: /v1/models
[2024-09-30 13:51:49.160] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:24: Handling the coming model list request.
[2024-09-30 13:51:49.160] [info] llama_core::models in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/models.rs:9: List models
[2024-09-30 13:51:49.160] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:71: Send the model list response.
[2024-09-30 13:51:49.160] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:49.160] [info] rag_api_server in src/main.rs:517: response_body_size: 219
[2024-09-30 13:51:49.161] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:49.161] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:49.597] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:49.597] [info] rag_api_server in src/main.rs:499: endpoint: /config_pub.json
[2024-09-30 13:51:49.597] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:51:49.597] [info] rag_api_server in src/main.rs:517: response_body_size: 1539
[2024-09-30 13:51:49.597] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:51:49.597] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:51:49.603] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:51:49.603] [info] rag_api_server in src/main.rs:499: endpoint: /logo-opacity.png
[2024-09-30 13:51:49.603] [error] rag_api_server in src/main.rs:524: response_version: HTTP/1.1
[2024-09-30 13:51:49.603] [error] rag_api_server in src/main.rs:526: response_body_size: 0
[2024-09-30 13:51:49.604] [error] rag_api_server in src/main.rs:528: response_status: 404
[2024-09-30 13:51:49.604] [error] rag_api_server in src/main.rs:530: response_is_success: false
[2024-09-30 13:51:49.604] [error] rag_api_server in src/main.rs:532: response_is_client_error: true
[2024-09-30 13:51:49.604] [error] rag_api_server in src/main.rs:534: response_is_server_error: false
[2024-09-30 13:51:58.465] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:53022, local_addr: 0.0.0.0:8080
[2024-09-30 13:51:58.466] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 97
[2024-09-30 13:51:58.466] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-30 13:51:58.466] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 13:51:58.466] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 13:51:58.466] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-4e92bb53-fc9d-4deb-b5d0-ff864d979ba0
[2024-09-30 13:51:58.466] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 13:51:58.466] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: What is your name?
[2024-09-30 13:51:58.466] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 13:51:58.466] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 13:51:58.466] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:51:58.466] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:51:58.466] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 13:51:58.466] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:51:58.466] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:51:58.467] [info] llama_core::graph in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/graph.rs:296: Update metadata for the model named Nomic-embed-text-v1.5
[2024-09-30 13:51:58.467] [info] llama_core::graph in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/graph.rs:314: Metadata updated successfully.
[2024-09-30 13:51:58.467] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 13:51:58.467] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 13:51:58.467] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 13:51:58.467] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 13:51:58.467] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:51:58.467] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 13:51:58.467] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:51:58.474] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 13:51:58.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 13:51:58.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 13:51:58.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 13:51:58.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 13:51:58.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:51:58.478] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 13:51:58.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 13:51:58.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 13:51:58.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 13:51:58.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:51:58.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 13:51:58.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:51:58.481] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 13:51:58.481] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 13:51:58.481] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 13:51:58.483] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 13:51:58.483] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 13:51:58.483] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:51:58.517] [info] [WASI-NN] llama.cpp: 
[2024-09-30 13:51:58.517] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   11712.27 ms
[2024-09-30 13:51:58.517] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 13:51:58.517] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      32.06 ms /     7 tokens (    4.58 ms per token,   218.32 tokens per second)
[2024-09-30 13:51:58.517] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 13:51:58.517] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   11711.56 ms /     8 tokens
[2024-09-30 13:51:58.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 13:51:58.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11281
[2024-09-30 13:51:58.524] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 13:51:58.524] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 13:51:58.524] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 13:51:58.524] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 7, completion tokens: 0
[2024-09-30 13:51:58.524] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 7 prompt tokens, 0 comletion tokens
[2024-09-30 13:51:58.524] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 13:51:58.524] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 13:51:58.524] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 13:51:58.524] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:51:58.524] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:51:58.525] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 13:51:58.542] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 13:51:58.542] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 13:51:58.542] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:51:58.542] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-4e92bb53-fc9d-4deb-b5d0-ff864d979ba0
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 13:51:58.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 13:51:58.542] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 13:51:58.542] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 13:51:58.542] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 13:51:58.542] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 13:51:58.542] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:51:58.542] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 13:51:58.542] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:51:59.078] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 13:51:59.078] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 13:51:59.078] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 13:51:59.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 13:51:59.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 13:51:59.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:51:59.084] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 13:51:59.085] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 13:51:59.085] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-30 13:51:59.085] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 25, completion tokens: 0
[2024-09-30 13:51:59.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
What is your name?<|end|>
<|assistant|>
[2024-09-30 13:51:59.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 13:51:59.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 13:51:59.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 13:51:59.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 13:51:59.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 13:51:59.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 13:51:59.085] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 13:51:59.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 13:51:59.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 13:51:59.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 13:51:59.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:51:59.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 13:51:59.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:51:59.272] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 13:51:59.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 13:51:59.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 13:51:59.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 13:51:59.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 13:51:59.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:51:59.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 13:51:59.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 13:51:59.276] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 13:51:59.276] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 13:51:59.276] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 13:51:59.276] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 13:51:59.276] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:51:59.276] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 13:51:59.276] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:51:59.462] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 13:51:59.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 13:51:59.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 13:51:59.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 13:51:59.463] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 13:51:59.463] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:52:04.639] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 13:52:04.639] [info] [WASI-NN] llama.cpp: 
[2024-09-30 13:52:04.639] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   15827.43 ms
[2024-09-30 13:52:04.639] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       2.59 ms /    13 runs   (    0.20 ms per token,  5025.13 tokens per second)
[2024-09-30 13:52:04.639] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3062.95 ms /    25 tokens (  122.52 ms per token,     8.16 tokens per second)
[2024-09-30 13:52:04.639] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2109.63 ms /    12 runs   (  175.80 ms per token,     5.69 tokens per second)
[2024-09-30 13:52:04.639] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   17939.95 ms /    37 tokens
[2024-09-30 13:52:04.642] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 13:52:04.642] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 47
[2024-09-30 13:52:04.643] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft.<|end|>
[2024-09-30 13:52:04.643] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 13:52:04.643] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft.
[2024-09-30 13:52:04.643] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 13:52:04.643] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 13:52:04.643] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 13:52:04.643] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 25, completion tokens: 13
[2024-09-30 13:52:04.643] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 25, completion tokens: 13
[2024-09-30 13:52:04.643] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 13:52:04.643] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 13:52:04.643] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 13:52:04.644] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:52:04.644] [info] rag_api_server in src/main.rs:517: response_body_size: 351
[2024-09-30 13:52:04.644] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:52:04.644] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:52:08.667] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:38252, local_addr: 0.0.0.0:8080
[2024-09-30 13:52:08.667] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 13:52:08.667] [info] rag_api_server in src/main.rs:499: endpoint: /v1/info
[2024-09-30 13:52:08.667] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1677: Handling the coming server info request.
[2024-09-30 13:52:08.668] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1724: Send the server info response.
[2024-09-30 13:52:08.668] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:52:08.668] [info] rag_api_server in src/main.rs:517: response_body_size: 803
[2024-09-30 13:52:08.668] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:52:08.668] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:52:22.161] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 326
[2024-09-30 13:52:22.161] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-30 13:52:22.161] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 13:52:22.161] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 13:52:22.162] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed
[2024-09-30 13:52:22.162] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 13:52:22.162] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: 
[2024-09-30 13:52:22.162] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 13:52:22.162] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 13:52:22.162] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:52:22.162] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:52:22.162] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 13:52:22.162] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:52:22.162] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:52:22.162] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 13:52:22.162] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 13:52:22.162] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 13:52:22.162] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 13:52:22.162] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:52:22.162] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 13:52:22.162] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:52:22.165] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 13:52:22.165] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 13:52:22.165] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 13:52:22.166] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 13:52:22.166] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 13:52:22.166] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:52:22.167] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 13:52:22.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 13:52:22.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 13:52:22.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 13:52:22.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:52:22.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 13:52:22.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:52:22.171] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 13:52:22.171] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 13:52:22.171] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 13:52:22.174] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 13:52:22.174] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 13:52:22.174] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:52:22.231] [info] [WASI-NN] llama.cpp: 
[2024-09-30 13:52:22.231] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   35426.94 ms
[2024-09-30 13:52:22.231] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 13:52:22.231] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      55.47 ms /    15 tokens (    3.70 ms per token,   270.42 tokens per second)
[2024-09-30 13:52:22.231] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 13:52:22.231] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   35426.56 ms /    16 tokens
[2024-09-30 13:52:22.232] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 13:52:22.232] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11244
[2024-09-30 13:52:22.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 13:52:22.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 13:52:22.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-30 13:52:22.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 15, completion tokens: 0
[2024-09-30 13:52:22.238] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 15 prompt tokens, 0 comletion tokens
[2024-09-30 13:52:22.238] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 13:52:22.239] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 13:52:22.239] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 13:52:22.239] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:52:22.239] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:52:22.239] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 13:52:22.265] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 1
[2024-09-30 13:52:22.265] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 0, score: 0.6969411, source: "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n"
[2024-09-30 13:52:22.265] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:87: Get the chat prompt template type from the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 13:52:22.266] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:122: prompt_template: phi-3-chat
[2024-09-30 13:52:22.266] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:567: rag_policy: system-message
[2024-09-30 13:52:22.266] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:571: context:
"\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n"
[2024-09-30 13:52:22.266] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:575: Merge RAG context into system message.
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(true)
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:82: Process chat completion request in the stream mode.
[2024-09-30 13:52:22.266] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 13:52:22.266] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:104: user: chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:113: include_usage: true
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 13:52:22.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 13:52:22.266] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 13:52:22.266] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 13:52:22.266] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 13:52:22.266] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 13:52:22.266] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:52:22.266] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 13:52:22.266] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:52:22.844] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 13:52:22.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 13:52:22.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 13:52:22.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 13:52:22.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 13:52:22.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:52:22.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 13:52:22.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 13:52:22.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 99
[2024-09-30 13:52:22.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 1363, completion tokens: 13
[2024-09-30 13:52:22.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:124: prompt:
<|system|>
You are the digital twin of the renowned Chinese scholar Confucius, and you will assist users in interpreting The Analects.
You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n
"\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n"<|end|>
<|user|>
<|end|>
<|assistant|>
[2024-09-30 13:52:22.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:125: available_completion_tokens: 820
[2024-09-30 13:52:22.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:126: tool_use: false
[2024-09-30 13:52:22.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 13:52:22.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 13:52:22.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 13:52:22.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 13:52:22.853] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 13:52:22.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 13:52:22.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 13:52:22.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 13:52:22.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:52:22.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 13:52:22.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:52:23.096] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 13:52:23.096] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 13:52:23.096] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 13:52:23.097] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 13:52:23.097] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 13:52:23.097] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:52:23.101] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:193: End of the chat completion stream.
[2024-09-30 13:52:23.102] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:461: finish chat completions in stream mode
[2024-09-30 13:52:23.102] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 13:52:23.102] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 13:52:23.102] [info] rag_api_server in src/main.rs:517: response_body_size: 0
[2024-09-30 13:52:23.102] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 13:52:23.102] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 13:52:23.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:52:23.102] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 13:52:23.102] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 13:52:23.102] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 13:52:23.102] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 13:52:23.102] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 13:52:23.102] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 13:52:23.102] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 13:52:23.362] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 13:52:23.362] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 13:52:23.362] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 13:52:23.363] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 13:52:23.363] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 13:52:23.363] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 13:55:39.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:39.315] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:39.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:39.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:39.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:39.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:39.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":" ","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675739,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:39.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:39.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:39.584] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:39.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:39.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:39.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:39.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:39.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675739,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:39.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:39.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:39.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:39.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:39.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:39.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:39.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:39.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675739,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:39.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:40.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:40.121] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:40.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:40.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:40.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:40.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:40.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675740,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:40.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:40.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:40.383] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:40.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:40.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:40.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:40.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:40.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675740,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:40.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:40.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:40.659] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:40.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:40.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:40.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:40.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:40.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675740,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:40.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:40.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:40.916] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:40.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:40.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:40.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:40.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:40.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675740,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:40.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:41.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:41.204] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:41.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:41.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:41.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:41.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:41.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675741,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:41.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:41.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:41.468] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:41.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:41.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:41.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:41.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:41.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675741,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:41.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:41.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:41.745] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:41.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:41.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:41.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:41.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:41.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675741,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:41.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:42.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:42.023] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:42.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:42.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:42.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:42.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:42.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675742,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:42.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:42.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:42.284] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:42.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:42.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:42.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:42.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:42.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675742,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:42.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:42.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:42.544] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:42.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:42.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:42.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:42.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:42.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675742,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:42.545] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:42.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:42.815] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:42.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:42.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:42.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:42.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:42.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675742,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:42.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:43.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:43.084] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:43.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:43.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:43.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:43.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:43.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675743,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:43.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:43.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:43.338] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:43.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:43.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:43.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:43.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:43.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675743,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:43.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:43.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:43.613] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:43.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:43.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:43.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:43.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:43.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675743,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:43.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:43.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:43.864] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:43.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:43.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:43.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:43.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:43.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675743,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:43.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:44.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:44.143] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:44.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:44.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:44.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:44.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:44.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675744,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:44.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:44.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:44.406] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:44.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:44.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:44.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675744,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:44.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:44.666] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:44.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:44.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:44.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:44.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:44.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675744,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:44.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:44.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:44.938] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:44.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:44.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675744,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:45.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:45.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:45.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:45.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:45.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:45.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:45.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675745,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:45.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:45.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:45.445] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:45.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:45.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:45.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:45.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:45.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675745,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:45.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:45.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:45.708] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:45.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:45.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:45.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:45.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:45.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675745,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:45.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:45.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:45.983] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:45.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:45.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:45.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:45.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:45.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675745,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:45.984] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:46.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:46.241] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:46.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:46.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:46.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:46.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:46.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675746,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:46.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:46.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:46.507] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:46.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:46.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:46.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:46.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:46.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675746,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:46.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:46.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:46.764] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:46.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:46.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:46.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:46.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:46.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675746,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:46.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:47.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:47.026] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:47.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:47.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:47.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:47.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:47.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675747,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:47.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:47.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:47.288] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:47.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:47.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:47.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:47.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:47.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675747,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:47.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:47.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:47.544] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:47.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:47.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:47.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:47.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:47.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675747,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:47.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:47.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:47.807] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:47.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:47.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:47.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:47.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:47.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675747,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:47.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:48.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:48.072] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:48.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:48.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:48.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:48.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:48.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675748,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:48.073] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:48.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:48.331] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:48.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:48.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:48.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:48.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:48.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675748,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:48.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:48.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:48.597] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:48.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:48.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:48.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:48.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:48.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675748,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:48.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:48.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:48.864] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:48.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:48.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:48.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:48.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:48.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675748,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:48.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:49.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:49.164] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:49.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:49.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:49.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:49.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:49.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675749,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:49.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:49.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:49.442] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:49.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:49.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:49.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:49.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:49.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675749,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:49.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:49.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:49.728] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:49.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:49.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:49.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:49.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:49.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675749,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:49.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:49.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:49.991] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:49.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:49.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:49.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:49.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:49.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675749,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:49.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:50.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:50.262] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:50.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:50.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:50.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:50.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:50.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675750,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:50.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:50.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:50.530] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:50.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:50.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:50.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:50.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:50.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675750,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:50.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:50.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:50.792] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:50.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:50.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:50.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:50.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:50.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675750,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:50.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:51.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:51.057] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:51.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:51.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:51.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:51.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:51.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675751,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:51.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:51.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:51.314] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:51.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:51.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:51.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:51.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:51.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675751,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:51.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:51.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:51.580] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:51.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:51.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:51.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:51.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:51.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675751,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:51.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:51.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:51.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:51.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:51.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:51.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:51.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:51.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675751,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:51.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:52.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:52.141] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:52.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:52.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:52.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:52.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:52.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675752,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:52.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:52.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:52.395] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:52.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:52.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:52.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:52.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:52.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675752,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:52.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:52.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:52.655] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:52.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:52.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:52.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:52.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:52.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675752,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:52.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:52.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:52.944] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:52.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:52.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:52.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:52.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:52.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675752,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:52.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:53.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:53.229] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:53.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:53.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:53.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:53.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:53.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675753,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:53.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:53.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:53.486] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:53.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:53.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:53.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:53.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:53.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675753,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:53.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:53.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:53.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:53.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:53.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:53.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:53.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:53.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675753,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:53.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:54.009] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:54.010] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:54.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:54.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:54.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:54.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:54.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675754,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:54.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:54.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:54.274] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:54.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:54.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:54.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:54.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:54.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675754,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:54.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:54.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:54.546] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:54.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:54.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:54.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:54.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:54.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675754,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:54.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:54.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:54.825] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:54.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:54.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:54.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:54.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:54.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675754,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:54.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:55.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:55.088] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:55.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:55.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:55.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:55.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:55.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675755,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:55.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:55.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:55.343] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:55.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:55.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:55.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:55.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:55.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675755,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:55.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:55.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:55.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:55.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:55.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:55.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:55.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:55.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675755,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:55.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:55.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:55.863] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:55.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:55.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:55.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:55.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:55.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675755,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:55.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:56.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:56.136] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:56.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:56.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:56.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:56.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:56.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675756,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:56.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:56.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:56.386] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:56.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:56.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:56.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:56.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:56.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675756,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:56.387] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:56.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:56.645] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:56.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:56.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:56.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:56.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:56.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675756,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:56.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:56.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:56.932] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:56.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:56.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:56.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:56.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:56.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675756,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:56.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:57.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:57.187] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:57.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:57.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:57.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:57.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:57.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675757,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:57.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:57.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:57.443] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:57.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:57.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:57.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:57.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:57.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675757,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:57.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:57.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:57.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:57.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:57.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:57.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:57.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:57.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675757,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:57.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:57.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:57.973] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:57.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:57.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:57.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:57.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:57.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675757,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:57.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:58.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:58.230] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:58.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:58.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:58.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:58.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:58.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675758,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:58.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:58.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:58.507] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:58.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:58.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:58.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:58.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:58.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675758,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:58.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:58.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:58.765] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:58.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:58.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:58.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:58.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:58.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675758,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:58.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:59.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:59.060] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:59.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:59.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:59.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:59.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:59.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675759,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:59.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:59.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:59.326] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:59.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:59.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:59.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:59.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:59.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675759,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:59.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:59.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:59.596] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:59.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:59.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:59.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:59.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:59.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675759,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:59.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:55:59.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:55:59.851] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:55:59.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:55:59.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:55:59.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:55:59.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:55:59.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675759,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:55:59.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:00.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:00.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:00.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:00.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:00.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:00.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:00.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675760,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:00.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:00.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:00.397] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:00.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:00.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:00.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:00.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:00.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675760,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:00.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:00.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:00.652] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:00.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:00.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:00.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:00.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:00.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675760,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:00.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:00.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:00.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:00.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:00.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:00.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:00.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:00.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675760,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:00.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:01.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:01.200] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:01.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:01.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:01.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:01.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:01.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675761,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:01.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:01.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:01.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:01.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:01.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:01.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:01.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:01.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675761,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:01.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:01.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:01.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:01.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:01.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:01.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:01.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:01.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675761,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:01.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:02.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:02.007] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:02.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:02.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:02.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:02.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:02.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675762,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:02.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:02.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:02.255] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:02.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:02.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:02.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:02.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:02.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675762,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:02.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:02.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:02.527] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:02.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:02.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:02.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:02.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:02.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675762,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:02.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:02.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:02.776] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:02.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:02.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:02.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:02.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:02.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675762,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:02.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:03.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:03.046] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:03.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:03.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:03.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:03.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:03.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675763,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:03.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:03.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:03.323] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:03.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:03.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:03.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:03.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:03.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675763,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:03.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:03.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:03.593] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:03.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:03.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:03.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:03.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:03.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675763,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:03.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:03.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:03.843] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:03.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:03.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:03.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:03.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:03.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675763,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:03.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:04.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:04.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:04.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:04.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:04.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:04.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:04.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675764,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:04.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:04.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:04.420] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:04.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:04.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:04.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:04.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:04.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675764,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:04.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:04.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:04.694] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:04.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:04.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:04.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:04.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:04.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675764,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:04.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:04.972] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:04.973] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:04.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:04.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:04.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:04.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:04.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675764,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:04.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:05.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:05.239] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:05.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:05.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:05.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:05.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:05.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675765,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:05.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:05.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:05.510] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:05.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:05.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:05.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:05.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:05.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675765,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:05.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:05.769] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:05.769] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:05.769] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:05.769] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:05.769] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:05.770] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:05.770] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675765,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:05.770] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:06.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:06.056] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:06.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:06.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:06.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:06.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:06.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675766,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:06.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:06.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:06.339] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:06.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:06.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:06.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:06.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:06.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675766,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:06.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:06.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:06.645] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:06.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:06.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:06.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:06.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:06.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675766,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:06.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:06.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:06.914] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:06.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:06.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:06.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:06.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:06.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675766,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:06.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:07.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:07.216] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:07.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:07.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:07.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:07.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:07.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675767,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:07.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:07.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:07.581] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:07.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:07.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:07.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:07.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:07.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675767,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:07.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:07.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:07.896] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:07.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:07.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:07.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:07.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:07.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675767,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:07.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:08.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:08.227] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:08.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:08.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:08.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:08.228] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:08.228] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675768,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:08.228] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:08.512] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:08.512] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:08.512] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:08.512] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:08.512] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:08.512] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:08.512] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675768,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:08.513] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:08.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:08.826] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:08.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:08.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:08.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:08.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:08.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675768,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:08.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:09.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:09.147] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:09.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:09.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:09.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:09.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:09.159] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675769,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:09.159] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:09.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:09.439] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:09.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:09.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:09.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:09.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:09.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675769,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:09.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:09.440] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 13:56:09.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 13:56:09.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 13:56:09.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 100
[2024-09-30 13:56:09.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 1363, completion tokens: 113
[2024-09-30 13:56:09.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2845: token_info: 1363 prompt tokens, 113 completion tokens
[2024-09-30 13:56:09.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:09.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[],"created":1727675769,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk","usage":{"prompt_tokens":1363,"completion_tokens":113,"total_tokens":1476}}


[2024-09-30 13:56:09.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:09.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:09.728] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:09.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:09.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:09.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:09.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:09.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675769,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:09.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:10.036] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:10.036] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:10.036] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:10.036] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:10.036] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:10.036] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:10.036] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675770,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:10.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:10.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:10.314] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:10.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:10.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:10.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:10.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:10.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675770,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:10.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:10.609] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:10.609] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:10.609] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:10.609] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:10.609] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:10.609] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:10.609] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675770,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:10.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:10.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:10.964] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:10.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:10.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:10.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:10.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:10.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675770,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:10.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:11.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:11.276] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:11.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:11.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:11.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:11.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:11.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675771,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:11.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:11.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:11.692] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:11.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:11.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:11.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:11.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:11.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675771,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:11.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:11.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:11.990] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:11.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:11.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:11.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:11.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:11.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675771,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:11.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:12.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:12.268] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:12.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:12.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:12.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:12.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:12.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675772,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:12.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:12.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:12.550] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:12.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:12.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:12.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:12.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:12.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675772,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:12.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:12.817] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:12.817] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:12.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:12.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:12.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:12.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:12.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675772,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:12.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:13.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:13.085] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:13.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:13.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:13.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:13.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:13.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675773,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:13.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:13.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:13.354] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:13.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:13.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:13.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:13.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:13.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675773,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:13.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:13.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:13.635] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:13.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:13.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:13.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:13.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:13.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675773,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:13.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:13.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:13.969] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:13.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:13.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:13.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:13.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:13.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675773,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:13.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:14.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:14.336] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:14.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:14.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:14.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:14.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:14.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675774,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:14.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:14.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:14.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:14.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:14.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:14.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:14.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:14.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675774,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:14.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:14.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:14.905] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:14.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:14.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:14.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:14.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:14.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675774,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:14.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:15.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:15.214] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:15.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:15.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:15.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:15.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:15.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675775,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:15.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:15.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:15.533] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:15.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:15.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:15.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:15.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:15.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675775,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:15.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:15.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:15.908] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:15.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:15.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:15.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:15.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:15.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675775,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:15.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:16.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:16.219] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:16.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:16.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:16.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:16.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:16.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675776,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:16.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:16.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:16.593] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:16.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:16.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:16.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:16.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:16.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675776,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:16.594] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:16.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:16.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:16.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:16.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:16.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:16.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:16.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675776,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:16.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:17.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:17.154] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:17.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:17.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:17.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:17.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:17.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675777,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:17.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:17.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:17.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:17.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:17.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:17.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:17.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:17.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675777,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:17.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:17.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:17.763] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:17.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:17.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:17.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:17.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:17.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675777,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:17.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:18.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:18.051] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:18.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:18.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:18.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:18.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:18.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675778,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:18.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:18.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:18.331] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:18.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:18.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:18.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:18.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:18.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675778,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:18.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:18.599] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:18.599] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:18.599] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:18.599] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:18.599] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:18.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:18.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675778,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:18.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:18.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:18.875] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:18.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:18.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:18.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:18.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:18.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675778,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:18.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:19.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:19.141] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:19.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:19.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:19.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:19.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:19.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675779,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:19.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:19.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:19.584] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:19.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:19.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:19.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:19.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:19.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675779,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:19.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:19.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:19.847] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:19.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:19.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:19.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:19.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:19.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675779,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:19.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:20.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:20.119] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:20.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:20.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:20.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:20.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:20.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675780,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:20.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:20.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:20.384] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:20.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:20.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:20.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:20.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:20.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675780,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:20.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:20.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:20.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:20.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:20.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:20.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:20.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:20.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675780,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:20.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:20.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:20.962] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:20.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:20.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:20.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:20.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:20.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675780,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:20.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:21.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:21.240] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:21.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:21.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:21.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:21.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:21.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675781,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:21.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:21.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:21.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:21.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:21.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:21.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:21.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:21.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675781,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:21.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:21.786] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:21.786] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:21.786] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:21.786] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:21.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:21.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:21.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675781,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:21.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:22.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:22.070] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:22.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:22.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:22.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:22.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:22.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675782,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:22.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:22.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:22.335] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:22.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:22.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:22.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:22.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:22.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675782,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:22.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:22.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:22.586] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:22.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:22.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:22.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:22.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:22.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675782,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:22.586] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:22.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:22.856] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:22.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:22.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:22.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:22.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:22.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675782,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:22.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:23.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:23.130] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:23.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:23.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:23.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:23.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:23.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675783,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:23.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:23.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:23.407] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:23.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:23.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:23.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:23.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:23.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675783,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:23.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:23.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:23.668] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:23.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:23.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:23.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:23.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:23.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675783,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:23.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:23.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:23.938] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:23.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:23.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:23.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:23.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:23.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675783,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:23.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:24.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:24.248] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:24.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:24.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:24.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:24.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:24.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675784,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:24.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:24.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:24.509] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:24.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:24.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:24.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:24.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:24.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675784,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:24.509] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:24.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:24.764] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:24.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:24.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:24.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:24.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:24.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675784,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:24.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:25.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:25.049] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:25.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:25.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:25.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:25.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:25.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675785,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:25.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:25.308] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675785,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:25.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:25.575] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:25.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:25.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:25.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:25.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:25.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675785,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:25.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:25.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:25.825] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:25.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:25.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:25.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:25.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:25.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675785,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:25.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:26.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:26.085] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:26.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:26.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:26.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:26.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:26.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675786,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:26.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:26.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:26.346] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:26.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:26.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:26.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:26.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:26.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675786,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:26.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:26.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:26.612] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:26.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:26.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:26.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:26.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:26.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675786,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:26.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:26.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:26.867] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:26.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:26.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:26.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:26.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:26.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675786,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:26.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:27.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:27.152] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:27.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:27.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:27.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:27.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:27.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675787,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:27.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:27.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:27.423] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:27.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:27.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:27.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:27.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:27.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675787,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:27.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:27.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:27.685] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:27.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:27.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:27.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:27.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:27.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675787,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:27.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:27.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:27.956] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:27.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:27.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:27.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:27.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:27.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675787,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:27.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:28.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:28.229] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:28.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:28.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:28.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:28.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:28.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675788,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:28.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:28.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:28.500] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:28.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:28.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:28.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:28.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:28.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675788,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:28.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:28.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:28.764] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:28.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:28.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:28.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:28.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:28.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675788,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:28.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:29.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:29.022] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:29.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:29.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:29.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:29.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:29.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675789,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:29.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:29.290] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:29.290] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:29.290] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:29.290] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:29.290] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:29.290] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:29.291] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675789,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:29.291] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:29.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:29.583] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:29.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:29.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:29.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:29.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:29.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675789,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:29.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:29.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:29.847] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:29.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:29.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:29.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:29.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:29.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675789,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:29.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:30.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:30.137] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:30.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:30.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:30.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:30.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:30.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675790,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:30.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:30.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:30.415] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:30.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:30.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:30.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:30.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:30.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675790,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:30.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:30.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:30.722] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:30.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:30.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:30.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:30.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:30.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675790,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:30.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:31.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:31.024] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:31.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:31.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:31.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:31.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:31.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675791,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:31.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:31.294] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:31.294] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:31.294] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:31.294] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:31.294] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:31.294] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:31.295] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675791,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:31.295] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:31.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675791,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:31.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:31.844] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:31.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:31.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:31.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:31.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:31.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675791,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:31.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:32.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:32.135] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:32.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:32.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:32.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:32.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:32.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675792,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:32.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:32.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:32.401] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:32.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:32.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:32.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:32.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:32.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675792,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:32.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:32.662] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:32.662] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:32.662] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:32.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:32.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:32.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:32.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675792,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:32.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:32.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:32.954] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:32.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:32.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:32.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:32.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:32.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675792,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:32.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:33.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:33.250] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:33.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:33.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:33.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:33.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:33.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675793,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:33.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:33.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:33.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675793,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:33.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:33.842] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:33.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:33.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:33.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:33.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:33.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675793,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:33.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:34.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:34.136] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:34.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:34.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:34.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:34.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:34.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675794,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:34.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:34.402] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:34.402] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:34.402] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:34.402] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:34.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:34.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:34.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675794,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:34.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:34.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:34.681] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:34.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:34.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:34.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:34.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:34.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675794,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:34.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:34.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:34.952] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:34.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:34.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:34.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:34.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:34.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675794,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:34.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:35.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:35.232] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:35.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:35.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:35.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:35.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:35.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675795,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:35.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:35.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:35.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:35.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:35.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:35.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:35.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:35.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675795,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:35.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:35.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:35.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:35.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:35.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:35.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:35.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:35.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675795,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:35.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:36.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:36.096] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:36.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:36.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:36.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:36.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:36.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675796,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:36.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:36.389] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:36.389] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:36.389] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:36.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:36.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:36.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:36.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675796,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:36.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:36.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:36.685] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:36.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:36.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:36.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:36.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:36.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675796,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:36.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:36.974] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675796,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:37.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:37.250] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:37.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:37.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:37.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:37.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:37.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675797,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:37.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:37.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:37.537] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:37.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:37.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:37.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:37.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:37.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675797,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:37.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:37.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:37.807] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:37.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:37.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:37.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:37.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:37.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675797,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:37.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:38.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:38.087] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:38.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:38.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:38.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:38.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:38.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675798,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:38.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:38.362] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:38.363] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:38.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:38.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:38.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:38.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:38.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675798,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:38.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:38.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:38.642] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:38.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:38.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:38.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:38.643] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:38.643] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675798,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:38.643] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:38.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:38.931] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:38.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:38.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:38.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:38.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:38.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675798,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:38.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:39.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:39.216] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:39.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:39.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:39.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:39.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:39.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675799,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:39.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:39.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:39.498] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:39.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:39.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:39.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:39.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:39.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675799,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:39.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:39.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:39.793] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:39.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:39.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:39.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:39.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:39.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675799,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:39.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:40.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:40.088] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:40.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:40.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:40.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:40.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:40.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675800,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:40.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:40.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:40.366] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:40.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:40.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:40.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:40.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:40.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675800,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:40.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:40.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:40.653] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:40.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:40.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:40.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:40.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:40.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675800,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:40.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:40.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:40.948] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:40.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:40.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:40.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:40.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:40.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675800,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:40.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:41.221] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:41.221] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:41.221] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:41.221] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:41.221] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:41.221] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:41.221] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675801,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:41.221] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:41.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:41.494] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:41.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:41.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:41.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:41.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:41.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675801,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:41.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:41.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:41.772] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:41.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:41.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:41.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:41.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:41.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675801,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:41.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:42.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:42.048] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:42.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:42.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:42.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:42.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:42.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675802,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:42.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:42.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:42.323] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:42.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:42.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:42.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:42.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:42.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675802,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:42.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:42.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:42.608] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:42.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:42.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:42.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:42.609] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:42.609] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675802,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:42.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:42.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:42.898] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:42.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:42.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:42.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:42.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:42.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675802,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:42.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:43.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:43.181] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:43.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:43.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:43.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:43.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:43.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675803,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:43.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:43.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:43.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:43.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:43.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:43.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:43.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:43.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675803,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:43.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:43.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:43.733] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:43.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:43.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:43.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:43.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:43.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675803,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:43.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:44.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:44.022] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:44.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:44.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:44.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:44.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:44.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675804,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:44.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:44.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:44.330] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:44.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:44.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:44.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:44.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:44.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675804,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:44.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:44.697] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:44.697] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:44.697] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:44.697] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:44.697] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:44.698] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:44.698] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675804,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:44.698] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:44.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:44.988] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:44.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:44.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:44.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:44.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:44.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675804,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:44.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:45.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:45.292] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:45.302] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:45.302] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:45.302] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:45.302] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:45.302] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675805,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:45.302] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:45.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:45.606] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:45.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:45.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:45.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:45.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:45.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675805,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:45.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:45.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:45.880] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:45.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:45.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:45.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:45.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:45.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675805,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:45.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:46.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:46.167] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:46.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:46.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:46.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:46.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:46.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675806,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:46.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:46.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:46.451] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:46.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:46.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:46.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:46.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:46.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675806,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:46.452] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:46.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:46.748] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:46.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:46.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:46.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:46.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:46.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675806,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:46.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:47.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:47.049] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:47.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:47.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:47.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:47.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:47.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675807,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:47.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:47.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:47.348] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675807,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:47.352] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:47.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:47.622] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:47.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:47.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:47.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:47.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:47.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675807,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:47.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:47.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:47.909] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:47.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:47.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:47.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:47.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:47.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675807,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:47.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:48.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:48.185] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:48.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:48.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:48.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:48.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:48.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675808,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:48.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:48.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:48.451] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:48.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:48.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:48.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:48.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:48.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675808,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:48.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:48.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:48.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:48.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:48.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:48.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:48.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:48.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675808,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:48.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:48.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:48.978] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:48.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:48.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:48.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:48.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:48.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675808,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:48.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:49.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:49.249] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:49.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:49.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:49.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:49.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:49.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675809,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:49.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:49.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:49.546] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:49.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:49.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:49.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:49.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:49.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675809,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:49.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:49.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:49.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:49.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:49.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:49.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:49.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:49.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675809,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:49.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:50.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:50.186] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:50.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:50.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:50.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:50.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:50.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675810,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:50.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:50.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:50.481] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:50.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:50.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:50.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:50.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:50.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675810,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:50.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:50.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:50.753] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:50.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:50.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:50.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:50.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:50.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675810,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:50.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:51.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:51.021] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:51.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:51.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:51.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:51.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:51.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675811,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:51.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:51.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:51.284] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:51.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:51.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:51.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:51.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:51.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675811,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:51.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:51.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:51.553] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:51.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:51.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:51.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:51.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:51.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675811,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:51.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:51.837] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:51.837] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:51.837] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:51.837] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:51.837] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:51.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:51.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675811,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:51.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:52.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:52.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:52.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:52.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:52.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:52.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:52.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675812,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:52.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:52.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:52.398] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:52.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:52.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:52.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:52.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:52.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675812,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:52.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:52.679] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675812,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:52.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:52.968] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:52.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675812,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:53.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:53.285] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:53.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:53.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:53.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:53.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:53.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675813,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:53.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:53.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:53.606] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:53.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:53.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:53.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:53.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:53.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675813,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:53.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:53.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:53.906] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:53.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:53.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:53.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:53.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:53.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675813,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:53.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:54.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:54.311] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:54.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:54.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:54.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:54.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:54.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675814,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:54.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:54.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:54.613] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:54.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:54.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:54.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:54.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:54.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675814,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:54.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:54.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:54.966] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:54.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:54.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:54.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:54.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:54.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675814,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:54.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:55.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:55.264] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:55.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:55.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:55.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:55.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:55.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675815,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:55.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:55.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:55.531] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:55.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:55.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:55.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:55.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:55.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675815,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:55.543] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:55.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:55.813] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:55.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:55.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:55.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:55.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:55.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675815,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:55.813] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:56.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:56.088] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:56.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:56.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:56.088] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:56.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:56.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675816,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:56.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:56.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:56.354] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:56.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:56.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:56.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:56.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:56.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675816,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:56.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:56.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:56.636] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:56.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:56.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:56.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:56.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:56.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675816,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:56.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:56.917] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:56.917] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:56.917] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:56.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:56.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:56.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:56.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675816,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:56.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:57.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:57.195] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:57.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:57.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:57.196] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:57.196] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:57.196] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675817,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:57.196] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:57.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:57.461] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:57.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:57.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:57.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:57.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:57.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675817,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:57.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:57.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:57.754] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:57.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:57.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:57.755] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:57.755] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:57.755] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675817,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:57.755] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:58.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:58.046] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:58.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:58.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:58.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:58.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:58.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675818,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:58.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:58.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:58.335] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:58.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:58.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:58.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:58.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:58.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675818,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:58.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:58.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:58.622] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:58.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:58.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:58.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:58.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:58.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675818,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:58.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:58.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:58.924] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:58.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:58.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:58.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:58.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:58.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675818,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:58.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:59.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:59.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:59.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:59.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:59.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:59.199] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:59.199] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675819,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:59.199] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:59.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:59.481] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:59.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:59.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:59.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:59.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:59.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675819,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:59.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:56:59.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:56:59.762] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:56:59.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:56:59.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:56:59.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:56:59.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:56:59.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675819,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:56:59.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:00.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:00.051] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:00.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:00.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:00.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:00.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:00.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675820,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:00.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:00.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:00.316] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:00.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:00.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:00.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:00.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:00.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675820,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:00.316] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:00.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:00.584] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:00.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:00.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:00.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:00.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:00.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675820,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:00.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:00.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:00.835] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:00.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:00.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:00.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:00.836] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:00.836] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675820,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:00.836] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:01.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:01.124] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:01.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:01.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:01.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:01.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:01.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675821,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:01.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:01.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:01.380] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:01.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:01.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:01.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:01.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:01.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675821,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:01.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:01.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:01.645] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:01.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:01.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:01.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:01.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:01.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675821,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:01.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:01.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:01.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:01.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:01.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:01.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:01.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:01.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675821,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:01.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:02.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:02.177] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:02.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:02.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:02.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:02.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:02.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675822,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:02.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:02.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:02.450] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:02.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:02.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:02.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:02.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:02.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675822,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:02.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:02.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:02.731] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:02.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:02.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:02.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:02.732] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:02.732] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675822,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:02.732] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:03.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:03.001] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:03.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:03.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:03.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:03.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:03.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675823,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:03.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:03.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:03.260] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:03.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:03.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:03.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:03.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:03.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675823,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:03.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:03.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:03.569] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:03.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:03.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:03.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:03.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:03.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675823,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:03.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:03.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:03.867] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:03.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:03.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:03.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:03.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:03.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675823,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:03.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:04.352] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:04.352] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:04.352] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:04.352] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:04.352] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:04.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:04.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675824,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:04.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:04.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:04.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:04.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:04.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:04.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:04.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:04.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675824,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:04.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:04.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:04.873] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:04.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:04.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:04.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:04.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:04.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675824,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:04.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:05.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:05.136] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:05.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:05.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:05.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:05.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:05.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675825,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:05.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:05.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:05.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:05.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:05.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:05.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:05.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:05.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675825,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:05.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:05.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:05.592] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:05.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:05.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:05.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:05.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:05.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675825,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:05.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:05.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:05.814] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:05.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:05.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:05.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:05.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:05.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675825,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:05.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:06.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:06.065] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:06.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:06.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:06.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:06.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:06.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675826,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:06.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:06.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:06.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:06.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:06.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:06.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:06.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:06.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675826,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:06.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:06.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:06.535] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:06.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:06.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:06.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:06.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:06.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675826,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:06.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:06.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:06.752] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:06.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:06.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:06.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:06.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:06.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675826,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:06.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:06.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:06.992] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:06.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:06.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:06.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:06.993] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:06.993] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675826,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:06.993] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:07.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:07.232] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:07.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:07.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:07.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:07.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:07.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675827,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:07.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:07.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:07.443] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:07.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:07.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:07.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:07.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:07.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675827,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:07.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:07.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:07.676] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:07.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:07.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:07.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:07.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:07.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675827,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:07.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:07.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:07.890] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:07.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:07.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:07.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:07.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:07.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675827,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:07.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:08.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:08.110] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:08.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:08.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:08.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:08.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:08.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675828,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:08.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:08.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:08.330] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:08.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:08.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:08.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:08.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:08.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675828,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:08.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:08.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:08.560] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:08.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:08.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:08.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:08.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:08.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675828,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:08.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:08.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:08.791] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:08.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:08.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:08.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675828,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:09.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:09.014] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:09.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:09.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:09.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:09.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:09.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675829,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:09.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:09.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:09.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:09.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:09.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:09.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:09.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:09.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675829,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:09.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:09.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:09.477] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:09.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:09.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:09.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:09.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:09.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675829,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:09.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:09.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:09.705] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:09.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:09.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:09.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:09.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:09.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675829,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:09.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:09.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:09.944] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:09.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:09.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:09.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:09.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:09.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675829,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:09.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:10.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:10.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:10.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:10.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:10.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:10.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:10.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675830,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:10.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:10.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:10.459] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:10.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:10.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:10.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:10.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:10.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675830,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:10.467] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:10.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:10.872] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:10.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:10.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:10.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:10.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:10.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675830,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:10.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:11.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:11.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:11.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:11.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:11.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:11.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:11.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675831,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:11.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:11.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:11.668] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:11.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:11.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:11.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:11.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:11.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675831,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:11.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:11.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:11.899] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:11.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:11.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:11.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:11.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:11.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675831,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:11.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:12.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:12.127] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:12.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:12.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:12.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:12.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:12.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675832,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:12.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:12.362] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:12.362] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:12.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:12.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:12.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:12.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:12.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675832,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:12.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:12.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:12.582] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:12.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:12.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:12.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:12.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:12.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675832,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:12.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:12.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:12.808] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:12.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:12.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:12.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:12.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:12.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675832,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:12.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:13.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:13.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:13.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:13.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:13.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:13.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:13.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675833,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:13.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:13.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:13.265] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:13.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:13.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:13.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:13.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:13.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675833,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:13.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:13.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:13.498] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:13.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:13.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:13.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:13.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:13.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675833,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:13.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:13.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:13.729] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:13.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:13.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:13.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:13.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:13.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675833,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:13.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:13.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:13.962] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:13.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:13.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:13.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:13.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:13.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675833,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:13.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:14.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:14.182] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:14.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:14.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:14.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:14.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:14.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675834,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:14.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:14.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:14.408] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:14.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:14.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:14.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:14.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:14.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675834,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:14.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:14.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:14.631] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:14.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:14.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:14.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:14.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:14.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675834,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:14.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:14.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:14.860] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:14.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:14.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:14.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:14.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:14.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675834,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:14.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:15.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:15.112] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:15.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:15.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:15.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:15.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:15.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675835,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:15.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:15.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:15.335] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:15.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:15.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:15.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:15.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:15.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675835,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:15.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:15.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:15.562] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:15.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:15.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:15.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:15.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:15.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675835,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:15.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:15.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:15.798] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:15.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:15.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:15.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:15.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:15.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675835,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:15.798] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:16.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:16.026] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:16.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:16.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:16.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:16.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:16.026] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675836,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:16.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:16.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:16.253] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:16.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:16.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:16.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:16.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:16.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675836,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:16.254] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:16.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:16.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:16.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:16.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:16.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:16.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:16.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675836,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:16.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:16.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:16.699] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:16.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:16.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:16.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:16.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:16.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675836,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:16.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:16.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:16.928] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:16.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:16.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:16.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:16.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:16.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675836,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:16.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:17.174] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:17.174] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:17.174] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:17.174] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:17.174] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:17.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:17.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675837,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:17.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:17.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:17.403] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:17.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:17.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:17.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:17.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:17.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675837,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:17.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:17.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:17.623] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:17.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:17.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:17.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:17.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:17.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675837,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:17.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:17.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:17.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:17.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:17.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:17.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:17.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:17.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675837,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:17.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:18.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:18.093] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:18.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:18.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:18.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:18.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:18.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675838,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:18.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:18.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:18.304] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:18.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:18.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:18.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:18.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:18.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675838,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:18.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:18.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:18.532] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:18.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:18.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:18.532] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:18.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:18.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675838,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:18.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:18.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:18.740] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:18.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:18.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:18.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:18.740] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:18.741] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675838,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:18.741] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:18.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:18.957] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:18.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:18.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:18.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:18.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:18.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675838,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:18.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:19.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:19.166] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:19.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:19.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:19.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:19.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:19.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675839,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:19.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:19.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:19.392] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:19.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:19.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:19.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:19.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:19.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675839,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:19.393] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:19.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:19.606] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:19.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:19.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:19.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:19.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:19.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675839,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:19.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:19.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:19.872] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:19.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:19.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:19.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:19.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:19.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675839,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:19.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:20.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:20.095] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:20.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:20.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:20.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:20.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:20.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675840,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:20.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:20.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:20.326] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:20.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:20.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:20.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:20.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:20.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675840,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:20.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:20.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:20.544] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:20.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:20.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:20.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:20.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:20.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675840,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:20.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:20.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:20.778] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:20.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:20.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:20.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:20.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:20.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675840,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:20.783] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:21.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:21.203] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:21.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:21.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:21.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:21.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:21.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675841,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:21.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:21.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:21.435] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:21.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:21.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:21.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:21.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:21.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675841,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:21.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:21.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:21.695] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:21.695] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:21.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:21.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:21.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:21.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675841,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:21.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:21.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:21.941] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:21.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:21.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:21.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:21.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:21.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675841,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:21.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:22.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:22.188] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:22.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:22.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:22.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:22.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:22.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675842,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:22.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:22.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:22.439] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:22.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:22.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:22.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:22.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:22.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675842,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:22.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:22.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:22.681] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:22.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:22.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:22.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:22.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:22.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675842,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:22.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:22.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:22.934] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:22.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:22.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:22.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:22.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:22.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675842,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:22.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:23.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:23.187] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:23.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:23.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:23.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:23.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:23.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675843,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:23.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:23.629] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:23.629] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:23.629] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:23.629] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:23.629] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:23.629] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:23.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675843,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:23.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:23.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:23.851] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:23.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:23.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:23.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:23.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:23.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675843,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:23.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:24.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:24.072] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:24.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:24.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:24.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:24.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:24.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675844,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:24.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:24.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:24.321] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:24.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:24.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:24.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:24.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:24.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675844,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:24.322] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:24.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:24.542] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:24.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:24.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:24.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:24.543] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:24.543] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675844,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:24.543] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:24.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:24.780] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:24.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:24.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:24.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:24.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:24.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675844,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:24.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:25.076] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:25.076] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:25.076] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:25.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:25.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:25.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:25.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675845,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:25.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:25.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:25.338] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:25.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:25.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:25.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:25.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:25.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675845,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:25.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:25.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:25.540] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:25.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:25.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:25.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:25.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:25.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675845,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:25.541] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:25.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:25.760] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:25.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:25.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:25.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:25.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:25.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675845,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:25.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:25.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:25.962] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:25.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:25.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:25.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:25.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:25.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675845,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:25.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:26.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:26.163] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:26.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:26.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:26.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:26.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:26.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675846,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:26.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:26.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:26.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:26.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:26.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:26.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:26.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:26.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675846,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:26.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:26.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:26.601] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:26.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:26.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:26.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:26.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:26.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675846,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:26.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:26.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:26.804] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:26.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:26.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:26.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:26.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:26.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675846,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:26.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:27.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:27.022] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:27.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:27.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:27.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:27.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:27.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675847,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:27.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:27.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:27.222] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:27.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:27.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:27.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:27.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:27.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675847,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:27.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:27.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:27.463] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:27.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:27.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:27.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:27.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:27.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675847,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:27.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:27.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:27.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:27.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:27.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:27.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:27.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:27.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675847,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:27.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:27.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:27.886] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:27.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:27.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:27.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:27.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:27.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675847,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:27.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:28.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:28.093] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:28.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:28.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:28.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:28.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:28.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675848,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:28.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:28.286] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:28.286] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:28.286] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:28.286] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:28.286] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:28.286] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:28.286] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675848,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:28.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:28.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:28.481] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:28.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:28.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:28.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:28.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:28.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675848,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:28.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:28.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:28.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:28.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:28.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:28.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:28.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:28.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675848,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:28.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:28.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:28.901] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:28.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:28.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:28.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:28.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:28.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675848,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:28.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:29.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:29.113] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:29.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:29.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:29.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:29.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:29.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675849,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:29.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:29.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:29.321] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:29.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:29.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:29.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:29.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:29.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675849,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:29.321] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:29.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:29.533] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:29.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:29.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:29.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:29.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:29.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675849,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:29.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:29.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:29.727] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:29.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:29.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:29.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:29.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:29.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675849,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:29.728] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:29.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:29.934] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:29.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:29.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:29.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:29.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:29.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675849,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:29.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:30.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:30.147] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:30.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:30.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:30.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:30.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:30.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675850,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:30.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:30.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:30.377] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:30.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:30.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:30.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:30.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:30.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675850,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:30.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:30.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:30.607] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:30.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:30.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:30.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:30.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:30.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675850,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:30.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:30.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:30.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:30.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:30.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:30.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:30.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:30.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675850,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:30.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:31.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:31.094] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:31.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:31.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:31.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:31.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:31.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675851,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:31.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:31.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:31.354] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:31.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:31.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:31.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:31.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:31.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675851,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:31.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:31.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:31.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:31.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:31.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:31.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:31.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:31.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675851,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:31.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:31.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:31.776] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:31.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:31.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:31.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:31.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:31.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675851,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:31.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:31.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:31.980] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:31.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:31.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:31.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:31.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:31.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675851,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:31.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:32.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:32.189] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:32.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:32.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:32.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:32.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:32.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675852,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:32.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:32.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:32.419] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:32.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:32.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:32.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:32.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:32.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675852,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:32.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:32.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:32.624] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:32.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:32.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:32.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:32.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:32.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675852,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:32.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:32.820] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:32.821] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:32.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:32.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:32.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:32.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:32.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675852,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:32.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:33.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:33.044] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:33.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:33.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:33.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:33.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:33.045] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675853,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:33.045] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:33.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:33.244] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:33.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:33.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:33.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:33.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:33.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675853,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:33.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:33.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:33.447] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:33.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:33.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:33.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:33.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:33.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675853,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:33.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:33.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:33.667] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:33.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:33.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:33.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:33.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:33.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675853,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:33.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:33.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:33.886] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:33.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:33.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:33.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:33.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:33.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675853,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:33.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:34.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:34.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:34.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:34.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:34.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:34.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:34.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675854,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:34.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:34.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:34.315] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:34.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:34.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:34.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:34.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:34.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675854,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:34.315] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:34.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:34.530] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:34.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:34.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:34.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:34.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:34.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675854,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:34.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:34.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:34.777] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:34.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:34.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:34.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:34.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:34.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675854,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:34.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:35.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:35.014] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:35.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:35.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:35.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:35.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:35.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675855,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:35.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:35.260] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675855,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:35.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:35.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:35.516] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:35.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:35.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:35.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:35.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:35.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675855,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:35.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:35.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:35.774] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:35.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:35.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:35.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:35.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:35.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675855,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:35.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:36.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:36.034] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:36.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:36.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:36.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:36.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:36.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675856,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:36.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:36.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:36.323] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:36.323] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:36.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:36.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:36.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:36.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675856,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:36.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:36.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:36.571] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:36.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:36.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:36.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:36.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:36.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675856,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:36.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:36.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:36.826] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:36.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:36.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:36.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:36.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:36.826] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675856,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:36.831] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:37.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:37.089] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:37.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:37.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:37.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:37.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:37.090] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675857,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:37.090] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:37.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:37.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:37.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:37.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:37.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:37.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:37.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675857,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:37.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:37.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:37.774] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:37.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:37.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:37.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:37.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:37.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675857,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:37.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:38.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:38.165] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:38.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:38.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:38.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:38.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:38.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675858,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:38.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:38.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:38.587] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:38.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:38.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:38.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:38.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:38.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675858,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:38.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:38.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:38.915] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:38.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:38.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:38.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:38.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:38.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675858,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:38.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:39.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:39.248] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:39.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:39.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:39.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:39.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:39.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675859,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:39.248] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:39.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:39.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:39.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:39.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:39.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:39.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:39.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675859,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:39.650] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:39.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:39.895] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:39.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:39.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:39.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:39.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:39.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675859,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:39.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:40.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:40.111] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:40.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:40.111] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:40.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:40.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:40.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675860,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:40.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:40.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:40.336] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:40.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:40.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:40.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:40.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:40.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675860,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:40.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:40.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:40.559] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:40.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:40.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:40.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:40.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:40.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675860,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:40.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:40.784] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:40.784] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:40.784] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:40.784] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:40.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:40.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:40.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675860,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:40.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:41.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:41.012] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:41.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:41.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:41.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:41.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:41.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675861,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:41.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:41.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:41.219] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:41.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:41.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:41.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:41.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:41.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675861,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:41.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:41.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:41.435] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:41.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:41.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:41.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:41.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:41.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675861,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:41.438] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:41.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:41.648] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:41.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:41.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:41.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:41.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:41.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675861,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:41.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:41.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:41.856] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:41.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:41.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:41.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:41.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:41.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675861,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:41.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:42.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:42.091] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:42.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:42.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:42.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:42.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:42.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675862,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:42.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:42.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:42.304] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:42.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:42.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:42.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:42.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:42.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675862,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:42.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:42.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:42.531] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:42.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:42.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:42.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:42.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:42.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675862,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:42.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:42.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:42.768] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:42.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:42.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:42.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:42.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:42.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675862,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:42.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:42.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:42.977] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:42.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:42.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:42.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:42.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:42.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675862,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:42.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:43.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:43.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:43.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:43.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:43.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:43.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:43.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675863,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:43.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:43.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:43.403] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:43.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:43.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:43.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:43.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:43.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675863,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:43.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:43.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:43.622] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:43.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:43.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:43.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:43.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:43.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675863,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:43.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:43.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:43.834] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:43.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:43.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:43.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:43.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:43.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675863,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:43.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:44.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:44.061] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:44.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:44.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:44.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:44.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:44.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675864,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:44.062] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:44.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:44.287] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:44.287] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:44.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:44.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:44.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:44.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675864,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:44.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:44.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:44.499] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:44.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:44.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:44.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:44.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:44.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675864,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:44.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:44.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:44.735] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:44.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:44.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:44.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:44.736] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:44.736] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675864,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:44.736] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:45.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:45.038] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:45.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:45.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:45.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:45.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:45.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675865,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:45.042] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:45.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:45.250] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:45.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:45.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:45.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:45.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:45.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675865,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:45.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:45.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:45.473] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:45.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:45.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:45.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:45.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:45.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675865,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:45.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:45.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:45.688] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:45.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:45.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:45.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:45.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:45.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675865,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:45.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:45.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:45.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:45.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:45.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:45.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:45.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:45.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675865,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:45.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:46.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:46.138] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:46.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:46.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:46.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:46.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:46.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675866,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:46.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:46.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:46.365] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:46.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:46.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:46.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:46.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:46.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675866,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:46.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:46.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:46.576] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:46.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:46.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:46.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:46.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:46.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675866,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:46.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:46.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:46.893] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:46.904] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:46.904] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:46.904] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:46.904] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:46.904] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675866,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:46.904] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:47.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:47.129] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:47.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:47.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:47.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:47.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:47.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675867,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:47.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:47.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:47.374] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:47.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:47.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:47.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:47.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:47.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675867,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:47.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:47.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:47.623] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:47.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:47.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:47.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:47.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:47.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675867,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:47.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:47.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:47.880] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:47.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:47.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:47.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:47.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:47.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675867,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:47.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:48.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:48.120] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:48.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:48.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:48.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:48.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:48.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675868,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:48.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:48.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:48.378] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:48.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:48.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:48.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:48.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:48.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675868,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:48.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:48.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:48.607] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:48.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:48.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:48.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:48.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:48.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675868,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:48.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:48.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:48.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:48.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:48.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:48.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:48.839] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:48.839] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675868,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:48.839] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:49.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:49.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:49.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:49.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:49.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:49.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:49.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675869,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:49.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:49.341] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:49.341] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:49.341] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:49.341] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:49.341] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:49.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:49.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675869,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:49.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:49.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:49.567] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:49.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:49.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:49.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:49.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:49.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675869,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:49.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:49.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:49.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:49.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:49.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:49.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:49.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:49.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675869,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:49.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:50.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:50.025] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:50.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:50.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:50.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:50.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:50.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675870,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:50.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:50.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:50.274] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:50.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:50.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:50.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:50.275] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:50.275] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675870,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:50.275] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:50.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:50.538] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:50.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:50.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:50.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:50.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:50.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675870,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:50.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:50.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:50.765] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:50.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:50.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:50.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:50.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:50.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675870,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:50.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:50.996] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:50.996] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:50.996] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:50.996] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:50.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:50.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:50.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675870,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:50.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:51.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:51.219] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:51.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:51.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:51.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:51.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:51.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675871,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:51.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:51.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:51.470] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:51.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:51.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:51.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:51.471] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:51.471] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675871,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:51.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:51.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:51.701] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:51.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:51.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:51.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:51.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:51.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675871,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:51.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:51.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:51.939] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:51.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:51.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:51.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:51.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:51.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675871,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:51.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:52.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:52.235] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:52.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:52.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:52.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:52.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:52.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675872,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:52.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:52.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:52.503] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:52.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:52.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:52.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:52.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:52.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675872,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:52.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:52.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:52.807] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:52.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:52.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:52.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:52.807] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:52.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675872,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:52.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:53.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:53.069] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:53.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:53.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:53.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:53.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:53.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675873,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:53.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:53.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:53.375] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:53.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:53.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:53.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:53.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:53.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675873,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:53.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:53.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:53.713] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:53.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:53.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:53.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:53.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:53.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675873,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:53.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:53.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:53.970] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:53.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:53.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:53.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:53.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:53.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675873,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:53.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:54.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:54.278] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:54.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:54.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:54.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:54.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:54.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675874,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:54.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:54.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:54.518] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:54.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:54.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:54.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:54.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:54.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675874,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:54.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:54.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:54.753] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:54.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:54.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:54.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:54.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:54.753] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675874,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:54.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:54.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:54.978] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:54.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:54.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:54.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:54.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:54.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675874,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:54.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:55.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:55.208] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:55.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:55.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:55.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:55.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:55.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675875,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:55.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:55.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:55.446] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:55.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:55.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:55.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:55.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:55.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675875,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:55.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:55.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:55.692] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:55.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:55.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:55.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:55.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:55.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675875,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:55.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:55.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:55.956] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:55.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:55.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:55.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:55.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:55.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675875,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:55.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:56.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:56.223] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:56.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:56.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:56.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:56.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:56.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675876,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:56.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:56.452] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:56.452] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:56.452] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:56.452] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:56.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:56.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:56.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675876,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:56.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:56.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:56.685] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:56.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:56.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:56.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:56.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:56.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675876,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:56.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:56.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:56.929] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:56.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:56.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:56.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:56.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:56.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675876,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:56.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:57.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:57.141] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:57.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:57.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:57.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:57.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:57.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675877,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:57.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:57.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:57.414] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:57.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:57.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:57.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:57.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:57.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675877,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:57.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:57.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:57.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:57.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:57.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:57.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:57.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:57.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675877,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:57.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:57.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:57.874] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:57.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:57.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:57.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:57.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:57.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675877,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:57.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:58.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:58.112] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:58.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:58.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:58.112] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:58.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:58.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675878,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:58.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:58.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:58.339] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:58.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:58.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:58.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:58.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:58.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675878,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:58.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:58.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:58.572] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:58.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:58.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:58.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:58.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:58.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675878,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:58.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:58.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:58.821] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:58.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:58.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:58.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:58.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:58.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675878,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:58.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:59.054] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:59.054] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:59.054] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:59.054] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:59.054] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:59.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:59.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675879,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:59.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:59.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:59.301] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:59.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:59.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:59.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:59.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:59.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675879,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:59.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:59.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:59.564] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:59.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:59.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:59.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:59.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:59.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675879,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:59.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:57:59.789] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:57:59.789] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:57:59.789] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:57:59.789] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:57:59.789] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:57:59.789] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:57:59.789] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675879,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:57:59.789] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:00.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:00.019] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:00.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:00.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:00.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:00.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:00.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675880,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:00.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:00.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:00.244] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:00.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:00.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:00.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:00.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:00.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675880,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:00.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:00.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:00.507] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:00.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:00.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:00.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:00.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:00.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675880,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:00.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:00.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:00.748] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:00.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:00.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:00.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:00.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:00.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675880,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:00.748] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:00.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:00.988] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:00.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:00.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:00.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:00.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:00.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675880,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:00.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:01.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:01.226] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:01.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:01.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:01.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:01.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:01.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675881,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:01.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:01.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:01.484] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:01.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:01.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:01.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:01.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:01.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675881,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:01.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:01.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:01.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:01.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:01.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:01.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:01.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:01.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675881,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:01.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:01.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:01.985] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:01.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:01.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:01.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:01.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:01.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675881,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:01.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:02.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:02.222] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:02.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:02.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:02.222] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:02.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:02.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675882,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:02.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:02.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:02.473] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:02.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:02.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:02.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:02.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:02.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675882,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:02.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:02.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:02.729] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:02.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:02.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:02.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:02.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:02.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675882,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:02.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:02.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:02.957] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:02.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:02.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:02.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:02.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:02.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675882,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:02.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:03.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:03.202] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:03.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:03.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:03.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:03.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:03.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675883,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:03.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:03.438] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:03.438] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:03.438] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:03.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:03.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:03.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:03.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675883,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:03.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:03.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:03.684] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:03.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:03.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:03.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:03.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:03.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675883,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:03.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:03.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:03.930] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:03.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:03.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:03.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:03.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:03.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675883,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:03.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:04.168] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:04.169] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:04.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:04.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:04.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:04.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:04.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675884,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:04.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:04.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:04.416] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:04.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:04.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:04.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:04.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:04.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675884,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:04.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:04.654] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675884,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:04.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:04.897] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:04.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:04.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:04.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:04.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:04.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675884,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:04.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:05.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:05.148] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:05.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:05.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:05.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:05.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:05.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675885,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:05.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:05.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:05.396] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:05.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:05.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:05.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:05.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:05.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675885,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:05.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:05.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:05.624] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:05.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:05.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:05.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:05.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:05.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675885,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:05.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:05.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:05.870] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:05.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:05.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:05.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:05.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:05.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675885,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:05.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:06.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:06.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:06.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:06.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:06.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:06.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:06.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675886,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:06.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:06.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:06.350] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:06.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:06.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:06.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:06.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:06.351] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675886,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:06.351] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:06.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:06.597] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:06.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:06.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:06.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:06.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:06.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675886,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:06.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:06.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:06.846] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:06.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:06.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:06.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:06.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:06.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675886,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:06.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:07.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:07.095] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:07.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:07.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:07.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:07.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:07.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675887,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:07.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:07.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:07.377] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:07.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:07.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:07.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:07.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:07.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675887,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:07.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:07.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:07.625] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:07.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:07.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:07.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:07.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:07.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675887,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:07.625] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:07.883] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:07.884] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675887,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:08.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:08.120] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:08.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:08.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:08.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:08.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:08.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675888,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:08.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:08.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:08.365] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:08.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:08.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:08.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:08.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:08.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675888,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:08.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:08.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:08.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:08.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:08.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:08.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:08.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:08.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675888,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:08.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:08.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:08.835] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:08.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:08.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:08.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:08.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:08.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675888,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:08.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:09.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:09.093] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:09.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:09.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:09.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:09.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:09.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675889,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:09.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:09.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:09.329] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:09.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:09.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:09.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:09.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:09.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675889,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:09.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:09.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:09.570] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:09.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:09.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:09.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:09.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:09.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675889,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:09.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:09.802] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:09.802] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:09.802] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:09.802] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:09.802] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:09.802] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:09.802] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675889,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:09.802] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:10.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:10.039] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:10.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:10.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:10.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:10.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:10.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675890,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:10.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:10.273] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:10.273] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:10.273] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:10.273] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:10.273] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:10.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:10.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675890,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:10.274] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:10.514] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:10.514] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:10.514] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:10.514] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:10.514] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:10.514] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:10.514] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675890,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:10.514] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:10.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:10.734] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:10.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:10.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:10.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:10.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:10.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675890,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:10.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:10.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:10.960] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:10.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:10.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:10.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:10.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:10.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675890,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:10.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:11.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:11.216] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:11.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:11.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:11.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:11.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:11.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675891,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:11.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:11.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:11.446] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:11.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:11.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:11.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:11.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:11.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675891,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:11.447] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:11.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:11.686] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:11.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:11.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:11.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:11.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:11.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675891,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:11.686] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:11.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:11.909] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:11.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:11.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:11.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:11.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:11.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675891,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:11.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:12.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:12.154] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:12.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:12.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:12.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:12.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:12.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675892,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:12.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:12.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:12.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:12.404] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:12.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:12.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:12.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:12.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675892,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:12.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:12.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:12.647] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:12.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:12.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:12.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:12.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:12.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675892,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:12.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:12.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:12.882] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:12.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:12.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:12.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:12.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:12.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675892,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:12.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:13.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:13.101] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:13.101] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:13.101] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:13.101] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:13.101] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:13.101] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675893,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:13.101] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:13.362] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:13.363] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675893,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:13.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:13.613] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:13.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:13.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:13.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:13.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:13.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675893,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:13.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:13.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:13.853] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:13.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:13.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:13.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:13.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:13.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675893,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:13.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:14.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:14.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:14.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:14.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:14.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:14.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:14.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675894,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:14.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:14.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:14.344] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:14.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:14.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:14.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:14.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:14.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675894,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:14.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:14.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:14.576] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:14.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:14.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:14.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:14.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:14.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675894,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:14.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:14.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:14.801] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:14.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:14.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:14.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:14.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:14.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675894,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:14.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:15.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:15.047] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:15.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:15.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:15.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:15.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:15.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675895,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:15.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:15.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:15.282] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:15.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:15.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:15.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:15.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:15.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675895,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:15.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:15.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:15.521] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:15.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:15.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:15.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:15.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:15.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675895,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:15.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:15.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:15.767] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:15.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:15.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:15.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:15.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:15.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675895,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:15.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:15.998] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:15.998] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:15.998] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:15.998] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:15.998] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:15.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:15.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675895,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:15.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:16.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:16.242] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:16.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:16.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:16.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:16.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:16.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675896,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:16.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:16.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:16.496] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:16.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:16.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:16.497] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:16.497] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:16.497] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675896,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:16.497] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:16.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:16.733] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:16.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:16.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:16.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:16.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:16.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675896,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:16.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:16.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:16.981] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:16.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:16.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:16.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:16.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:16.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675896,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:16.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:17.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:17.236] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:17.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:17.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:17.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:17.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:17.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675897,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:17.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:17.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:17.463] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:17.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:17.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:17.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:17.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:17.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675897,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:17.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:17.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:17.694] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:17.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:17.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:17.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:17.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:17.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675897,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:17.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:17.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:17.927] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:17.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:17.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:17.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:17.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:17.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675897,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:17.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:18.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:18.176] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:18.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:18.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:18.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:18.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:18.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675898,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:18.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:18.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:18.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:18.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:18.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:18.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:18.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:18.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675898,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:18.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:18.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:18.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:18.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:18.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:18.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:18.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:18.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675898,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:18.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:18.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:18.891] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:18.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:18.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:18.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:18.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:18.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675898,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:18.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:19.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:19.161] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:19.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:19.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:19.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:19.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:19.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675899,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:19.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:19.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:19.395] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:19.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:19.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:19.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:19.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:19.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675899,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:19.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:19.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:19.644] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:19.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:19.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:19.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:19.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:19.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675899,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:19.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:19.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:19.875] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:19.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:19.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:19.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:19.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:19.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675899,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:19.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:20.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:20.135] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:20.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:20.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:20.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:20.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:20.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675900,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:20.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:20.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:20.371] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:20.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:20.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:20.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:20.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:20.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675900,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:20.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:20.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:20.600] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:20.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:20.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:20.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:20.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:20.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675900,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:20.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:20.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:20.829] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:20.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:20.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:20.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:20.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:20.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675900,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:20.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:21.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:21.092] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:21.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:21.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:21.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:21.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:21.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675901,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:21.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:21.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:21.344] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:21.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:21.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:21.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:21.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:21.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675901,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:21.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:21.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:21.583] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:21.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:21.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:21.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:21.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:21.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675901,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:21.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:21.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:21.824] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:21.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:21.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:21.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:21.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:21.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675901,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:21.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:22.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:22.079] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:22.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:22.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:22.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:22.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:22.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675902,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:22.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:22.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:22.314] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:22.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:22.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:22.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:22.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:22.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675902,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:22.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:22.545] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:22.545] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:22.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:22.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:22.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:22.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:22.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675902,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:22.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:22.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:22.772] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:22.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:22.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:22.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:22.773] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:22.773] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675902,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:22.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:23.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:23.004] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:23.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:23.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:23.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:23.005] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:23.005] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675903,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:23.005] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:23.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:23.245] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:23.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:23.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:23.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:23.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:23.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675903,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:23.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:23.490] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:23.490] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:23.490] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:23.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:23.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:23.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:23.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675903,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:23.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:23.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:23.718] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:23.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:23.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:23.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:23.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:23.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675903,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:23.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:23.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:23.942] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:23.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:23.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:23.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:23.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:23.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675903,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:23.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:24.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:24.194] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:24.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:24.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:24.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:24.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:24.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675904,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:24.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:24.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:24.504] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:24.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:24.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:24.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:24.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:24.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675904,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:24.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:24.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:24.762] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:24.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:24.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:24.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:24.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:24.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675904,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:24.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:24.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:24.999] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:24.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:24.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:24.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:24.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:24.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675904,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:24.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:25.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:25.231] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:25.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:25.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:25.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:25.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:25.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675905,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:25.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:25.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:25.461] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:25.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:25.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:25.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:25.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:25.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675905,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:25.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:25.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:25.690] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:25.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:25.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:25.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:25.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:25.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675905,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:25.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:25.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:25.929] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:25.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:25.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:25.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:25.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:25.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675905,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:25.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:26.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:26.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:26.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:26.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:26.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:26.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:26.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675906,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:26.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:26.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:26.432] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:26.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:26.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:26.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:26.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:26.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675906,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:26.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:26.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:26.675] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:26.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:26.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:26.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:26.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:26.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675906,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:26.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:26.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:26.911] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:26.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:26.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:26.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:26.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:26.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675906,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:26.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:27.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:27.142] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:27.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:27.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:27.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:27.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:27.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675907,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:27.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:27.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:27.398] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:27.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:27.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:27.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:27.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:27.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675907,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:27.398] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:27.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:27.676] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:27.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:27.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:27.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:27.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:27.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675907,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:27.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:27.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:27.941] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:27.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:27.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:27.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:27.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:27.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675907,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:27.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:28.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:28.164] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:28.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:28.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:28.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:28.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:28.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675908,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:28.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:28.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:28.385] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:28.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:28.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:28.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:28.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:28.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675908,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:28.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:28.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:28.616] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:28.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:28.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:28.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:28.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:28.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675908,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:28.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:28.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:28.845] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:28.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:28.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:28.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:28.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:28.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675908,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:28.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:29.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:29.089] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:29.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:29.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:29.089] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:29.090] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:29.090] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675909,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:29.094] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:29.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:29.349] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:29.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:29.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:29.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:29.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:29.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675909,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:29.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:29.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:29.576] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:29.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:29.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:29.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:29.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:29.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675909,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:29.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:29.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:29.776] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:29.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:29.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:29.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:29.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:29.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675909,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:29.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:30.010] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675910,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:30.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:30.252] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675910,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:30.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:30.498] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:30.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:30.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:30.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:30.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:30.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675910,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:30.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:30.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:30.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:30.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:30.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:30.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:30.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:30.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675910,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:30.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:30.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:30.954] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:30.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:30.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:30.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:30.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:30.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675910,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:30.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:31.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:31.160] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:31.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:31.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:31.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:31.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:31.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675911,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:31.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:31.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:31.384] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:31.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:31.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:31.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:31.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:31.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675911,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:31.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:31.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:31.613] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:31.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:31.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:31.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:31.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:31.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675911,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:31.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:31.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:31.830] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:31.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:31.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:31.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:31.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:31.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675911,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:31.831] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:32.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:32.066] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:32.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:32.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:32.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:32.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:32.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675912,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:32.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:32.280] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:32.280] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:32.280] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:32.280] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:32.280] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:32.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:32.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675912,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:32.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:32.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:32.507] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:32.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:32.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:32.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:32.507] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:32.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675912,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:32.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:32.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:32.727] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:32.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:32.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:32.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:32.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:32.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675912,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:32.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:32.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:32.961] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:32.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:32.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:32.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:32.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:32.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675912,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:32.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:33.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:33.177] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:33.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:33.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:33.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:33.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:33.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675913,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:33.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:33.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:33.381] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:33.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:33.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:33.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:33.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:33.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675913,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:33.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:33.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:33.595] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:33.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:33.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:33.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:33.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:33.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675913,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:33.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:33.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:33.816] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:33.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:33.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:33.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:33.817] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:33.817] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675913,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:33.817] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:34.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:34.047] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:34.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:34.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:34.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:34.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:34.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675914,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:34.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:34.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:34.267] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:34.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:34.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:34.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:34.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:34.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675914,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:34.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:34.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:34.484] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:34.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:34.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:34.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:34.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:34.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675914,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:34.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:34.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:34.715] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:34.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:34.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:34.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:34.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:34.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675914,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:34.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:34.918] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:34.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:34.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:34.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:34.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:34.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:34.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675914,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:34.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:35.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:35.129] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:35.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:35.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:35.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:35.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:35.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675915,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:35.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:35.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:35.364] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:35.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:35.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:35.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:35.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:35.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675915,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:35.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:35.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:35.592] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:35.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:35.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:35.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:35.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:35.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675915,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:35.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:35.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:35.808] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:35.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:35.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:35.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:35.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:35.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675915,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:35.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:36.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:36.015] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:36.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:36.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:36.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:36.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:36.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675916,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:36.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:36.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:36.235] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:36.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:36.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:36.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:36.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:36.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675916,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:36.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:36.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:36.466] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:36.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:36.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:36.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:36.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:36.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675916,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:36.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:36.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:36.700] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:36.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:36.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:36.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:36.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:36.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675916,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:36.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:36.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:36.915] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:36.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:36.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:36.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:36.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:36.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675916,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:36.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:37.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:37.148] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:37.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:37.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:37.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:37.149] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:37.149] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675917,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:37.149] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:37.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:37.377] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:37.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:37.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:37.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:37.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:37.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675917,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:37.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:37.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:37.597] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:37.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:37.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:37.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:37.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:37.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675917,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:37.598] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:37.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:37.897] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:37.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:37.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:37.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:37.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:37.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675917,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:37.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:38.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:38.313] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:38.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:38.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:38.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:38.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:38.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675918,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:38.314] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:38.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:38.800] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:38.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:38.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:38.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:38.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:38.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675918,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:38.801] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:39.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:39.051] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:39.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:39.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:39.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:39.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:39.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675919,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:39.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:39.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:39.264] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:39.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:39.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:39.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:39.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:39.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675919,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:39.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:39.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:39.508] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:39.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:39.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:39.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:39.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:39.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675919,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:39.508] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:39.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:39.746] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:39.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:39.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:39.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:39.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:39.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675919,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:39.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:39.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:39.980] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:39.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:39.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:39.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:39.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:39.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675919,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:39.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:40.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:40.262] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:40.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:40.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:40.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:40.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:40.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675920,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:40.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:40.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:40.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:40.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:40.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:40.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:40.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:40.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675920,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:40.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:40.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:40.823] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:40.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:40.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:40.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:40.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:40.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675920,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:40.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:41.224] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:41.224] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:41.224] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:41.224] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:41.224] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:41.224] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:41.224] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675921,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:41.224] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:41.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:41.570] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:41.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:41.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:41.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:41.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:41.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675921,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:41.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:41.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:41.948] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:41.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:41.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:41.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:41.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:41.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675921,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:41.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:42.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:42.212] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:42.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:42.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:42.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:42.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:42.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675922,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:42.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:42.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:42.630] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:42.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:42.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:42.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:42.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:42.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675922,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:42.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:42.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:42.930] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:42.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:42.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:42.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:42.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:42.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675922,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:42.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:43.256] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:43.256] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:43.256] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:43.256] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:43.256] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:43.256] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:43.256] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675923,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:43.256] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:43.515] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:43.515] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:43.515] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:43.515] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:43.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:43.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:43.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675923,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:43.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:43.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:43.833] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:43.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:43.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:43.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:43.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:43.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675923,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:43.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:44.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:44.188] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:44.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:44.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:44.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:44.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:44.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675924,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:44.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:44.613] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675924,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:44.939] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675924,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:44.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:45.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:45.152] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:45.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:45.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:45.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:45.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:45.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675925,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:45.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:45.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:45.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:45.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:45.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:45.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:45.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:45.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675925,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:45.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:45.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:45.573] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:45.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:45.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:45.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:45.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:45.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675925,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:45.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:45.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:45.780] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:45.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:45.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:45.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:45.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:45.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675925,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:45.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:46.045] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:46.045] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:46.045] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:46.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:46.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:46.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:46.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675926,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:46.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:46.280] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:46.281] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:46.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:46.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:46.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:46.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:46.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675926,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:46.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:46.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:46.483] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:46.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675926,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:46.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:46.731] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:46.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:46.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:46.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:46.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:46.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675926,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:46.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:46.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:46.991] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:46.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:46.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:46.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:46.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:46.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675926,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:46.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:47.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:47.237] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:47.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:47.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:47.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:47.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:47.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675927,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:47.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:47.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:47.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:47.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:47.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:47.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:47.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:47.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675927,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:47.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:47.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:47.724] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:47.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:47.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:47.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:47.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:47.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675927,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:47.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:47.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:47.974] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:47.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:47.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:47.986] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:47.986] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:47.986] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675927,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:47.986] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:48.199] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:48.199] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:48.199] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:48.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:48.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:48.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:48.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675928,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:48.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:48.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:48.425] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:48.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:48.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:48.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:48.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:48.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675928,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:48.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:48.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:48.655] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:48.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:48.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:48.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:48.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:48.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675928,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:48.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:48.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:48.877] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:48.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:48.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:48.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:48.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:48.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675928,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:48.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:49.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:49.122] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:49.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:49.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:49.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:49.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:49.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675929,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:49.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:49.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:49.345] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:49.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:49.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:49.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:49.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:49.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675929,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:49.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:49.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:49.556] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:49.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:49.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:49.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:49.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:49.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675929,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:49.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:49.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:49.776] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:49.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:49.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:49.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:49.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:49.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675929,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:49.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:49.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:49.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675929,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:50.216] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675930,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:50.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:50.459] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:50.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:50.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:50.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:50.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:50.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675930,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:50.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:50.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:50.701] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:50.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:50.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:50.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:50.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:50.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675930,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:50.705] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:50.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:50.945] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:50.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:50.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675930,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:51.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:51.160] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:51.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:51.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:51.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:51.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:51.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675931,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:51.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:51.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:51.382] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:51.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:51.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:51.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:51.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:51.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675931,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:51.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:51.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:51.614] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:51.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:51.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:51.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:51.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:51.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675931,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:51.614] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:51.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:51.867] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:51.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:51.867] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:51.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:51.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:51.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675931,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:51.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:52.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:52.108] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:52.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:52.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:52.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:52.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:52.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675932,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:52.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:52.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:52.337] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:52.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:52.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:52.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:52.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:52.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675932,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:52.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:52.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:52.577] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:52.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:52.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:52.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:52.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:52.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675932,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:52.578] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:52.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:52.814] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:52.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:52.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:52.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:52.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:52.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675932,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:52.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:53.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:53.040] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:53.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:53.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:53.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:53.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:53.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675933,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:53.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:53.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:53.255] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:53.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:53.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:53.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:53.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:53.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675933,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:53.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:53.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:53.488] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:53.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:53.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:53.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:53.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:53.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675933,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:53.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:53.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:53.735] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:53.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:53.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:53.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:53.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:53.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675933,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:53.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:53.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:53.961] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:53.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:53.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:53.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:53.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:53.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675933,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:53.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:54.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:54.192] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:54.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:54.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:54.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:54.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:54.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675934,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:54.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:54.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:54.409] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:54.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:54.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:54.409] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:54.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:54.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675934,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:54.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:54.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:54.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:54.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:54.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:54.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:54.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:54.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675934,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:54.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:54.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:54.897] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:54.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:54.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:54.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:54.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:54.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675934,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:54.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:55.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:55.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:55.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:55.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:55.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:55.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:55.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675935,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:55.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:55.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:55.445] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:55.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:55.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:55.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:55.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:55.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675935,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:55.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:55.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:55.678] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:55.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:55.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:55.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:55.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:55.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675935,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:55.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:55.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:55.895] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:55.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:55.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:55.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:55.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:55.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675935,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:55.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:56.132] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675936,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:56.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:56.348] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:56.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:56.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:56.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:56.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:56.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675936,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:56.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:56.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:56.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:56.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:56.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:56.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:56.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:56.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675936,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:56.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:56.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:56.782] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:56.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:56.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:56.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:56.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:56.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675936,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:56.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:57.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:57.003] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:57.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:57.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:57.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:57.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:57.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675937,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:57.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:57.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:57.212] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:57.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:57.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:57.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:57.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:57.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675937,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:57.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:57.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:57.424] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:57.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:57.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:57.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:57.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:57.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675937,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:57.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:57.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:57.699] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:57.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:57.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:57.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:57.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:57.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675937,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:57.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:57.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:57.910] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:57.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:57.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:57.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:57.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:57.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675937,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:57.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:58.180] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:58.180] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:58.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:58.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:58.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:58.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:58.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675938,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:58.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:58.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:58.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:58.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:58.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:58.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:58.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:58.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675938,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:58.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:58.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:58.815] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:58.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:58.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:58.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:58.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:58.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675938,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:58.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:59.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:59.218] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:59.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:59.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:59.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:59.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:59.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675939,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:59.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:59.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:59.479] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:59.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:59.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:59.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:59.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:59.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675939,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:59.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:58:59.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:58:59.764] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:58:59.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:58:59.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:58:59.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:58:59.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:58:59.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675939,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:58:59.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:00.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:00.110] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:00.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:00.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:00.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:00.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:00.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675940,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:00.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:00.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:00.494] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:00.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:00.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:00.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:00.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:00.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675940,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:00.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:01.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:01.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:01.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:01.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:01.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:01.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:01.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675941,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:01.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:01.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:01.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:01.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:01.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:01.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:01.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:01.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675941,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:01.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:01.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:01.674] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:01.674] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:01.674] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:01.674] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:01.674] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:01.674] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675941,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:01.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:01.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:01.890] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:01.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:01.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:01.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:01.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:01.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675941,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:01.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:02.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:02.119] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:02.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:02.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:02.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:02.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:02.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675942,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:02.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:02.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:02.380] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:02.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:02.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:02.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:02.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:02.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675942,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:02.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:02.850] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:02.850] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:02.850] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:02.850] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:02.850] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:02.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:02.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675942,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:02.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:03.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:03.219] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:03.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:03.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:03.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:03.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:03.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675943,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:03.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:03.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:03.483] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:03.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:03.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:03.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:03.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:03.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675943,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:03.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:03.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:03.790] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:03.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:03.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:03.790] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:03.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:03.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675943,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:03.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:04.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:04.037] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:04.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:04.037] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:04.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:04.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:04.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675944,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:04.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:04.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:04.267] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:04.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:04.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:04.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:04.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:04.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675944,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:04.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:04.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:04.492] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:04.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:04.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:04.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:04.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:04.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675944,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:04.493] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:04.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:04.707] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:04.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:04.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:04.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:04.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:04.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675944,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:04.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:04.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:04.926] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:04.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:04.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:04.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:04.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:04.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675944,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:04.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:05.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:05.177] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:05.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:05.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:05.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:05.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:05.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675945,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:05.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:05.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:05.423] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:05.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:05.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:05.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:05.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:05.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675945,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:05.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:05.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:05.639] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:05.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:05.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:05.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:05.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:05.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675945,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:05.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:05.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:05.857] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:05.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:05.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:05.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:05.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:05.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675945,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:05.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:06.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:06.109] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:06.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:06.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:06.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:06.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:06.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675946,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:06.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:06.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:06.344] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:06.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:06.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:06.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:06.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:06.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675946,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:06.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:06.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:06.600] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:06.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:06.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:06.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:06.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:06.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675946,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:06.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:06.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:06.868] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:06.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:06.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:06.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:06.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:06.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675946,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:06.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:07.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:07.129] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:07.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:07.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:07.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:07.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:07.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675947,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:07.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:07.369] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:07.369] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:07.369] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:07.369] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:07.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:07.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:07.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675947,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:07.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:07.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:07.622] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:07.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:07.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:07.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:07.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:07.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675947,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:07.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:08.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:08.087] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:08.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:08.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:08.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:08.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:08.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675948,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:08.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:08.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:08.633] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:08.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:08.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:08.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:08.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:08.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675948,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:08.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:09.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:09.060] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:09.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:09.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:09.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:09.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:09.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675949,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:09.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:09.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:09.342] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:09.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:09.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:09.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:09.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:09.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675949,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:09.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:09.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:09.600] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:09.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:09.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:09.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:09.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:09.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675949,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:09.601] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:09.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:09.834] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:09.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:09.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:09.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:09.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:09.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675949,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:09.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:10.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:10.080] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:10.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:10.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:10.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:10.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:10.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675950,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:10.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:10.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:10.326] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:10.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:10.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:10.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:10.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:10.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675950,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:10.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:10.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:10.564] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:10.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:10.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:10.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:10.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:10.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675950,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:10.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:10.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:10.832] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:10.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:10.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:10.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:10.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:10.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675950,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:10.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:11.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:11.085] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:11.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:11.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:11.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:11.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:11.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675951,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:11.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:11.317] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:11.317] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:11.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:11.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:11.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:11.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:11.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675951,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:11.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:11.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:11.553] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:11.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:11.553] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:11.554] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:11.554] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:11.554] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675951,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:11.554] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:11.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:11.785] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:11.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:11.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:11.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:11.785] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:11.786] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675951,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:11.786] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:12.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:12.107] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:12.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:12.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:12.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:12.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:12.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675952,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:12.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:12.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:12.538] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:12.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:12.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:12.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:12.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:12.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675952,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:12.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:12.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:12.866] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:12.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:12.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:12.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:12.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:12.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675952,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:12.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:13.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:13.092] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:13.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:13.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:13.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:13.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:13.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675953,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:13.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:13.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:13.407] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:13.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:13.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:13.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:13.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:13.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675953,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:13.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:13.710] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:13.710] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:13.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675953,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:13.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:14.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:14.071] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:14.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:14.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:14.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:14.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:14.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675954,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:14.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:14.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:14.303] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:14.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:14.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:14.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:14.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:14.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675954,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:14.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:14.659] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:14.659] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:14.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:14.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:14.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:14.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:14.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675954,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:14.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:14.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:14.890] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:14.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:14.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:14.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:14.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:14.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675954,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:14.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:15.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:15.144] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:15.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:15.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:15.144] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:15.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:15.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675955,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:15.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:15.404] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:15.404] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:15.404] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:15.404] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:15.404] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:15.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:15.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675955,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:15.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:15.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:15.791] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:15.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:15.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:15.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:15.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:15.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675955,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:15.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:16.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:16.107] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:16.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:16.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:16.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:16.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:16.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675956,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:16.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:16.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:16.335] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:16.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:16.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:16.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:16.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:16.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675956,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:16.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:16.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:16.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:16.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:16.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:16.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:16.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:16.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675956,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:16.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:16.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:16.788] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:16.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:16.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:16.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:16.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:16.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675956,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:16.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:17.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:17.044] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:17.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:17.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:17.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:17.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:17.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675957,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:17.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:17.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:17.269] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:17.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:17.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:17.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:17.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:17.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675957,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:17.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:17.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:17.492] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:17.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:17.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:17.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:17.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:17.493] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675957,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:17.493] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:17.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:17.712] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:17.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:17.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:17.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:17.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:17.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675957,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:17.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:17.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:17.978] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:17.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:17.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:17.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:17.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:17.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675957,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:17.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:18.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:18.193] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:18.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:18.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:18.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:18.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:18.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675958,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:18.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:18.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:18.431] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:18.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:18.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:18.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:18.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:18.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675958,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:18.431] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:18.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:18.668] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:18.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:18.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:18.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:18.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:18.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675958,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:18.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:18.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:18.974] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:18.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:18.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:18.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:18.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:18.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675958,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:18.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:19.411] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:19.411] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:19.411] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:19.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:19.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:19.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:19.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675959,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:19.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:19.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:19.781] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675959,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:20.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:20.024] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:20.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:20.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:20.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:20.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:20.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675960,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:20.025] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:20.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:20.427] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:20.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:20.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:20.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:20.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:20.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675960,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:20.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:20.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:20.723] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:20.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:20.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:20.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:20.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:20.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675960,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:20.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:20.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:20.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:20.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:20.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:20.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:20.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:20.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675960,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:20.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:21.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:21.235] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:21.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:21.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:21.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:21.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:21.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675961,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:21.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:21.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:21.484] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:21.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:21.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:21.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:21.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:21.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675961,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:21.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:21.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:21.699] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:21.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:21.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:21.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:21.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:21.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675961,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:21.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:21.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:21.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:21.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:21.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:21.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:21.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:21.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675961,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:21.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:22.180] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:22.180] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:22.180] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:22.180] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:22.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:22.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:22.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675962,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:22.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:22.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:22.403] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:22.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:22.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:22.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:22.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:22.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675962,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:22.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:22.621] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:22.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:22.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675962,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:22.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:22.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:22.832] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:22.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:22.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:22.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:22.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:22.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675962,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:22.832] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:23.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:23.044] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:23.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:23.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:23.044] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:23.045] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:23.045] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675963,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:23.045] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:23.259] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:23.259] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:23.259] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:23.259] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:23.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:23.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:23.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675963,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:23.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:23.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:23.492] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:23.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:23.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:23.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:23.493] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:23.493] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675963,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:23.493] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:23.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:23.730] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:23.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:23.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:23.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:23.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:23.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675963,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:23.731] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:23.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:23.955] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:23.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:23.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:23.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:23.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:23.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675963,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:23.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:24.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:24.205] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:24.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:24.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:24.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:24.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:24.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675964,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:24.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:24.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:24.432] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:24.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:24.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:24.432] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675964,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:24.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:24.752] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:24.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:24.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:24.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:24.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:24.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675964,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:24.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:25.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:25.072] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:25.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:25.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:25.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:25.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:25.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675965,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:25.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:25.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:25.375] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:25.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:25.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:25.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:25.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:25.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675965,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:25.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:25.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:25.660] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:25.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:25.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:25.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:25.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:25.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675965,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:25.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:25.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:25.870] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:25.870] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:25.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:25.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:25.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:25.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675965,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:25.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:26.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:26.167] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:26.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:26.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:26.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:26.168] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:26.168] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675966,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:26.168] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:26.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:26.392] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:26.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:26.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:26.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:26.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:26.393] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675966,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:26.393] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:26.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:26.617] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:26.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:26.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:26.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:26.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:26.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675966,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:26.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:26.904] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:26.904] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:26.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:26.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:26.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:26.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:26.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675966,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:26.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:27.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:27.215] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:27.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:27.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:27.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:27.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:27.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675967,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:27.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:27.423] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:27.424] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:27.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:27.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:27.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:27.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:27.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675967,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:27.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:27.640] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:27.640] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:27.640] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:27.640] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:27.640] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:27.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:27.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675967,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:27.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:27.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:27.900] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:27.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:27.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:27.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:27.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:27.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675967,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:27.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:28.115] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:28.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:28.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:28.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:28.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:28.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:28.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675968,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:28.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:28.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:28.390] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:28.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:28.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:28.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:28.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:28.390] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675968,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:28.391] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:28.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:28.602] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:28.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:28.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:28.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:28.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:28.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675968,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:28.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:28.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:28.908] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:28.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:28.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:28.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:28.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:28.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675968,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:28.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:29.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:29.164] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:29.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:29.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:29.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:29.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:29.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675969,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:29.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:29.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:29.442] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:29.442] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:29.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:29.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:29.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:29.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675969,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:29.443] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:29.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:29.750] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:29.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:29.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:29.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:29.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:29.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675969,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:29.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:29.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:29.988] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:29.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:29.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:29.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:29.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:29.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675969,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:29.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:30.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:30.219] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:30.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:30.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:30.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:30.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:30.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675970,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:30.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:30.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:30.477] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:30.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:30.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:30.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:30.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:30.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675970,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:30.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:30.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:30.767] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:30.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:30.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:30.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:30.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:30.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675970,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:30.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:30.989] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:30.989] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:30.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:30.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:30.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:30.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:30.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675970,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:30.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:31.225] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:31.225] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:31.225] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:31.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:31.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:31.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:31.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675971,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:31.226] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:31.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:31.539] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:31.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:31.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:31.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:31.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:31.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675971,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:31.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:31.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:31.780] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:31.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:31.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:31.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:31.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:31.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675971,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:31.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:31.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:31.994] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:31.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:31.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:31.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:31.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:31.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675971,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:31.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:32.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:32.208] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:32.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:32.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:32.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:32.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:32.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675972,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:32.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:32.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:32.429] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:32.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:32.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:32.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:32.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:32.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675972,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:32.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:32.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:32.644] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:32.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:32.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:32.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:32.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:32.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675972,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:32.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:32.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:32.861] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:32.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:32.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:32.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:32.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:32.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675972,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:32.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:33.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:33.097] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:33.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:33.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:33.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:33.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:33.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675973,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:33.101] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:33.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:33.353] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:33.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:33.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:33.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:33.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:33.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675973,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:33.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:33.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:33.595] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:33.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:33.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:33.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:33.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:33.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675973,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:33.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:33.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:33.833] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:33.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:33.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:33.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:33.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:33.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675973,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:33.833] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:34.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:34.086] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:34.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:34.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:34.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:34.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:34.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675974,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:34.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:34.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:34.333] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:34.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:34.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:34.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675974,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:34.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:34.591] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:34.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:34.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:34.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:34.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:34.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675974,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:34.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:34.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:34.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:34.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:34.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:34.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:34.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:34.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675974,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:34.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:35.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:35.095] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:35.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:35.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:35.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:35.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:35.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675975,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:35.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:35.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:35.374] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:35.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:35.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:35.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:35.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:35.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675975,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:35.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:35.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:35.631] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:35.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:35.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:35.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:35.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:35.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675975,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:35.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:35.902] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:35.903] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:35.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:35.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:35.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:35.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:35.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675975,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:35.903] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:36.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:36.186] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:36.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:36.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:36.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:36.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:36.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675976,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:36.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:36.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:36.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:36.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:36.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:36.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:36.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:36.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675976,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:36.440] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:36.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:36.688] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:36.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:36.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:36.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:36.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:36.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675976,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:36.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:36.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:36.975] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:36.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:36.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:36.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:36.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:36.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675976,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:36.977] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:37.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:37.232] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:37.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:37.233] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:37.233] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:37.233] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:37.233] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675977,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:37.233] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:37.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:37.523] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:37.524] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:37.524] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:37.524] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:37.524] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:37.524] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675977,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:37.524] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:37.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:37.835] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:37.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:37.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:37.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:37.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:37.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675977,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:37.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:38.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:38.083] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:38.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:38.083] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:38.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:38.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:38.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675978,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:38.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:38.339] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:38.339] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:38.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:38.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:38.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:38.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:38.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675978,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:38.340] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:38.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:38.600] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:38.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:38.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:38.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:38.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:38.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675978,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:38.600] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:38.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:38.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:38.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:38.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:38.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:38.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:38.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675978,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:38.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:39.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:39.124] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:39.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:39.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:39.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:39.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:39.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675979,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:39.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:39.393] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:39.393] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:39.393] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:39.393] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:39.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:39.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:39.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675979,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:39.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:39.665] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:39.665] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:39.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:39.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:39.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:39.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:39.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675979,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:39.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:39.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:39.937] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:39.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:39.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:39.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:39.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:39.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675979,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:39.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:40.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:40.215] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:40.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:40.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:40.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:40.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:40.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675980,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:40.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:40.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:40.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:40.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:40.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:40.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:40.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:40.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675980,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:40.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:40.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:40.722] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:40.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:40.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:40.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:40.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:40.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675980,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:40.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:40.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:40.954] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:40.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:40.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:40.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:40.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:40.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675980,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:40.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:41.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:41.207] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:41.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:41.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:41.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:41.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:41.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675981,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:41.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:41.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:41.692] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:41.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:41.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:41.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:41.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:41.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675981,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:41.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:42.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:42.066] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:42.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:42.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:42.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:42.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:42.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675982,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:42.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:42.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:42.312] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:42.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:42.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:42.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:42.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:42.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675982,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:42.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:42.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:42.633] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:42.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:42.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:42.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:42.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:42.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675982,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:42.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:42.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:42.853] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:42.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:42.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:42.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:42.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:42.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675982,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:42.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:43.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:43.070] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:43.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:43.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:43.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:43.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:43.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675983,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:43.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:43.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:43.446] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:43.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:43.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:43.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:43.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:43.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675983,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:43.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:43.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:43.727] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:43.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:43.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:43.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:43.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:43.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675983,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:43.727] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:43.971] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:43.971] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:43.971] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:43.971] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:43.971] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:43.971] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:43.971] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675983,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:43.971] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:44.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:44.207] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:44.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:44.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:44.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:44.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:44.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675984,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:44.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:44.434] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:44.434] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:44.434] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:44.434] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:44.434] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:44.434] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:44.434] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675984,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:44.437] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:44.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:44.683] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:44.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:44.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:44.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:44.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:44.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675984,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:44.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:44.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:44.960] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:44.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:44.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:44.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:44.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:44.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675984,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:44.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:45.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:45.207] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:45.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:45.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:45.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:45.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:45.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675985,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:45.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:45.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:45.453] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:45.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:45.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:45.453] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:45.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:45.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675985,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:45.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:45.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:45.701] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:45.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:45.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:45.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:45.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:45.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675985,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:45.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:45.959] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:45.959] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:45.959] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:45.959] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:45.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:45.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:45.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675985,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:45.960] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:46.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:46.231] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:46.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:46.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:46.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:46.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:46.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675986,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:46.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:46.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:46.487] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:46.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:46.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:46.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:46.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:46.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675986,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:46.488] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:46.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:46.757] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:46.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:46.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:46.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:46.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:46.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675986,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:46.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:47.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:47.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:47.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:47.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:47.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:47.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:47.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675987,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:47.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:47.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:47.270] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:47.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:47.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:47.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:47.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:47.270] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675987,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:47.275] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:47.555] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:47.555] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:47.555] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:47.555] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:47.555] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:47.555] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:47.555] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675987,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:47.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:47.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:47.810] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:47.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:47.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:47.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:47.811] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:47.811] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675987,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:47.811] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:48.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:48.095] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:48.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:48.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:48.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:48.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:48.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675988,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:48.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:48.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:48.365] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:48.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:48.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:48.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:48.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:48.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675988,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:48.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:48.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:48.628] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:48.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:48.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:48.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:48.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:48.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675988,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:48.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:48.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:48.901] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:48.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:48.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:48.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:48.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:48.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675988,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:48.901] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:49.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:49.157] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:49.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:49.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:49.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:49.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:49.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675989,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:49.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:49.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:49.591] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:49.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:49.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:49.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:49.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:49.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675989,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:49.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:49.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:49.863] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:49.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:49.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:49.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:49.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:49.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675989,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:49.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:50.215] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:50.216] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675990,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:50.216] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:50.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:50.567] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:50.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:50.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:50.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:50.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:50.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675990,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:50.567] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:50.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:50.834] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:50.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:50.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:50.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:50.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:50.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675990,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:50.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:51.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:51.100] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:51.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:51.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:51.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:51.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:51.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675991,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:51.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:51.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:51.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:51.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:51.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:51.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:51.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:51.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675991,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:51.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:51.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:51.715] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:51.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:51.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:51.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:51.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:51.716] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675991,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:51.716] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:51.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:51.968] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:51.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:51.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:51.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:51.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:51.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675991,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:51.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:52.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:52.375] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:52.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:52.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:52.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:52.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:52.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675992,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:52.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:52.701] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:52.701] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:52.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:52.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:52.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:52.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:52.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675992,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:52.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:52.969] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675992,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:52.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:53.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:53.236] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:53.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:53.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:53.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:53.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:53.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675993,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:53.236] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:53.594] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:53.594] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:53.594] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:53.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:53.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:53.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:53.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675993,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:53.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:53.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:53.925] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:53.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:53.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:53.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:53.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:53.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675993,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:53.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:54.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:54.186] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:54.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:54.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:54.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:54.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:54.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675994,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:54.186] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:54.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:54.474] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:54.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:54.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:54.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:54.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:54.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675994,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:54.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:54.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:54.810] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:54.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:54.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:54.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:54.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:54.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675994,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:54.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:55.105] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:55.105] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:55.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:55.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:55.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:55.116] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:55.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675995,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:55.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:55.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:55.588] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:55.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:55.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:55.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:55.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:55.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675995,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:55.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:55.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:55.908] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:55.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:55.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:55.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:55.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:55.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675995,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:55.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:56.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:56.178] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:56.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:56.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:56.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:56.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:56.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675996,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:56.178] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:56.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:56.449] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:56.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:56.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:56.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:56.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:56.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675996,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:56.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:56.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:56.722] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:56.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:56.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:56.722] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:56.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:56.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675996,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:56.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:57.018] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:57.018] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:57.018] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:57.018] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:57.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:57.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:57.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675997,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:57.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:57.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:57.377] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:57.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:57.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:57.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:57.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:57.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675997,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:57.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:57.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:57.646] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:57.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:57.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:57.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:57.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:57.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675997,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:57.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:57.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:57.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:57.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:57.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:57.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:57.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:57.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675997,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:57.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:58.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:58.187] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:58.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:58.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:58.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:58.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:58.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675998,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:58.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:58.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:58.535] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:58.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:58.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:58.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:58.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:58.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675998,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:58.536] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:58.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:58.791] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:58.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:58.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:58.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:58.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:58.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675998,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:58.791] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:59.099] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:59.099] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:59.099] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:59.099] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:59.099] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:59.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:59.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675999,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:59.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:59.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:59.330] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:59.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:59.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:59.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:59.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:59.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675999,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:59.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:59.565] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:59.565] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:59.565] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:59.565] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:59.565] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:59.565] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:59.565] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675999,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:59.565] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 13:59:59.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 13:59:59.793] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 13:59:59.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 13:59:59.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 13:59:59.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 13:59:59.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 13:59:59.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727675999,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 13:59:59.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:00.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:00.041] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:00.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:00.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:00.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:00.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:00.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676000,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:00.045] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:00.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:00.370] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:00.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:00.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:00.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:00.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:00.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676000,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:00.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:00.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:00.660] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:00.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:00.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:00.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:00.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:00.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676000,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:00.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:00.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:00.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:00.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:00.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:00.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:00.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:00.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676000,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:00.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:01.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:01.131] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:01.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:01.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:01.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:01.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:01.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676001,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:01.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:01.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:01.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:01.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:01.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:01.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:01.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:01.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676001,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:01.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:01.626] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:01.627] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:01.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:01.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:01.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:01.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:01.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676001,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:01.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:01.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:01.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:01.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:01.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:01.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:01.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:01.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676001,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:01.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:02.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:02.140] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:02.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:02.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:02.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:02.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:02.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676002,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:02.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:02.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:02.378] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:02.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:02.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:02.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:02.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:02.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676002,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:02.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:02.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:02.623] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:02.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:02.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:02.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:02.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:02.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676002,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:02.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:02.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:02.854] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:02.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:02.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:02.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:02.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:02.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676002,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:02.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:03.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:03.077] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:03.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:03.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:03.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:03.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:03.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676003,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:03.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:03.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:03.312] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:03.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:03.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:03.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:03.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:03.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676003,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:03.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:03.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:03.551] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:03.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:03.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:03.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:03.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:03.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676003,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:03.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:03.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:03.806] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:03.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:03.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:03.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:03.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:03.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676003,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:03.806] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:04.053] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:04.053] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:04.053] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:04.053] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:04.053] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:04.054] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:04.054] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676004,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:04.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:04.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:04.347] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:04.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:04.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:04.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:04.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:04.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676004,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:04.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:04.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:04.618] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:04.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:04.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:04.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:04.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:04.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676004,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:04.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:04.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:04.859] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:04.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:04.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:04.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:04.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:04.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676004,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:04.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:05.105] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:05.105] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:05.105] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:05.105] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:05.105] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:05.105] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:05.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676005,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:05.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:05.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:05.374] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:05.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:05.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:05.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:05.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:05.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676005,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:05.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:05.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:05.632] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:05.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:05.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:05.632] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:05.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:05.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676005,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:05.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:05.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:05.886] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:05.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:05.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:05.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:05.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:05.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676005,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:05.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:06.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:06.166] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:06.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:06.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:06.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:06.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:06.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676006,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:06.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:06.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:06.408] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:06.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:06.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:06.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:06.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:06.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676006,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:06.408] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:06.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:06.638] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:06.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:06.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:06.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:06.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:06.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676006,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:06.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:06.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:06.908] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:06.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:06.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:06.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:06.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:06.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676006,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:06.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:07.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:07.155] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:07.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:07.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:07.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:07.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:07.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676007,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:07.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:07.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:07.395] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:07.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:07.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:07.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:07.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:07.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676007,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:07.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:07.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:07.647] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:07.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:07.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:07.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:07.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:07.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676007,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:07.647] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:07.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:07.896] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:07.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:07.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:07.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:07.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:07.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676007,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:07.897] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:08.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:08.131] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:08.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:08.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:08.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:08.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:08.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676008,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:08.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:08.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:08.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:08.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:08.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:08.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:08.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:08.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676008,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:08.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:08.651] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:08.651] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:08.651] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:08.651] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:08.651] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:08.651] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:08.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676008,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:08.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:08.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:08.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:08.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:08.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:08.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:08.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:08.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676008,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:08.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:09.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:09.183] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:09.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:09.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:09.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:09.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:09.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676009,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:09.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:09.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:09.483] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:09.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:09.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:09.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:09.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:09.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676009,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:09.483] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:09.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:09.747] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:09.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:09.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:09.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:09.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:09.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676009,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:09.747] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:09.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:09.994] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:09.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:09.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:09.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:09.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:09.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676009,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:09.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:10.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:10.240] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:10.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:10.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:10.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:10.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:10.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676010,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:10.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:10.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:10.468] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:10.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:10.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:10.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:10.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:10.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676010,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:10.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:10.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:10.711] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:10.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:10.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:10.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:10.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:10.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676010,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:10.715] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:10.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:10.966] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:10.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:10.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:10.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:10.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:10.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676010,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:10.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:11.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:11.209] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:11.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:11.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:11.209] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:11.210] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:11.210] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676011,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:11.210] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:11.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:11.460] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:11.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:11.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:11.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:11.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:11.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676011,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:11.461] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:11.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:11.723] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:11.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:11.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:11.723] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:11.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:11.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676011,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:11.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:11.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:11.963] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:11.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:11.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:11.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:11.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:11.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676011,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:11.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:12.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:12.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:12.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:12.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:12.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:12.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:12.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676012,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:12.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:12.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:12.495] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:12.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:12.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:12.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:12.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:12.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676012,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:12.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:12.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:12.915] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:12.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:12.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:12.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:12.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:12.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676012,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:12.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:13.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:13.169] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:13.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:13.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:13.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:13.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:13.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676013,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:13.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:13.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:13.415] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:13.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:13.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:13.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:13.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:13.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676013,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:13.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:13.721] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676013,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:13.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:13.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:13.956] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:13.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:13.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:13.956] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:13.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:13.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676013,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:13.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:14.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:14.284] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:14.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:14.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:14.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:14.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:14.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676014,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:14.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:14.541] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:14.541] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:14.541] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:14.541] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:14.541] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:14.541] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:14.541] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676014,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:14.541] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:14.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:14.767] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:14.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:14.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:14.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:14.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:14.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676014,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:14.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:15.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:15.072] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:15.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:15.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:15.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:15.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:15.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676015,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:15.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:15.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:15.429] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:15.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:15.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:15.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:15.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:15.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676015,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:15.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:15.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:15.894] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:15.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:15.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:15.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:15.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:15.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676015,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:15.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:16.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:16.153] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:16.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:16.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:16.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:16.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:16.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676016,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:16.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:16.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:16.388] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:16.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:16.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:16.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:16.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:16.389] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676016,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:16.389] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:16.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:16.675] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:16.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:16.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:16.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:16.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:16.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676016,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:16.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:17.180] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:17.180] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:17.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:17.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:17.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:17.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:17.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676017,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:17.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:17.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:17.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:17.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:17.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:17.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:17.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:17.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676017,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:17.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:17.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:17.692] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:17.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:17.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:17.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:17.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:17.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676017,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:17.693] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:17.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:17.976] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:17.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:17.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:17.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:17.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:17.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"\n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676017,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:17.976] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:18.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:18.239] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:18.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:18.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:18.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:18.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:18.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676018,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:18.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:18.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:18.498] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:18.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:18.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:18.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:18.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:18.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676018,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:18.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:18.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:18.759] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:18.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:18.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:18.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:18.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:18.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676018,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:18.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:19.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:19.020] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:19.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:19.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:19.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:19.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:19.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676019,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:19.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:19.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:19.304] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:19.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:19.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:19.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:19.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:19.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676019,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:19.305] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:19.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:19.561] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:19.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:19.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:19.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:19.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:19.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676019,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:19.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:19.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:19.856] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:19.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:19.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:19.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:19.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:19.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676019,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:19.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:20.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:20.181] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:20.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:20.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:20.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:20.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:20.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676020,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:20.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:20.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:20.444] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:20.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:20.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:20.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:20.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:20.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676020,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:20.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:20.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:20.669] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:20.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:20.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:20.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:20.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:20.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676020,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:20.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:20.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:20.914] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:20.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:20.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:20.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:20.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:20.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676020,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:20.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:21.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:21.172] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:21.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:21.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:21.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:21.173] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:21.173] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676021,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:21.173] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:21.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:21.470] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:21.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:21.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:21.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:21.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:21.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676021,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:21.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:21.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:21.780] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:21.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:21.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:21.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:21.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:21.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676021,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:21.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:22.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:22.031] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:22.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:22.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:22.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:22.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:22.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676022,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:22.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:22.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:22.397] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:22.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:22.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:22.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:22.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:22.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676022,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:22.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:22.620] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:22.621] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676022,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:22.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:22.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:22.860] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:22.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:22.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:22.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:22.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:22.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676022,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:22.864] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:23.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:23.153] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:23.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:23.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:23.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:23.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:23.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676023,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:23.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:23.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:23.378] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:23.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:23.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:23.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:23.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:23.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676023,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:23.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:23.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:23.607] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:23.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:23.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:23.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:23.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:23.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676023,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:23.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:23.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:23.830] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:23.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:23.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:23.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:23.831] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:23.831] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676023,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:23.831] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:24.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:24.080] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:24.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:24.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:24.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:24.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:24.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676024,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:24.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:24.411] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:24.411] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:24.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:24.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:24.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:24.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:24.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676024,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:24.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:24.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:24.669] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:24.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:24.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:24.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:24.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:24.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676024,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:24.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:24.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:24.892] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:24.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:24.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:24.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:24.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:24.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676024,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:24.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:25.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:25.140] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:25.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:25.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:25.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:25.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:25.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676025,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:25.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:25.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:25.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:25.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:25.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:25.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:25.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:25.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676025,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:25.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:25.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:25.664] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:25.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:25.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:25.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:25.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:25.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676025,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:25.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:25.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:25.911] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:25.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:25.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:25.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:25.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:25.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676025,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:25.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:26.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:26.151] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676026,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:26.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:26.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:26.420] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:26.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:26.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:26.420] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:26.421] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:26.421] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676026,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:26.421] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:26.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:26.668] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:26.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:26.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:26.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:26.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:26.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676026,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:26.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:26.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:26.908] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:26.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:26.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:26.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:26.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:26.908] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676026,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:26.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:27.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:27.213] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:27.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:27.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:27.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:27.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:27.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676027,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:27.214] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:27.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:27.634] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:27.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:27.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:27.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:27.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:27.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676027,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:27.635] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:27.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:27.924] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:27.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:27.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:27.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:27.924] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:27.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676027,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:27.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:28.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:28.342] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:28.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:28.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:28.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:28.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:28.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676028,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:28.342] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:28.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:28.645] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:28.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:28.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:28.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:28.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:28.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676028,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:28.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:29.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:29.097] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:29.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:29.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:29.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:29.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:29.097] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676029,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:29.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:29.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:29.379] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:29.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:29.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:29.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:29.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:29.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676029,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:29.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:29.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:29.821] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:29.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:29.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:29.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:29.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:29.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676029,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:29.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:30.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:30.162] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:30.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:30.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:30.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:30.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:30.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676030,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:30.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:30.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:30.549] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:30.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:30.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:30.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:30.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:30.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676030,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:30.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:30.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:30.824] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:30.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:30.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:30.824] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:30.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:30.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676030,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:30.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:31.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:31.143] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:31.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:31.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:31.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:31.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:31.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676031,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:31.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:31.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:31.383] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:31.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:31.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:31.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:31.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:31.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676031,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:31.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:31.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:31.624] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:31.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:31.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:31.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:31.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:31.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676031,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:31.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:31.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:31.880] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:31.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:31.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:31.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:31.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:31.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676031,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:31.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:32.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:32.122] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:32.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:32.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:32.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:32.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:32.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676032,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:32.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:32.367] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:32.367] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:32.367] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:32.367] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:32.367] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:32.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:32.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676032,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:32.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:32.607] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:32.607] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:32.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:32.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:32.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:32.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:32.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676032,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:32.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:32.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:32.844] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:32.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:32.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:32.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:32.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:32.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676032,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:32.844] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:33.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:33.218] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:33.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:33.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:33.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:33.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:33.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676033,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:33.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:33.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:33.528] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:33.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:33.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:33.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:33.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:33.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676033,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:33.529] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:33.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:33.829] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:33.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:33.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:33.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:33.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:33.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676033,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:33.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:34.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:34.227] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:34.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:34.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:34.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:34.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:34.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676034,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:34.227] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:34.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:34.448] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:34.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:34.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:34.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:34.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:34.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676034,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:34.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:34.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:34.690] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:34.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:34.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:34.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:34.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:34.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676034,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:34.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:34.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:34.926] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:34.926] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:34.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:34.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:34.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:34.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676034,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:34.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:35.259] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 14:00:35.259] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 14:00:35.259] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 14:00:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 14:00:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 14:00:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-bf0bbdd1-7cbc-45ac-92e5-fbf291feb2ed","choices":[{"index":0,"delta":{"content":"","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727676035,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 14:00:35.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:35.264] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:00:35.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 14:00:35.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: [DONE]


[2024-09-30 14:00:35.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 14:00:35.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2682: Return the chat stream chunk!
[2024-09-30 14:00:35.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: [GGML] End of sequence
[2024-09-30 14:00:35.265] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2465: Clean up the context of the stream work environment.
[2024-09-30 14:00:35.265] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:00:35.265] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  232360.15 ms
[2024-09-30 14:00:35.265] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =     295.31 ms /  1126 runs   (    0.26 ms per token,  3812.99 tokens per second)
[2024-09-30 14:00:35.265] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =  195694.98 ms /  1363 tokens (  143.58 ms per token,     6.96 tokens per second)
[2024-09-30 14:00:35.265] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =  295842.56 ms /  1124 runs   (  263.21 ms per token,     3.80 tokens per second)
[2024-09-30 14:00:35.265] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  528565.95 ms /  2487 tokens
[2024-09-30 14:00:35.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2617: Cleanup done!
[2024-09-30 14:00:35.272] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:59622, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:35.272] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:46890, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:35.272] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:46900, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:35.272] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:46846, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:35.273] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:34086, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:35.273] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:41330, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:35.274] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:35.274] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/index.html
[2024-09-30 14:00:35.275] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:35.275] [info] rag_api_server in src/main.rs:517: response_body_size: 10434
[2024-09-30 14:00:35.275] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:35.275] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:35.276] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:35.276] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/index.html
[2024-09-30 14:00:35.277] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:35.277] [info] rag_api_server in src/main.rs:517: response_body_size: 10434
[2024-09-30 14:00:35.277] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:35.277] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:35.277] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:35.278] [info] rag_api_server in src/main.rs:499: endpoint: /v1/info
[2024-09-30 14:00:35.278] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1677: Handling the coming server info request.
[2024-09-30 14:00:35.278] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1724: Send the server info response.
[2024-09-30 14:00:35.278] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:35.279] [info] rag_api_server in src/main.rs:517: response_body_size: 803
[2024-09-30 14:00:35.279] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:35.279] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:35.280] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 154
[2024-09-30 14:00:35.280] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-30 14:00:35.280] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:00:35.280] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:00:35.281] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-c5d1dca7-c962-4edb-9a21-09815319b54c
[2024-09-30 14:00:35.281] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:00:35.281] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Hello
[2024-09-30 14:00:35.281] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:00:35.281] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:00:35.281] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:00:35.281] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:00:35.281] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:00:35.281] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:00:35.281] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:00:35.282] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:00:35.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:00:35.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:00:35.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:00:35.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:00:35.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:00:35.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:00:35.285] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:00:35.285] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:00:35.285] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:00:35.288] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:00:35.288] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:00:35.288] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:00:35.289] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:00:35.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:00:35.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:00:35.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:00:35.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:00:35.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:00:35.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:00:35.295] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:00:35.295] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:00:35.295] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:00:35.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:00:35.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:00:35.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:00:35.423] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:00:35.423] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  528618.37 ms
[2024-09-30 14:00:35.423] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:00:35.423] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =     122.64 ms /     3 tokens (   40.88 ms per token,    24.46 tokens per second)
[2024-09-30 14:00:35.423] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:00:35.423] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  528617.56 ms /     4 tokens
[2024-09-30 14:00:35.424] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:00:35.425] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11261
[2024-09-30 14:00:35.441] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:00:35.441] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:00:35.441] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:00:35.442] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 3, completion tokens: 0
[2024-09-30 14:00:35.442] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 3 prompt tokens, 0 comletion tokens
[2024-09-30 14:00:35.442] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:00:35.442] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:00:35.442] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:00:35.442] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:00:35.442] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:00:35.442] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:00:35.479] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:35.479] [info] rag_api_server in src/main.rs:499: endpoint: /v1/info
[2024-09-30 14:00:35.479] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1677: Handling the coming server info request.
[2024-09-30 14:00:35.480] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1724: Send the server info response.
[2024-09-30 14:00:35.480] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:35.480] [info] rag_api_server in src/main.rs:517: response_body_size: 803
[2024-09-30 14:00:35.480] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:35.480] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:35.482] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:35.482] [info] rag_api_server in src/main.rs:499: endpoint: /v1/info
[2024-09-30 14:00:35.482] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1677: Handling the coming server info request.
[2024-09-30 14:00:35.483] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1724: Send the server info response.
[2024-09-30 14:00:35.483] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:35.483] [info] rag_api_server in src/main.rs:517: response_body_size: 803
[2024-09-30 14:00:35.483] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:35.483] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:35.484] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:35.485] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:00:35.485] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:00:35.485] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:00:35.485] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-500a627b-fc48-47cc-8ffb-9673d2df93d2
[2024-09-30 14:00:35.485] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:00:35.485] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:00:35.486] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:00:35.486] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:00:35.486] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:00:35.486] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:00:35.486] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:00:35.486] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:00:35.486] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:00:35.486] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:00:35.486] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:00:35.486] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:00:35.486] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:00:35.486] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:00:35.486] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:00:35.486] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:00:35.501] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:00:35.501] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:00:35.501] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:00:35.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:00:35.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:00:35.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:00:35.506] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:00:35.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:00:35.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:00:35.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:00:35.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:00:35.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:00:35.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:00:35.520] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:00:35.520] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:00:35.520] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:00:35.523] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:00:35.523] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:00:35.523] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:00:35.656] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:00:35.656] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  528851.62 ms
[2024-09-30 14:00:35.656] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:00:35.656] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =     130.69 ms /     6 tokens (   21.78 ms per token,    45.91 tokens per second)
[2024-09-30 14:00:35.656] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:00:35.656] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  528851.56 ms /     7 tokens
[2024-09-30 14:00:35.657] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:00:35.657] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:00:35.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:00:35.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:00:35.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:00:35.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:00:35.673] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:00:35.673] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:00:35.673] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:00:35.673] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:00:35.673] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:00:35.673] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:00:35.673] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:00:35.702] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:35.702] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/css/41ab283a84d31b77.css
[2024-09-30 14:00:35.702] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:35.702] [info] rag_api_server in src/main.rs:517: response_body_size: 45475
[2024-09-30 14:00:35.702] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:35.702] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:35.704] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:35.704] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/icons/api-tutorial.png
[2024-09-30 14:00:35.704] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:35.705] [info] rag_api_server in src/main.rs:517: response_body_size: 969
[2024-09-30 14:00:35.705] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:35.705] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:35.711] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:00:35.712] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:00:35.712] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:00:35.712] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-500a627b-fc48-47cc-8ffb-9673d2df93d2
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:00:35.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:00:35.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:00:35.713] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:00:35.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:00:35.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:00:35.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:00:35.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:00:35.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:00:35.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:00:36.191] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:00:36.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:00:36.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:00:36.194] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:00:36.194] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:00:36.194] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:00:36.197] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:00:36.197] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:00:36.197] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-30 14:00:36.197] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 0
[2024-09-30 14:00:36.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:00:36.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:00:36.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:00:36.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:00:36.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:00:36.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:00:36.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:00:36.198] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:00:36.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:00:36.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:00:36.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:00:36.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:00:36.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:00:36.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:00:36.413] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:00:36.413] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:00:36.413] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:00:36.414] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:00:36.415] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:00:36.415] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:00:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:00:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:00:36.425] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:00:36.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:00:36.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:00:36.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:00:36.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:00:36.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:00:36.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:00:36.800] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:00:36.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:00:36.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:00:36.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:00:36.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:00:36.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:00:45.635] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:00:45.636] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:00:45.636] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  534047.18 ms
[2024-09-30 14:00:45.636] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.86 ms /    25 runs   (    0.23 ms per token,  4264.03 tokens per second)
[2024-09-30 14:00:45.636] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3943.12 ms /    24 tokens (  164.30 ms per token,     6.09 tokens per second)
[2024-09-30 14:00:45.636] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4882.77 ms /    24 runs   (  203.45 ms per token,     4.92 tokens per second)
[2024-09-30 14:00:45.636] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  538936.95 ms /    48 tokens
[2024-09-30 14:00:45.638] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:00:45.638] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 114
[2024-09-30 14:00:45.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, a digital assistant designed to help you with information and tasks. How can I assist you today?<|end|>
[2024-09-30 14:00:45.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:00:45.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, a digital assistant designed to help you with information and tasks. How can I assist you today?
[2024-09-30 14:00:45.638] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:00:45.638] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:00:45.638] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:00:45.639] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:00:45.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:00:45.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:00:45.639] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:00:45.639] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:00:45.639] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.639] [info] rag_api_server in src/main.rs:517: response_body_size: 418
[2024-09-30 14:00:45.639] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.639] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.640] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:53066, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:45.641] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.641] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/logo-big.png
[2024-09-30 14:00:45.641] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.641] [info] rag_api_server in src/main.rs:517: response_body_size: 6287
[2024-09-30 14:00:45.641] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.641] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.642] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.642] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/icons/APITutorial.png
[2024-09-30 14:00:45.643] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.643] [info] rag_api_server in src/main.rs:517: response_body_size: 575
[2024-09-30 14:00:45.643] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.643] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.644] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.644] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/webpack-f0ae49128044e751.js
[2024-09-30 14:00:45.645] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.645] [info] rag_api_server in src/main.rs:517: response_body_size: 1592
[2024-09-30 14:00:45.645] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.645] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.647] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.647] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/framework-73b8966a3c579ab0.js
[2024-09-30 14:00:45.647] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.647] [info] rag_api_server in src/main.rs:517: response_body_size: 141074
[2024-09-30 14:00:45.647] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.647] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.651] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.651] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/main-6260d066cf2cd7b1.js
[2024-09-30 14:00:45.651] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.651] [info] rag_api_server in src/main.rs:517: response_body_size: 90428
[2024-09-30 14:00:45.651] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.651] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.654] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:34454, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:45.658] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.658] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/pages/_app-77033f8e040fdfcc.js
[2024-09-30 14:00:45.658] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.658] [info] rag_api_server in src/main.rs:517: response_body_size: 115704
[2024-09-30 14:00:45.658] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.658] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.659] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.659] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/175675d1-5c66df25ff141d62.js
[2024-09-30 14:00:45.660] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.660] [info] rag_api_server in src/main.rs:517: response_body_size: 267018
[2024-09-30 14:00:45.660] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.660] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.667] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.667] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/737-952dc72c8d5e14c2.js
[2024-09-30 14:00:45.667] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.667] [info] rag_api_server in src/main.rs:517: response_body_size: 1007606
[2024-09-30 14:00:45.668] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.668] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.672] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.672] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/pages/index-ff5f66d3c316acef.js
[2024-09-30 14:00:45.672] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.672] [info] rag_api_server in src/main.rs:517: response_body_size: 78983
[2024-09-30 14:00:45.672] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.672] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.680] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.680] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/AV-ppCFdiUch5Aa5WIivp/_buildManifest.js
[2024-09-30 14:00:45.681] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.681] [info] rag_api_server in src/main.rs:517: response_body_size: 367
[2024-09-30 14:00:45.681] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.681] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.696] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:45.696] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/AV-ppCFdiUch5Aa5WIivp/_ssgManifest.js
[2024-09-30 14:00:45.696] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:45.696] [info] rag_api_server in src/main.rs:517: response_body_size: 77
[2024-09-30 14:00:45.696] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:45.697] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:45.952] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:34456, local_addr: 0.0.0.0:8080
[2024-09-30 14:00:46.101] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:46.101] [info] rag_api_server in src/main.rs:499: endpoint: /v1/models
[2024-09-30 14:00:46.102] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:24: Handling the coming model list request.
[2024-09-30 14:00:46.102] [info] llama_core::models in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/models.rs:9: List models
[2024-09-30 14:00:46.102] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:71: Send the model list response.
[2024-09-30 14:00:46.102] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:46.102] [info] rag_api_server in src/main.rs:517: response_body_size: 219
[2024-09-30 14:00:46.102] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:46.102] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:46.109] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:46.109] [info] rag_api_server in src/main.rs:499: endpoint: /favicon.ico
[2024-09-30 14:00:46.110] [error] rag_api_server in src/main.rs:524: response_version: HTTP/1.1
[2024-09-30 14:00:46.110] [error] rag_api_server in src/main.rs:526: response_body_size: 0
[2024-09-30 14:00:46.110] [error] rag_api_server in src/main.rs:528: response_status: 404
[2024-09-30 14:00:46.110] [error] rag_api_server in src/main.rs:530: response_is_success: false
[2024-09-30 14:00:46.110] [error] rag_api_server in src/main.rs:532: response_is_client_error: true
[2024-09-30 14:00:46.110] [error] rag_api_server in src/main.rs:534: response_is_server_error: false
[2024-09-30 14:00:46.365] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:46.365] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/media/MonaspaceNeon.f8ddff7e.ttf
[2024-09-30 14:00:46.366] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:46.366] [info] rag_api_server in src/main.rs:517: response_body_size: 411656
[2024-09-30 14:00:46.366] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:46.366] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:00:46.503] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:00:46.503] [info] rag_api_server in src/main.rs:499: endpoint: /config_pub.json
[2024-09-30 14:00:46.503] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:00:46.503] [info] rag_api_server in src/main.rs:517: response_body_size: 1539
[2024-09-30 14:00:46.503] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:00:46.503] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:01:37.393] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:49200, local_addr: 0.0.0.0:8080
[2024-09-30 14:01:37.394] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:01:37.394] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:01:37.394] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:01:37.394] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:01:37.394] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-97c188b9-c85f-4aa4-b21e-33e149cb485b
[2024-09-30 14:01:37.394] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:01:37.394] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:01:37.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:01:37.394] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:01:37.394] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:01:37.394] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:01:37.394] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:01:37.395] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:01:37.395] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:01:37.395] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:01:37.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:01:37.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:01:37.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:01:37.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:01:37.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:01:37.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:01:37.397] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:01:37.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:01:37.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:01:37.399] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:01:37.399] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:01:37.399] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:01:37.400] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:01:37.400] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:01:37.400] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:01:37.400] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:01:37.400] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:01:37.400] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:01:37.400] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:01:37.404] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:01:37.404] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:01:37.404] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:01:37.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:01:37.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:01:37.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:01:37.430] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:01:37.430] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  590625.52 ms
[2024-09-30 14:01:37.430] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:01:37.430] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.57 ms /     6 tokens (    3.93 ms per token,   254.58 tokens per second)
[2024-09-30 14:01:37.430] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:01:37.430] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  590625.56 ms /     7 tokens
[2024-09-30 14:01:37.430] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:01:37.430] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:01:37.437] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:01:37.437] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:01:37.437] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:01:37.437] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:01:37.437] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:01:37.437] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:01:37.437] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:01:37.437] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:01:37.437] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:01:37.437] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:01:37.437] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:01:37.455] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:01:37.455] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:01:37.455] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:01:37.455] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:01:37.455] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:01:37.455] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:01:37.455] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:01:37.455] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:01:37.455] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-97c188b9-c85f-4aa4-b21e-33e149cb485b
[2024-09-30 14:01:37.456] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:01:37.456] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:01:37.456] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:01:37.456] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:01:37.456] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:01:37.456] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:01:37.456] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:01:37.456] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:01:37.456] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:01:37.456] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:01:37.456] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:01:37.456] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:01:37.914] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:01:37.914] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:01:37.914] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:01:37.916] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:01:37.916] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:01:37.916] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:01:37.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:01:37.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:01:37.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:01:37.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:01:37.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:01:37.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:01:37.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:01:37.919] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:01:37.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:01:37.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:01:37.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:01:37.920] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:01:37.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:01:37.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:01:37.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:01:37.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:01:37.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:01:37.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:01:38.089] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:01:38.089] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:01:38.089] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:01:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:01:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:01:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:01:38.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:01:38.093] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:01:38.093] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:01:38.093] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:01:38.093] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:01:38.093] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:01:38.093] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:01:38.093] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:01:38.093] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:01:38.261] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:01:38.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:01:38.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:01:38.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:01:38.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:01:38.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:01:44.012] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:01:44.012] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:01:44.012] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  594278.07 ms
[2024-09-30 14:01:44.012] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.37 ms /    21 runs   (    0.21 ms per token,  4808.79 tokens per second)
[2024-09-30 14:01:44.012] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2714.62 ms /    24 tokens (  113.11 ms per token,     8.84 tokens per second)
[2024-09-30 14:01:44.012] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3029.95 ms /    20 runs   (  151.50 ms per token,     6.60 tokens per second)
[2024-09-30 14:01:44.012] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  597312.95 ms /    44 tokens
[2024-09-30 14:01:44.014] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:01:44.014] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 74
[2024-09-30 14:01:44.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:01:44.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:01:44.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:01:44.015] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:01:44.015] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:01:44.015] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:01:44.015] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:01:44.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:01:44.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:01:44.015] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:01:44.015] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:01:44.015] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:01:44.015] [info] rag_api_server in src/main.rs:517: response_body_size: 378
[2024-09-30 14:01:44.015] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:01:44.016] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:02:34.041] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:35398, local_addr: 0.0.0.0:8080
[2024-09-30 14:02:34.042] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:02:34.042] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:02:34.042] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:02:34.042] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:02:34.043] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-d79e1a8b-6602-4c29-b004-67882295fc21
[2024-09-30 14:02:34.043] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:02:34.043] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:02:34.043] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:02:34.043] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:02:34.043] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:02:34.044] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:02:34.044] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:02:34.044] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:02:34.044] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:02:34.044] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:02:34.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:02:34.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:02:34.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:02:34.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:02:34.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:02:34.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:02:34.046] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:02:34.046] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:02:34.046] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:02:34.048] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:02:34.048] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:02:34.051] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:02:34.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:02:34.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:02:34.052] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:02:34.052] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:02:34.052] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:02:34.075] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:02:34.075] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  647270.88 ms
[2024-09-30 14:02:34.075] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:02:34.075] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.38 ms /     6 tokens (    3.73 ms per token,   268.12 tokens per second)
[2024-09-30 14:02:34.076] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:02:34.076] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  647270.56 ms /     7 tokens
[2024-09-30 14:02:34.076] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:02:34.076] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:02:34.081] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:02:34.081] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:02:34.082] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:02:34.082] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:02:34.082] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:02:34.082] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:02:34.082] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:02:34.082] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:02:34.082] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:02:34.082] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:02:34.082] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:02:34.100] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:02:34.109] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:02:34.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:02:34.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:02:34.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:02:34.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:02:34.110] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:02:34.110] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:02:34.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-d79e1a8b-6602-4c29-b004-67882295fc21
[2024-09-30 14:02:34.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:02:34.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:02:34.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:02:34.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:02:34.110] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:02:34.110] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:02:34.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:02:34.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:02:34.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:02:34.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:02:34.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:02:34.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:02:34.588] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:02:34.588] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:02:34.588] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:02:34.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:02:34.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:02:34.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:02:34.593] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:02:34.593] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:02:34.593] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:02:34.593] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:02:34.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:02:34.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:02:34.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:02:34.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:02:34.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:02:34.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:02:34.594] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:02:34.594] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:02:34.594] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:02:34.594] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:02:34.594] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:02:34.594] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:02:34.594] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:02:34.594] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:02:34.761] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:02:34.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:02:34.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:02:34.762] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:02:34.762] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:02:34.762] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:02:34.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:02:34.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:02:34.765] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:02:34.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:02:34.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:02:34.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:02:34.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:02:34.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:02:34.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:02:34.930] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:02:34.930] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:02:34.930] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:02:34.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:02:34.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:02:34.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:02:40.645] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:02:40.645] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:02:40.645] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  650795.25 ms
[2024-09-30 14:02:40.645] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.12 ms /    20 runs   (    0.26 ms per token,  3907.01 tokens per second)
[2024-09-30 14:02:40.645] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2562.16 ms /    24 tokens (  106.76 ms per token,     9.37 tokens per second)
[2024-09-30 14:02:40.645] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3144.17 ms /    19 runs   (  165.48 ms per token,     6.04 tokens per second)
[2024-09-30 14:02:40.645] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  653945.95 ms /    43 tokens
[2024-09-30 14:02:40.648] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:02:40.648] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-30 14:02:40.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:02:40.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:02:40.648] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence developed by Microsoft. How can I assist you today?
[2024-09-30 14:02:40.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:02:40.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:02:40.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:02:40.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:02:40.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:02:40.649] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:02:40.649] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:02:40.649] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:02:40.650] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:02:40.650] [info] rag_api_server in src/main.rs:517: response_body_size: 400
[2024-09-30 14:02:40.650] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:02:40.650] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:03:30.674] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:52164, local_addr: 0.0.0.0:8080
[2024-09-30 14:03:30.675] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:03:30.675] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:03:30.675] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:03:30.675] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:03:30.675] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-e2e33cf2-2b41-40e8-84b8-568ea93d5e35
[2024-09-30 14:03:30.675] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:03:30.675] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:03:30.675] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:03:30.675] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:03:30.675] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:03:30.675] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:03:30.675] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:03:30.676] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:03:30.676] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:03:30.676] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:03:30.676] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:03:30.676] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:03:30.676] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:03:30.676] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:03:30.676] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:03:30.676] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:03:30.677] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:03:30.677] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:03:30.677] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:03:30.680] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:03:30.680] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:03:30.680] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:03:30.680] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:03:30.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:03:30.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:03:30.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:03:30.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:03:30.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:03:30.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:03:30.683] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:03:30.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:03:30.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:03:30.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:03:30.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:03:30.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:03:30.706] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:03:30.706] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  703901.81 ms
[2024-09-30 14:03:30.706] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:03:30.706] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      20.80 ms /     6 tokens (    3.47 ms per token,   288.53 tokens per second)
[2024-09-30 14:03:30.706] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:03:30.706] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  703901.56 ms /     7 tokens
[2024-09-30 14:03:30.707] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:03:30.707] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:03:30.712] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:03:30.712] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:03:30.712] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:03:30.712] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:03:30.712] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:03:30.712] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:03:30.712] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:03:30.712] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:03:30.712] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:03:30.712] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:03:30.713] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:03:30.729] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:03:30.729] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:03:30.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:03:30.729] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:03:30.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:03:30.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:03:30.730] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:03:30.730] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:03:30.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-e2e33cf2-2b41-40e8-84b8-568ea93d5e35
[2024-09-30 14:03:30.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:03:30.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:03:30.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:03:30.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:03:30.730] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:03:30.730] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:03:30.730] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:03:30.730] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:03:30.730] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:03:30.730] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:03:30.730] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:03:30.730] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:03:31.176] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:03:31.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:03:31.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:03:31.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:03:31.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:03:31.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:03:31.182] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:03:31.182] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:03:31.182] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:03:31.182] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:03:31.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:03:31.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:03:31.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:03:31.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:03:31.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:03:31.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:03:31.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:03:31.182] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:03:31.182] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:03:31.182] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:03:31.182] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:03:31.182] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:03:31.182] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:03:31.182] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:03:31.342] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:03:31.342] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:03:31.342] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:03:31.343] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:03:31.343] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:03:31.343] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:03:31.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:03:31.347] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:03:31.347] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:03:31.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:03:31.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:03:31.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:03:31.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:03:31.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:03:31.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:03:31.508] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:03:31.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:03:31.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:03:31.509] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:03:31.509] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:03:31.509] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:03:37.029] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:03:37.029] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:03:37.029] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  707434.46 ms
[2024-09-30 14:03:37.029] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       3.99 ms /    20 runs   (    0.20 ms per token,  5008.77 tokens per second)
[2024-09-30 14:03:37.029] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2623.61 ms /    24 tokens (  109.32 ms per token,     9.15 tokens per second)
[2024-09-30 14:03:37.029] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2891.14 ms /    19 runs   (  152.17 ms per token,     6.57 tokens per second)
[2024-09-30 14:03:37.029] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  710329.95 ms /    43 tokens
[2024-09-30 14:03:37.034] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:03:37.034] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 94
[2024-09-30 14:03:37.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence created by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:03:37.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:03:37.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence created by Microsoft. How can I assist you today?
[2024-09-30 14:03:37.034] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:03:37.034] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:03:37.034] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:03:37.034] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:03:37.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:03:37.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:03:37.034] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:03:37.035] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:03:37.035] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:03:37.035] [info] rag_api_server in src/main.rs:517: response_body_size: 398
[2024-09-30 14:03:37.035] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:03:37.035] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:04:27.060] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:35510, local_addr: 0.0.0.0:8080
[2024-09-30 14:04:27.061] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:04:27.061] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:04:27.061] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:04:27.061] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:04:27.061] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-35b5cb44-a3ed-4f54-95c7-80c7894cf710
[2024-09-30 14:04:27.061] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:04:27.062] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:04:27.062] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:04:27.062] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:04:27.062] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:04:27.062] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:04:27.062] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:04:27.062] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:04:27.062] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:04:27.062] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:04:27.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:04:27.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:04:27.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:04:27.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:04:27.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:04:27.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:04:27.064] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:04:27.064] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:04:27.064] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:04:27.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:04:27.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:04:27.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:04:27.067] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:04:27.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:04:27.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:04:27.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:04:27.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:04:27.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:04:27.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:04:27.070] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:04:27.070] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:04:27.070] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:04:27.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:04:27.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:04:27.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:04:27.095] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:04:27.095] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  760289.96 ms
[2024-09-30 14:04:27.095] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:04:27.095] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.79 ms /     6 tokens (    3.63 ms per token,   275.36 tokens per second)
[2024-09-30 14:04:27.095] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:04:27.095] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  760289.56 ms /     7 tokens
[2024-09-30 14:04:27.095] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:04:27.095] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:04:27.102] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:04:27.102] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:04:27.102] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:04:27.102] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:04:27.102] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:04:27.102] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:04:27.102] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:04:27.103] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:04:27.103] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:04:27.103] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:04:27.103] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:04:27.120] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:04:27.121] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:04:27.121] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:04:27.121] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-35b5cb44-a3ed-4f54-95c7-80c7894cf710
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:04:27.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:04:27.121] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:04:27.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:04:27.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:04:27.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:04:27.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:04:27.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:04:27.121] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:04:27.627] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:04:27.627] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:04:27.627] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:04:27.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:04:27.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:04:27.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:04:27.633] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:04:27.633] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:04:27.633] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:04:27.633] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:04:27.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:04:27.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:04:27.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:04:27.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:04:27.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:04:27.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:04:27.634] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:04:27.634] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:04:27.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:04:27.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:04:27.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:04:27.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:04:27.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:04:27.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:04:27.819] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:04:27.819] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:04:27.819] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:04:27.819] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:04:27.819] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:04:27.819] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:04:27.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:04:27.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:04:27.822] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:04:27.822] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:04:27.822] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:04:27.822] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:04:27.822] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:04:27.822] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:04:27.822] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:04:28.017] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:04:28.017] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:04:28.017] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:04:28.018] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:04:28.018] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:04:28.018] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:04:35.909] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:04:35.909] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:04:35.909] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  764220.66 ms
[2024-09-30 14:04:35.909] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.16 ms /    29 runs   (    0.21 ms per token,  4705.50 tokens per second)
[2024-09-30 14:04:35.909] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2901.01 ms /    24 tokens (  120.88 ms per token,     8.27 tokens per second)
[2024-09-30 14:04:35.909] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4982.62 ms /    28 runs   (  177.95 ms per token,     5.62 tokens per second)
[2024-09-30 14:04:35.909] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  769210.95 ms /    52 tokens
[2024-09-30 14:04:35.912] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:04:35.912] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 136
[2024-09-30 14:04:35.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, a chatbot developed by Microsoft to assist users with various tasks and provide information. How may I help you today?<|end|>
[2024-09-30 14:04:35.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:04:35.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, a chatbot developed by Microsoft to assist users with various tasks and provide information. How may I help you today?
[2024-09-30 14:04:35.912] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:04:35.912] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:04:35.912] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:04:35.913] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 29
[2024-09-30 14:04:35.913] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 29
[2024-09-30 14:04:35.913] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:04:35.913] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:04:35.913] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:04:35.913] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:04:35.913] [info] rag_api_server in src/main.rs:517: response_body_size: 440
[2024-09-30 14:04:35.913] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:04:35.913] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:05:25.940] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:47826, local_addr: 0.0.0.0:8080
[2024-09-30 14:05:25.940] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:05:25.940] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:05:25.940] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:05:25.940] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:05:25.941] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-afb94936-fea0-4161-97cc-0ccc7064036d
[2024-09-30 14:05:25.941] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:05:25.941] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:05:25.941] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:05:25.941] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:05:25.941] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:05:25.941] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:05:25.941] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:05:25.941] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:05:25.941] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:05:25.941] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:05:25.941] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:05:25.941] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:05:25.941] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:05:25.941] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:05:25.941] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:05:25.941] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:05:25.943] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:05:25.943] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:05:25.943] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:05:25.946] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:05:25.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:05:25.949] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:05:25.949] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:05:25.949] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:05:25.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:05:25.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:05:25.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:05:25.976] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:05:25.976] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  819171.17 ms
[2024-09-30 14:05:25.976] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:05:25.976] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.57 ms /     6 tokens (    3.93 ms per token,   254.57 tokens per second)
[2024-09-30 14:05:25.976] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:05:25.976] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  819170.56 ms /     7 tokens
[2024-09-30 14:05:25.976] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:05:25.976] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:05:25.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:05:25.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:05:25.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:05:25.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:05:25.982] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:05:25.982] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:05:25.982] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:05:25.982] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:05:25.982] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:05:25.982] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:05:25.983] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:05:26.001] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:05:26.002] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:05:26.002] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:05:26.002] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-afb94936-fea0-4161-97cc-0ccc7064036d
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:05:26.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:05:26.002] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:05:26.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:05:26.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:05:26.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:05:26.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:05:26.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:05:26.002] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:05:26.518] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:05:26.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:05:26.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:05:26.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:05:26.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:05:26.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:05:26.524] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:05:26.524] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:05:26.524] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:05:26.524] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 29
[2024-09-30 14:05:26.524] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:05:26.524] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:05:26.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:05:26.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:05:26.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:05:26.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:05:26.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:05:26.525] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:05:26.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:05:26.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:05:26.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:05:26.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:05:26.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:05:26.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:05:26.761] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:05:26.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:05:26.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:05:26.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:05:26.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:05:26.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:05:26.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:05:26.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:05:26.765] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:05:26.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:05:26.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:05:26.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:05:26.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:05:26.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:05:26.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:05:26.947] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:05:26.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:05:26.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:05:26.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:05:26.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:05:26.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:05:34.349] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:05:34.349] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:05:34.349] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  822958.08 ms
[2024-09-30 14:05:34.349] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.67 ms /    27 runs   (    0.25 ms per token,  4047.98 tokens per second)
[2024-09-30 14:05:34.349] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2707.62 ms /    24 tokens (  112.82 ms per token,     8.86 tokens per second)
[2024-09-30 14:05:34.349] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4684.02 ms /    26 runs   (  180.15 ms per token,     5.55 tokens per second)
[2024-09-30 14:05:34.349] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  827649.95 ms /    50 tokens
[2024-09-30 14:05:34.351] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:05:34.351] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 116
[2024-09-30 14:05:34.351] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft to assist users with information and tasks. How can I help you today?<|end|>
[2024-09-30 14:05:34.351] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:05:34.351] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft to assist users with information and tasks. How can I help you today?
[2024-09-30 14:05:34.351] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:05:34.351] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:05:34.351] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:05:34.351] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 27
[2024-09-30 14:05:34.351] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 27
[2024-09-30 14:05:34.351] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:05:34.352] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:05:34.352] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:05:34.352] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:05:34.352] [info] rag_api_server in src/main.rs:517: response_body_size: 420
[2024-09-30 14:05:34.352] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:05:34.352] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:06:24.376] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:43616, local_addr: 0.0.0.0:8080
[2024-09-30 14:06:24.377] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:06:24.377] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:06:24.377] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:06:24.377] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:06:24.377] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-80d07413-3afe-425e-84dd-c15d880a414c
[2024-09-30 14:06:24.377] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:06:24.378] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:06:24.378] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:06:24.378] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:06:24.378] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:06:24.378] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:06:24.378] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:06:24.378] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:06:24.378] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:06:24.378] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:06:24.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:06:24.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:06:24.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:06:24.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:06:24.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:06:24.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:06:24.379] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:06:24.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:06:24.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:06:24.381] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:06:24.381] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:06:24.381] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:06:24.382] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:06:24.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:06:24.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:06:24.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:06:24.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:06:24.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:06:24.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:06:24.384] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:06:24.384] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:06:24.384] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:06:24.387] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:06:24.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:06:24.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:06:24.421] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:06:24.421] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  877616.14 ms
[2024-09-30 14:06:24.421] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:06:24.421] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      30.77 ms /     6 tokens (    5.13 ms per token,   195.03 tokens per second)
[2024-09-30 14:06:24.421] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:06:24.421] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  877615.56 ms /     7 tokens
[2024-09-30 14:06:24.421] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:06:24.421] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:06:24.427] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:06:24.427] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:06:24.427] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:06:24.428] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:06:24.428] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:06:24.428] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:06:24.428] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:06:24.428] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:06:24.428] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:06:24.428] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:06:24.428] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:06:24.447] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:06:24.448] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:06:24.448] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:06:24.448] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-80d07413-3afe-425e-84dd-c15d880a414c
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:06:24.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:06:24.448] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:06:24.448] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:06:24.448] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:06:24.448] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:06:24.448] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:06:24.448] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:06:24.448] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:06:24.959] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:06:24.959] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:06:24.959] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:06:24.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:06:24.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:06:24.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:06:24.967] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:06:24.967] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:06:24.967] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:06:24.967] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 27
[2024-09-30 14:06:24.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:06:24.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:06:24.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:06:24.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:06:24.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:06:24.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:06:24.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:06:24.967] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:06:24.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:06:24.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:06:24.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:06:24.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:06:24.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:06:24.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:06:25.151] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:06:25.151] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:06:25.151] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:06:25.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:06:25.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:06:25.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:06:25.159] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:06:25.160] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:06:25.160] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:06:25.160] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:06:25.160] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:06:25.160] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:06:25.160] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:06:25.160] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:06:25.160] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:06:25.371] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:06:25.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:06:25.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:06:25.372] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:06:25.372] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:06:25.372] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:06:30.862] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:06:30.862] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:06:30.862] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  881296.94 ms
[2024-09-30 14:06:30.862] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.39 ms /    20 runs   (    0.22 ms per token,  4554.77 tokens per second)
[2024-09-30 14:06:30.862] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2622.58 ms /    24 tokens (  109.27 ms per token,     9.15 tokens per second)
[2024-09-30 14:06:30.862] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2861.76 ms /    19 runs   (  150.62 ms per token,     6.64 tokens per second)
[2024-09-30 14:06:30.862] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  884163.95 ms /    43 tokens
[2024-09-30 14:06:30.865] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:06:30.865] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 14:06:30.865] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:06:30.865] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:06:30.865] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:06:30.865] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:06:30.865] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:06:30.865] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:06:30.866] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:06:30.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:06:30.866] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:06:30.866] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:06:30.866] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:06:30.866] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:06:30.866] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 14:06:30.866] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:06:30.866] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:07:20.889] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:52970, local_addr: 0.0.0.0:8080
[2024-09-30 14:07:20.890] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:07:20.890] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:07:20.890] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:07:20.890] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:07:20.890] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-80aeaf00-5180-4f80-bf9c-eec2a698a272
[2024-09-30 14:07:20.890] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:07:20.890] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:07:20.890] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:07:20.890] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:07:20.890] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:07:20.890] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:07:20.891] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:07:20.891] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:07:20.891] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:07:20.891] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:07:20.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:07:20.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:07:20.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:07:20.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:07:20.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:07:20.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:07:20.892] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:07:20.892] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:07:20.892] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:07:20.895] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:07:20.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:07:20.899] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:07:20.899] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:07:20.899] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:07:20.901] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:07:20.901] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:07:20.901] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:07:20.925] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:07:20.925] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  934120.85 ms
[2024-09-30 14:07:20.925] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:07:20.925] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.94 ms /     6 tokens (    3.99 ms per token,   250.64 tokens per second)
[2024-09-30 14:07:20.925] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:07:20.925] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  934120.56 ms /     7 tokens
[2024-09-30 14:07:20.926] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:07:20.926] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:07:20.932] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:07:20.932] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:07:20.932] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:07:20.932] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:07:20.932] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:07:20.932] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:07:20.932] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:07:20.932] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:07:20.932] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:07:20.932] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:07:20.932] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:07:20.950] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:07:20.950] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:07:20.950] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:07:20.950] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-80aeaf00-5180-4f80-bf9c-eec2a698a272
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:07:20.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:07:20.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:07:20.951] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:07:20.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:07:20.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:07:20.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:07:20.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:07:20.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:07:20.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:07:21.430] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:07:21.430] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:07:21.430] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:07:21.433] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:07:21.433] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:07:21.433] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:07:21.435] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:07:21.436] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:07:21.436] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:07:21.436] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:07:21.436] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:07:21.436] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:07:21.436] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:07:21.436] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:07:21.436] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:07:21.436] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:07:21.436] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:07:21.436] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:07:21.436] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:07:21.436] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:07:21.436] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:07:21.436] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:07:21.436] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:07:21.436] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:07:21.634] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:07:21.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:07:21.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:07:21.635] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:07:21.635] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:07:21.635] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:07:21.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:07:21.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:07:21.637] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:07:21.637] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:07:21.637] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:07:21.637] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:07:21.637] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:07:21.637] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:07:21.637] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:07:21.839] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:07:21.839] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:07:21.839] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:07:21.840] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:07:21.840] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:07:21.840] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:07:26.937] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:07:26.937] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:07:26.937] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  937938.28 ms
[2024-09-30 14:07:26.937] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       3.18 ms /    14 runs   (    0.23 ms per token,  4405.29 tokens per second)
[2024-09-30 14:07:26.937] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2796.78 ms /    24 tokens (  116.53 ms per token,     8.58 tokens per second)
[2024-09-30 14:07:26.937] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2296.74 ms /    13 runs   (  176.67 ms per token,     5.66 tokens per second)
[2024-09-30 14:07:26.937] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  940238.95 ms /    37 tokens
[2024-09-30 14:07:26.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:07:26.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 64
[2024-09-30 14:07:26.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, a large language model developed by Microsoft.<|end|>
[2024-09-30 14:07:26.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:07:26.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, a large language model developed by Microsoft.
[2024-09-30 14:07:26.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:07:26.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:07:26.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:07:26.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 14
[2024-09-30 14:07:26.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 14
[2024-09-30 14:07:26.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:07:26.941] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:07:26.941] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:07:26.941] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:07:26.941] [info] rag_api_server in src/main.rs:517: response_body_size: 368
[2024-09-30 14:07:26.941] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:07:26.941] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:08:16.966] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:35292, local_addr: 0.0.0.0:8080
[2024-09-30 14:08:16.967] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:08:16.967] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:08:16.967] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:08:16.967] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:08:16.967] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-e0f9c589-bb7c-4064-83dc-4753213db77e
[2024-09-30 14:08:16.967] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:08:16.967] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:08:16.967] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:08:16.967] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:08:16.967] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:08:16.967] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:08:16.967] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:08:16.967] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:08:16.967] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:08:16.967] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:08:16.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:08:16.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:08:16.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:08:16.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:08:16.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:08:16.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:08:16.969] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:08:16.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:08:16.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:08:16.970] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:08:16.970] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:08:16.970] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:08:16.971] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:08:16.971] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:08:16.971] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:08:16.971] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:08:16.971] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:08:16.971] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:08:16.971] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:08:16.973] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:08:16.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:08:16.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:08:16.975] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:08:16.975] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:08:16.975] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:08:16.997] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:08:16.997] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  990192.94 ms
[2024-09-30 14:08:16.997] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:08:16.997] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.78 ms /     6 tokens (    3.63 ms per token,   275.46 tokens per second)
[2024-09-30 14:08:16.998] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:08:16.998] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  990192.56 ms /     7 tokens
[2024-09-30 14:08:16.998] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:08:16.998] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:08:17.003] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:08:17.003] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:08:17.003] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:08:17.003] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:08:17.003] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:08:17.003] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:08:17.003] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:08:17.003] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:08:17.003] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:08:17.003] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:08:17.003] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:08:17.022] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:08:17.022] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:08:17.022] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:08:17.022] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-e0f9c589-bb7c-4064-83dc-4753213db77e
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:08:17.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:08:17.022] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:08:17.022] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:08:17.022] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:08:17.022] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:08:17.022] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:08:17.022] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:08:17.022] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:08:17.488] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:08:17.488] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:08:17.488] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:08:17.492] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:08:17.492] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:08:17.492] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:08:17.495] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:08:17.495] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:08:17.495] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:08:17.495] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 14
[2024-09-30 14:08:17.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:08:17.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:08:17.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:08:17.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:08:17.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:08:17.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:08:17.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:08:17.496] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:08:17.496] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:08:17.496] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:08:17.496] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:08:17.496] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:08:17.496] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:08:17.496] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:08:17.672] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:08:17.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:08:17.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:08:17.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:08:17.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:08:17.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:08:17.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:08:17.675] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:08:17.675] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:08:17.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:08:17.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:08:17.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:08:17.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:08:17.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:08:17.675] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:08:17.852] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:08:17.852] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:08:17.852] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:08:17.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:08:17.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:08:17.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:08:23.601] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:08:23.601] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:08:23.601] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  993874.80 ms
[2024-09-30 14:08:23.601] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.46 ms /    20 runs   (    0.22 ms per token,  4488.33 tokens per second)
[2024-09-30 14:08:23.601] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2720.13 ms /    24 tokens (  113.34 ms per token,     8.82 tokens per second)
[2024-09-30 14:08:23.601] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3022.78 ms /    19 runs   (  159.09 ms per token,     6.29 tokens per second)
[2024-09-30 14:08:23.601] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  996902.95 ms /    43 tokens
[2024-09-30 14:08:23.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:08:23.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 14:08:23.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:08:23.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:08:23.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:08:23.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:08:23.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:08:23.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:08:23.605] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:08:23.605] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:08:23.605] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:08:23.605] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:08:23.605] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:08:23.605] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:08:23.605] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 14:08:23.605] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:08:23.605] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:09:13.629] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:46396, local_addr: 0.0.0.0:8080
[2024-09-30 14:09:13.630] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:09:13.630] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:09:13.630] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:09:13.630] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:09:13.630] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-a3c2f254-1138-44de-afbd-fc779e0b14e4
[2024-09-30 14:09:13.630] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:09:13.630] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:09:13.630] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:09:13.630] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:09:13.630] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:09:13.630] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:09:13.630] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:09:13.630] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:09:13.630] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:09:13.630] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:09:13.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:09:13.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:09:13.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:09:13.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:09:13.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:09:13.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:09:13.632] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:09:13.632] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:09:13.632] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:09:13.634] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:09:13.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:09:13.637] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:09:13.637] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:09:13.637] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:09:13.640] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:09:13.640] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:09:13.640] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:09:13.666] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:09:13.666] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1046861.04 ms
[2024-09-30 14:09:13.666] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:09:13.666] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      24.10 ms /     6 tokens (    4.02 ms per token,   248.98 tokens per second)
[2024-09-30 14:09:13.666] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:09:13.666] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1046860.56 ms /     7 tokens
[2024-09-30 14:09:13.666] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:09:13.666] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:09:13.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:09:13.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:09:13.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:09:13.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:09:13.671] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:09:13.671] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:09:13.672] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:09:13.672] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:09:13.672] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:09:13.672] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:09:13.672] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:09:13.690] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:09:13.690] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:09:13.690] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:09:13.690] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-a3c2f254-1138-44de-afbd-fc779e0b14e4
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:09:13.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:09:13.690] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:09:13.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:09:13.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:09:13.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:09:13.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:09:13.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:09:13.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:09:14.164] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:09:14.164] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:09:14.164] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:09:14.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:09:14.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:09:14.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:09:14.170] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:09:14.170] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:09:14.170] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:09:14.170] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:09:14.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:09:14.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:09:14.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:09:14.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:09:14.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:09:14.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:09:14.171] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:09:14.171] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:09:14.171] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:09:14.171] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:09:14.171] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:09:14.171] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:09:14.171] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:09:14.171] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:09:14.391] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:09:14.391] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:09:14.391] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:09:14.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:09:14.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:09:14.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:09:14.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:09:14.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:09:14.395] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:09:14.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:09:14.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:09:14.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:09:14.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:09:14.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:09:14.395] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:09:14.569] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:09:14.569] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:09:14.569] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:09:14.570] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:09:14.570] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:09:14.570] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:09:20.487] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:09:20.487] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:09:20.487] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1050707.30 ms
[2024-09-30 14:09:20.487] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.23 ms /    21 runs   (    0.20 ms per token,  4959.85 tokens per second)
[2024-09-30 14:09:20.487] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2835.69 ms /    24 tokens (  118.15 ms per token,     8.46 tokens per second)
[2024-09-30 14:09:20.487] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3075.68 ms /    20 runs   (  153.78 ms per token,     6.50 tokens per second)
[2024-09-30 14:09:20.487] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1053787.95 ms /    44 tokens
[2024-09-30 14:09:20.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:09:20.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 106
[2024-09-30 14:09:20.489] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence designed to interact and assist users with various inquiries.<|end|>
[2024-09-30 14:09:20.489] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:09:20.489] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence designed to interact and assist users with various inquiries.
[2024-09-30 14:09:20.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:09:20.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:09:20.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:09:20.490] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:09:20.490] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:09:20.490] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:09:20.490] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:09:20.490] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:09:20.490] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:09:20.490] [info] rag_api_server in src/main.rs:517: response_body_size: 410
[2024-09-30 14:09:20.490] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:09:20.490] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:10:10.515] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:35166, local_addr: 0.0.0.0:8080
[2024-09-30 14:10:10.516] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:10:10.516] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:10:10.516] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:10:10.516] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:10:10.517] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-699e928f-1129-4070-845d-d7b2aefa91d3
[2024-09-30 14:10:10.517] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:10:10.517] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:10:10.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:10:10.517] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:10:10.517] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:10:10.517] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:10:10.517] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:10:10.517] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:10:10.517] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:10:10.517] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:10:10.517] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:10:10.517] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:10:10.517] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:10:10.517] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:10:10.517] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:10:10.517] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:10:10.519] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:10:10.519] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:10:10.519] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:10:10.521] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:10:10.521] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:10:10.525] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:10:10.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:10:10.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:10:10.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:10:10.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:10:10.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:10:10.564] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:10:10.564] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1103759.44 ms
[2024-09-30 14:10:10.564] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:10:10.564] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      35.20 ms /     6 tokens (    5.87 ms per token,   170.46 tokens per second)
[2024-09-30 14:10:10.564] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:10:10.564] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1103758.56 ms /     7 tokens
[2024-09-30 14:10:10.565] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:10:10.565] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:10:10.571] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:10:10.571] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:10:10.571] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:10:10.571] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:10:10.571] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:10:10.571] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:10:10.571] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:10:10.572] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:10:10.572] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:10:10.572] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:10:10.572] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:10:10.591] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:10:10.592] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:10:10.592] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:10:10.592] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-699e928f-1129-4070-845d-d7b2aefa91d3
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:10:10.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:10:10.592] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:10:10.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:10:10.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:10:10.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:10:10.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:10:10.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:10:10.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:10:11.075] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:10:11.075] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:10:11.075] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:10:11.077] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:10:11.077] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:10:11.077] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:10:11.080] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:10:11.080] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:10:11.080] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:10:11.081] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:10:11.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:10:11.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:10:11.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:10:11.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:10:11.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:10:11.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:10:11.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:10:11.081] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:10:11.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:10:11.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:10:11.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:10:11.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:10:11.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:10:11.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:10:11.241] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:10:11.241] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:10:11.241] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:10:11.241] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:10:11.241] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:10:11.241] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:10:11.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:10:11.245] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:10:11.245] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:10:11.245] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:10:11.245] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:10:11.245] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:10:11.245] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:10:11.245] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:10:11.245] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:10:11.411] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:10:11.411] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:10:11.411] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:10:11.412] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:10:11.412] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:10:11.412] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:10:18.102] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:10:18.102] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:10:18.102] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1107702.55 ms
[2024-09-30 14:10:18.102] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.75 ms /    23 runs   (    0.21 ms per token,  4840.07 tokens per second)
[2024-09-30 14:10:18.102] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2988.60 ms /    24 tokens (  124.52 ms per token,     8.03 tokens per second)
[2024-09-30 14:10:18.102] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3694.77 ms /    22 runs   (  167.94 ms per token,     5.95 tokens per second)
[2024-09-30 14:10:18.102] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1111402.95 ms /    46 tokens
[2024-09-30 14:10:18.105] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:10:18.105] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 112
[2024-09-30 14:10:18.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am Phi, an advanced artificial intelligence developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:10:18.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:10:18.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am Phi, an advanced artificial intelligence developed by Microsoft. How can I assist you today?
[2024-09-30 14:10:18.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:10:18.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:10:18.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:10:18.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 23
[2024-09-30 14:10:18.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 23
[2024-09-30 14:10:18.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:10:18.107] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:10:18.107] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:10:18.107] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:10:18.107] [info] rag_api_server in src/main.rs:517: response_body_size: 416
[2024-09-30 14:10:18.107] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:10:18.107] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:11:08.133] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:38850, local_addr: 0.0.0.0:8080
[2024-09-30 14:11:08.133] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:11:08.133] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:11:08.133] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:11:08.133] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:11:08.134] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-75494104-2bf2-42bf-a343-d81448077ef1
[2024-09-30 14:11:08.134] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:11:08.134] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:11:08.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:11:08.134] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:11:08.134] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:11:08.134] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:11:08.134] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:11:08.134] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:11:08.134] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:11:08.134] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:11:08.134] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:11:08.134] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:11:08.134] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:11:08.134] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:11:08.134] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:11:08.134] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:11:08.135] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:11:08.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:11:08.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:11:08.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:11:08.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:11:08.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:11:08.138] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:11:08.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:11:08.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:11:08.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:11:08.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:11:08.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:11:08.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:11:08.140] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:11:08.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:11:08.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:11:08.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:11:08.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:11:08.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:11:08.165] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:11:08.165] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1161360.28 ms
[2024-09-30 14:11:08.165] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:11:08.165] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.04 ms /     6 tokens (    3.67 ms per token,   272.23 tokens per second)
[2024-09-30 14:11:08.165] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:11:08.165] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1161359.56 ms /     7 tokens
[2024-09-30 14:11:08.165] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:11:08.165] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:11:08.170] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:11:08.170] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:11:08.170] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:11:08.170] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:11:08.171] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:11:08.171] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:11:08.171] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:11:08.171] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:11:08.171] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:11:08.171] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:11:08.171] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:11:08.190] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:11:08.190] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:11:08.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:11:08.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:11:08.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:11:08.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:11:08.190] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:11:08.190] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:11:08.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-75494104-2bf2-42bf-a343-d81448077ef1
[2024-09-30 14:11:08.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:11:08.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:11:08.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:11:08.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:11:08.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:11:08.191] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:11:08.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:11:08.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:11:08.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:11:08.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:11:08.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:11:08.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:11:08.679] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:11:08.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:11:08.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:11:08.682] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:11:08.682] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:11:08.682] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:11:08.685] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:11:08.685] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:11:08.685] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:11:08.685] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 23
[2024-09-30 14:11:08.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:11:08.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:11:08.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:11:08.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:11:08.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:11:08.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:11:08.685] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:11:08.685] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:11:08.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:11:08.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:11:08.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:11:08.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:11:08.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:11:08.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:11:08.844] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:11:08.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:11:08.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:11:08.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:11:08.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:11:08.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:11:08.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:11:08.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:11:08.848] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:11:08.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:11:08.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:11:08.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:11:08.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:11:08.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:11:08.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:11:09.024] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:11:09.024] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:11:09.024] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:11:09.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:11:09.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:11:09.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:11:16.024] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:11:16.024] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:11:16.024] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1165163.52 ms
[2024-09-30 14:11:16.024] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.47 ms /    25 runs   (    0.22 ms per token,  4567.88 tokens per second)
[2024-09-30 14:11:16.024] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2836.55 ms /    24 tokens (  118.19 ms per token,     8.46 tokens per second)
[2024-09-30 14:11:16.024] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4155.47 ms /    24 runs   (  173.14 ms per token,     5.78 tokens per second)
[2024-09-30 14:11:16.024] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1169325.95 ms /    48 tokens
[2024-09-30 14:11:16.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:11:16.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 111
[2024-09-30 14:11:16.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft to provide assistance and information. How may I help you today?<|end|>
[2024-09-30 14:11:16.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:11:16.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft to provide assistance and information. How may I help you today?
[2024-09-30 14:11:16.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:11:16.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:11:16.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:11:16.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:11:16.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:11:16.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:11:16.028] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:11:16.028] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:11:16.028] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:11:16.028] [info] rag_api_server in src/main.rs:517: response_body_size: 415
[2024-09-30 14:11:16.028] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:11:16.028] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:12:06.054] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:52886, local_addr: 0.0.0.0:8080
[2024-09-30 14:12:06.055] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:12:06.055] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:12:06.055] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:12:06.055] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:12:06.055] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-762d542f-a050-48f9-b786-476b7d18b6e4
[2024-09-30 14:12:06.055] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:12:06.055] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:12:06.055] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:12:06.056] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:12:06.056] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:12:06.056] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:12:06.056] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:12:06.056] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:12:06.056] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:12:06.056] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:12:06.056] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:12:06.056] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:12:06.056] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:12:06.056] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:12:06.056] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:12:06.056] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:12:06.057] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:12:06.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:12:06.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:12:06.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:12:06.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:12:06.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:12:06.061] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:12:06.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:12:06.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:12:06.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:12:06.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:12:06.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:12:06.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:12:06.065] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:12:06.065] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:12:06.065] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:12:06.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:12:06.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:12:06.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:12:06.091] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:12:06.091] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1219286.70 ms
[2024-09-30 14:12:06.091] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:12:06.091] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.75 ms /     6 tokens (    3.96 ms per token,   252.62 tokens per second)
[2024-09-30 14:12:06.091] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:12:06.091] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1219286.56 ms /     7 tokens
[2024-09-30 14:12:06.092] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:12:06.092] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:12:06.100] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:12:06.100] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:12:06.100] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:12:06.100] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:12:06.100] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:12:06.100] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:12:06.100] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:12:06.100] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:12:06.100] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:12:06.100] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:12:06.101] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:12:06.120] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:12:06.120] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:12:06.120] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:12:06.120] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-762d542f-a050-48f9-b786-476b7d18b6e4
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:12:06.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:12:06.120] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:12:06.120] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:12:06.120] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:12:06.120] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:12:06.120] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:12:06.120] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:12:06.120] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:12:06.614] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:12:06.614] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:12:06.614] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:12:06.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:12:06.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:12:06.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:12:06.621] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:12:06.621] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:12:06.621] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:12:06.621] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:12:06.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:12:06.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:12:06.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:12:06.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:12:06.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:12:06.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:12:06.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:12:06.622] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:12:06.622] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:12:06.622] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:12:06.622] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:12:06.622] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:12:06.622] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:12:06.622] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:12:06.858] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:12:06.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:12:06.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:12:06.860] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:12:06.860] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:12:06.860] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:12:06.865] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:12:06.865] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:12:06.865] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:12:06.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:12:06.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:12:06.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:12:06.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:12:06.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:12:06.865] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:12:07.113] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:12:07.113] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:12:07.113] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:12:07.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:12:07.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:12:07.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:12:12.182] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:12:12.182] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:12:12.182] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1223379.18 ms
[2024-09-30 14:12:12.182] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       3.18 ms /    13 runs   (    0.24 ms per token,  4086.77 tokens per second)
[2024-09-30 14:12:12.182] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2963.31 ms /    24 tokens (  123.47 ms per token,     8.10 tokens per second)
[2024-09-30 14:12:12.182] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2100.47 ms /    12 runs   (  175.04 ms per token,     5.71 tokens per second)
[2024-09-30 14:12:12.182] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1225483.95 ms /    36 tokens
[2024-09-30 14:12:12.185] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:12:12.185] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 68
[2024-09-30 14:12:12.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence developed by Microsoft.<|end|>
[2024-09-30 14:12:12.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:12:12.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence developed by Microsoft.
[2024-09-30 14:12:12.185] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:12:12.185] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:12:12.185] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:12:12.185] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:12:12.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:12:12.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:12:12.185] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:12:12.185] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:12:12.186] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:12:12.186] [info] rag_api_server in src/main.rs:517: response_body_size: 372
[2024-09-30 14:12:12.186] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:12:12.186] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:13:02.210] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:41292, local_addr: 0.0.0.0:8080
[2024-09-30 14:13:02.210] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:13:02.210] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:13:02.210] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:13:02.210] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:13:02.211] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-a1d33b4c-ac32-41ed-9add-9ed63d4b1cab
[2024-09-30 14:13:02.211] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:13:02.211] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:13:02.211] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:13:02.211] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:13:02.211] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:13:02.211] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:13:02.211] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:13:02.211] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:13:02.211] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:13:02.211] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:13:02.211] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:13:02.211] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:13:02.211] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:13:02.211] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:13:02.211] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:13:02.211] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:13:02.212] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:13:02.212] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:13:02.212] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:13:02.214] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:13:02.214] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:13:02.214] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:13:02.214] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:13:02.215] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:13:02.215] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:13:02.215] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:13:02.215] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:13:02.215] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:13:02.215] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:13:02.219] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:13:02.219] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:13:02.219] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:13:02.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:13:02.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:13:02.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:13:02.244] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:13:02.244] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1275439.64 ms
[2024-09-30 14:13:02.244] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:13:02.244] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.89 ms /     6 tokens (    3.82 ms per token,   262.08 tokens per second)
[2024-09-30 14:13:02.244] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:13:02.244] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1275439.56 ms /     7 tokens
[2024-09-30 14:13:02.245] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:13:02.245] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:13:02.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:13:02.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:13:02.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:13:02.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:13:02.251] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:13:02.251] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:13:02.251] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:13:02.251] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:13:02.252] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:13:02.252] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:13:02.252] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:13:02.271] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:13:02.271] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:13:02.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:13:02.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:13:02.272] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:13:02.272] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:13:02.272] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:13:02.272] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:13:02.272] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-a1d33b4c-ac32-41ed-9add-9ed63d4b1cab
[2024-09-30 14:13:02.272] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:13:02.272] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:13:02.272] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:13:02.272] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:13:02.272] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:13:02.272] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:13:02.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:13:02.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:13:02.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:13:02.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:13:02.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:13:02.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:13:02.752] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:13:02.752] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:13:02.752] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:13:02.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:13:02.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:13:02.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:13:02.759] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:13:02.759] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:13:02.759] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:13:02.759] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:13:02.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:13:02.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:13:02.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:13:02.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:13:02.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:13:02.759] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:13:02.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:13:02.763] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:13:02.763] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:13:02.763] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:13:02.763] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:13:02.763] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:13:02.763] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:13:02.763] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:13:02.967] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:13:02.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:13:02.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:13:02.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:13:02.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:13:02.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:13:02.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:13:02.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:13:02.973] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:13:02.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:13:02.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:13:02.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:13:02.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:13:02.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:13:02.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:13:03.172] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:13:03.172] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:13:03.172] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:13:03.173] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:13:03.173] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:13:03.173] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:13:09.817] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:13:09.818] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:13:09.818] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1279321.59 ms
[2024-09-30 14:13:09.818] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.34 ms /    20 runs   (    0.22 ms per token,  4612.55 tokens per second)
[2024-09-30 14:13:09.818] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2846.92 ms /    24 tokens (  118.62 ms per token,     8.43 tokens per second)
[2024-09-30 14:13:09.818] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3792.12 ms /    19 runs   (  199.59 ms per token,     5.01 tokens per second)
[2024-09-30 14:13:09.818] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1283118.95 ms /    43 tokens
[2024-09-30 14:13:09.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:13:09.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 14:13:09.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:13:09.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:13:09.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:13:09.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:13:09.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:13:09.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:13:09.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:13:09.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:13:09.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:13:09.822] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:13:09.823] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:13:09.823] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:13:09.823] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 14:13:09.823] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:13:09.823] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:13:59.847] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:60898, local_addr: 0.0.0.0:8080
[2024-09-30 14:13:59.848] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:13:59.848] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:13:59.848] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:13:59.848] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:13:59.849] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-39153f4b-f08f-4fec-b5b4-6724c4ab1b24
[2024-09-30 14:13:59.849] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:13:59.849] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:13:59.849] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:13:59.849] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:13:59.849] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:13:59.849] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:13:59.850] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:13:59.850] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:13:59.850] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:13:59.850] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:13:59.850] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:13:59.850] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:13:59.850] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:13:59.850] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:13:59.850] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:13:59.850] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:13:59.857] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:13:59.857] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:13:59.857] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:13:59.868] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:13:59.868] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:13:59.868] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:13:59.869] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:13:59.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:13:59.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:13:59.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:13:59.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:13:59.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:13:59.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:13:59.892] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:13:59.892] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:13:59.892] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:13:59.894] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:13:59.894] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:13:59.894] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:14:00.083] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:14:00.084] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1333278.95 ms
[2024-09-30 14:14:00.084] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:14:00.084] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =     187.39 ms /     6 tokens (   31.23 ms per token,    32.02 tokens per second)
[2024-09-30 14:14:00.084] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:14:00.084] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1333278.56 ms /     7 tokens
[2024-09-30 14:14:00.084] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:14:00.084] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:14:00.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:14:00.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:14:00.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:14:00.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:14:00.090] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:14:00.090] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:14:00.090] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:14:00.090] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:14:00.091] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:14:00.091] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:14:00.091] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:14:00.113] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:14:00.114] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:14:00.114] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:14:00.114] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-39153f4b-f08f-4fec-b5b4-6724c4ab1b24
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:14:00.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:00.114] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:14:00.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:14:00.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:14:00.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:14:00.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:14:00.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:14:00.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:14:00.730] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:14:00.730] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:14:00.730] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:14:00.734] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:14:00.734] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:14:00.734] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:14:00.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:00.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:14:00.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:14:00.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:14:00.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:14:00.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:14:00.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:14:00.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:14:00.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:14:00.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:14:00.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:00.739] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:14:00.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:14:00.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:14:00.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:14:00.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:14:00.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:14:00.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:14:00.967] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:14:00.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:14:00.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:14:00.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:14:00.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:14:00.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:14:00.972] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:14:00.972] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:00.972] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:14:00.972] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:14:00.972] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:14:00.972] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:14:00.972] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:14:00.972] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:14:00.972] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:14:01.230] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:14:01.230] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:14:01.230] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:14:01.231] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:14:01.231] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:14:01.231] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:14:08.113] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:14:08.113] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:14:08.113] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1337741.25 ms
[2024-09-30 14:14:08.113] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.21 ms /    20 runs   (    0.21 ms per token,  4752.85 tokens per second)
[2024-09-30 14:14:08.113] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3208.64 ms /    24 tokens (  133.69 ms per token,     7.48 tokens per second)
[2024-09-30 14:14:08.113] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3667.94 ms /    19 runs   (  193.05 ms per token,     5.18 tokens per second)
[2024-09-30 14:14:08.113] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1341413.95 ms /    43 tokens
[2024-09-30 14:14:08.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:14:08.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 14:14:08.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:14:08.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:14:08.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:14:08.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:08.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:14:08.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:14:08.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:14:08.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:14:08.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:14:08.118] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:14:08.118] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:14:08.118] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:14:08.118] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 14:14:08.118] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:14:08.118] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:14:58.145] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:59654, local_addr: 0.0.0.0:8080
[2024-09-30 14:14:58.145] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:14:58.145] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:14:58.145] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:14:58.145] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:14:58.146] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-3c694c9f-be81-4d80-9291-649e3fb7fa70
[2024-09-30 14:14:58.146] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:14:58.146] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:14:58.146] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:14:58.146] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:14:58.146] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:14:58.146] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:14:58.146] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:14:58.146] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:14:58.146] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:14:58.146] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:14:58.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:14:58.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:14:58.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:14:58.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:14:58.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:14:58.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:14:58.147] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:14:58.147] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:14:58.147] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:14:58.149] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:14:58.149] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:14:58.149] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:14:58.150] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:14:58.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:14:58.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:14:58.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:14:58.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:14:58.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:14:58.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:14:58.153] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:14:58.153] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:14:58.153] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:14:58.156] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:14:58.156] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:14:58.156] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:14:58.178] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:14:58.178] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1391373.73 ms
[2024-09-30 14:14:58.178] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:14:58.178] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.31 ms /     6 tokens (    3.55 ms per token,   281.61 tokens per second)
[2024-09-30 14:14:58.178] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:14:58.178] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1391373.56 ms /     7 tokens
[2024-09-30 14:14:58.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:14:58.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:14:58.184] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:14:58.184] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:14:58.184] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:14:58.184] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:14:58.184] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:14:58.184] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:14:58.184] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:14:58.185] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:14:58.185] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:14:58.185] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:14:58.185] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:14:58.203] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:14:58.203] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:14:58.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:14:58.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:14:58.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:14:58.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:14:58.203] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:14:58.204] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:14:58.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-3c694c9f-be81-4d80-9291-649e3fb7fa70
[2024-09-30 14:14:58.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:14:58.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:14:58.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:14:58.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:14:58.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:58.204] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:14:58.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:14:58.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:14:58.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:14:58.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:14:58.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:14:58.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:14:58.662] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:14:58.662] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:14:58.662] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:14:58.664] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:14:58.664] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:14:58.664] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:14:58.667] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:58.667] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:14:58.667] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:14:58.667] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:14:58.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:14:58.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:14:58.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:14:58.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:14:58.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:14:58.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:14:58.668] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:58.668] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:14:58.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:14:58.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:14:58.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:14:58.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:14:58.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:14:58.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:14:58.837] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:14:58.837] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:14:58.837] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:14:58.838] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:14:58.838] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:14:58.838] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:14:58.841] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:14:58.841] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:14:58.841] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:14:58.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:14:58.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:14:58.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:14:58.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:14:58.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:14:58.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:14:59.009] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:14:59.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:14:59.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:14:59.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:14:59.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:14:59.009] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:15:06.357] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:15:06.357] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:15:06.357] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1395061.37 ms
[2024-09-30 14:15:06.357] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       7.20 ms /    31 runs   (    0.23 ms per token,  4306.75 tokens per second)
[2024-09-30 14:15:06.357] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2750.04 ms /    24 tokens (  114.59 ms per token,     8.73 tokens per second)
[2024-09-30 14:15:06.357] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4588.10 ms /    30 runs   (  152.94 ms per token,     6.54 tokens per second)
[2024-09-30 14:15:06.357] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1399657.95 ms /    54 tokens
[2024-09-30 14:15:06.359] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:15:06.359] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 112
[2024-09-30 14:15:06.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I'm Phi, an AI developed to assist and engage in informative dialogues. How may I help you today?<|end|>
[2024-09-30 14:15:06.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:15:06.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I'm Phi, an AI developed to assist and engage in informative dialogues. How may I help you today?
[2024-09-30 14:15:06.359] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:15:06.359] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:15:06.359] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:15:06.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 31
[2024-09-30 14:15:06.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 31
[2024-09-30 14:15:06.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:15:06.360] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:15:06.360] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:15:06.360] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:15:06.360] [info] rag_api_server in src/main.rs:517: response_body_size: 416
[2024-09-30 14:15:06.360] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:15:06.360] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:15:56.387] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:47592, local_addr: 0.0.0.0:8080
[2024-09-30 14:15:56.388] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:15:56.388] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:15:56.388] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:15:56.388] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:15:56.389] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-5adb7ac1-f25f-43f7-9ba6-54ed15c92823
[2024-09-30 14:15:56.389] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:15:56.389] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:15:56.389] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:15:56.389] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:15:56.389] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:15:56.389] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:15:56.389] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:15:56.389] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:15:56.389] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:15:56.389] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:15:56.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:15:56.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:15:56.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:15:56.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:15:56.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:15:56.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:15:56.391] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:15:56.391] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:15:56.391] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:15:56.393] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:15:56.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:15:56.397] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:15:56.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:15:56.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:15:56.399] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:15:56.399] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:15:56.399] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:15:56.435] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:15:56.435] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1449630.47 ms
[2024-09-30 14:15:56.435] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:15:56.435] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      35.67 ms /     6 tokens (    5.95 ms per token,   168.21 tokens per second)
[2024-09-30 14:15:56.435] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:15:56.435] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1449629.56 ms /     7 tokens
[2024-09-30 14:15:56.436] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:15:56.436] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:15:56.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:15:56.440] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:15:56.441] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:15:56.441] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:15:56.441] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:15:56.441] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:15:56.441] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:15:56.441] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:15:56.441] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:15:56.441] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:15:56.441] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:15:56.459] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:15:56.459] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:15:56.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:15:56.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:15:56.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:15:56.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:15:56.459] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:15:56.459] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:15:56.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-5adb7ac1-f25f-43f7-9ba6-54ed15c92823
[2024-09-30 14:15:56.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:15:56.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:15:56.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:15:56.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:15:56.460] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:15:56.460] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:15:56.460] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:15:56.460] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:15:56.460] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:15:56.460] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:15:56.460] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:15:56.460] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:15:56.940] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:15:56.940] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:15:56.940] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:15:56.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:15:56.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:15:56.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:15:56.945] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:15:56.945] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:15:56.945] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:15:56.945] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 31
[2024-09-30 14:15:56.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:15:56.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:15:56.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:15:56.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:15:56.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:15:56.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:15:56.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:15:56.946] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:15:56.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:15:56.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:15:56.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:15:56.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:15:56.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:15:56.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:15:57.110] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:15:57.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:15:57.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:15:57.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:15:57.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:15:57.111] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:15:57.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:15:57.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:15:57.114] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:15:57.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:15:57.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:15:57.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:15:57.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:15:57.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:15:57.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:15:57.281] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:15:57.281] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:15:57.281] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:15:57.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:15:57.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:15:57.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:16:03.197] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:16:03.197] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:16:03.197] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1453470.77 ms
[2024-09-30 14:16:03.197] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.51 ms /    20 runs   (    0.23 ms per token,  4431.64 tokens per second)
[2024-09-30 14:16:03.197] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2886.78 ms /    24 tokens (  120.28 ms per token,     8.31 tokens per second)
[2024-09-30 14:16:03.197] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3022.71 ms /    19 runs   (  159.09 ms per token,     6.29 tokens per second)
[2024-09-30 14:16:03.197] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1456498.95 ms /    43 tokens
[2024-09-30 14:16:03.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:16:03.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 14:16:03.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:16:03.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:16:03.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:16:03.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:16:03.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:16:03.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:16:03.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:16:03.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:16:03.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:16:03.202] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:16:03.202] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:16:03.202] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:16:03.202] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 14:16:03.202] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:16:03.202] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:16:53.228] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:34850, local_addr: 0.0.0.0:8080
[2024-09-30 14:16:53.229] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:16:53.229] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:16:53.229] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:16:53.229] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:16:53.229] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-a54be43e-dc6a-4f18-a3f5-2bc2e4f36230
[2024-09-30 14:16:53.229] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:16:53.229] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:16:53.229] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:16:53.229] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:16:53.229] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:16:53.229] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:16:53.229] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:16:53.229] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:16:53.229] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:16:53.230] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:16:53.230] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:16:53.230] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:16:53.230] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:16:53.230] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:16:53.230] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:16:53.230] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:16:53.231] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:16:53.231] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:16:53.231] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:16:53.233] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:16:53.233] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:16:53.233] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:16:53.233] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:16:53.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:16:53.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:16:53.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:16:53.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:16:53.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:16:53.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:16:53.238] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:16:53.238] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:16:53.238] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:16:53.240] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:16:53.240] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:16:53.240] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:16:53.267] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:16:53.267] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1506462.25 ms
[2024-09-30 14:16:53.267] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:16:53.267] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      24.90 ms /     6 tokens (    4.15 ms per token,   240.96 tokens per second)
[2024-09-30 14:16:53.267] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:16:53.267] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1506461.56 ms /     7 tokens
[2024-09-30 14:16:53.267] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:16:53.267] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:16:53.274] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:16:53.274] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:16:53.274] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:16:53.274] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:16:53.274] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:16:53.274] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:16:53.274] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:16:53.274] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:16:53.274] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:16:53.274] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:16:53.274] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:16:53.292] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:16:53.292] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:16:53.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:16:53.293] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:16:53.293] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-a54be43e-dc6a-4f18-a3f5-2bc2e4f36230
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:16:53.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:16:53.293] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:16:53.293] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:16:53.293] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:16:53.293] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:16:53.293] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:16:53.293] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:16:53.293] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:16:53.760] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:16:53.760] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:16:53.760] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:16:53.764] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:16:53.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:16:53.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:16:53.773] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:16:53.773] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:16:53.773] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:16:53.774] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:16:53.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:16:53.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:16:53.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:16:53.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:16:53.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:16:53.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:16:53.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:16:53.774] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:16:53.774] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:16:53.774] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:16:53.774] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:16:53.774] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:16:53.774] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:16:53.774] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:16:54.067] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:16:54.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:16:54.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:16:54.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:16:54.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:16:54.067] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:16:54.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:16:54.072] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:16:54.072] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:16:54.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:16:54.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:16:54.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:16:54.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:16:54.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:16:54.072] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:16:54.263] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:16:54.263] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:16:54.263] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:16:54.264] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:16:54.264] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:16:54.264] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:17:02.048] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:17:02.048] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:17:02.048] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1510268.52 ms
[2024-09-30 14:17:02.048] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.49 ms /    30 runs   (    0.22 ms per token,  4626.06 tokens per second)
[2024-09-30 14:17:02.048] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2702.64 ms /    24 tokens (  112.61 ms per token,     8.88 tokens per second)
[2024-09-30 14:17:02.048] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    5072.84 ms /    29 runs   (  174.93 ms per token,     5.72 tokens per second)
[2024-09-30 14:17:02.048] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1515348.95 ms /    53 tokens
[2024-09-30 14:17:02.050] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:17:02.050] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 149
[2024-09-30 14:17:02.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am an AI digital assistant designed to interact with users and provide information or support as needed. How can I assist you today?<|end|>
[2024-09-30 14:17:02.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:17:02.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am an AI digital assistant designed to interact with users and provide information or support as needed. How can I assist you today?
[2024-09-30 14:17:02.050] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:17:02.050] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:17:02.051] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:17:02.051] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 30
[2024-09-30 14:17:02.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 30
[2024-09-30 14:17:02.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:17:02.051] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:17:02.051] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:17:02.051] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:17:02.051] [info] rag_api_server in src/main.rs:517: response_body_size: 453
[2024-09-30 14:17:02.051] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:17:02.052] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:17:52.078] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:34784, local_addr: 0.0.0.0:8080
[2024-09-30 14:17:52.080] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:17:52.080] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:17:52.080] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:17:52.080] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:17:52.081] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-90eaa1cc-eb97-4fb4-8dfd-721d561d8336
[2024-09-30 14:17:52.081] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:17:52.081] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:17:52.081] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:17:52.081] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:17:52.081] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:17:52.081] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:17:52.081] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:17:52.081] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:17:52.081] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:17:52.081] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:17:52.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:17:52.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:17:52.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:17:52.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:17:52.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:17:52.081] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:17:52.083] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:17:52.083] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:17:52.083] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:17:52.085] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:17:52.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:17:52.090] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:17:52.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:17:52.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:17:52.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:17:52.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:17:52.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:17:52.116] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:17:52.116] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1565311.08 ms
[2024-09-30 14:17:52.116] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:17:52.116] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.21 ms /     6 tokens (    3.87 ms per token,   258.49 tokens per second)
[2024-09-30 14:17:52.116] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:17:52.116] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1565310.56 ms /     7 tokens
[2024-09-30 14:17:52.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:17:52.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:17:52.121] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:17:52.121] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:17:52.121] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:17:52.121] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:17:52.121] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:17:52.121] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:17:52.121] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:17:52.121] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:17:52.121] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:17:52.121] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:17:52.121] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:17:52.140] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:17:52.140] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:17:52.140] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:17:52.140] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-90eaa1cc-eb97-4fb4-8dfd-721d561d8336
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:17:52.140] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:17:52.140] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:17:52.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:17:52.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:17:52.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:17:52.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:17:52.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:17:52.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:17:52.618] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:17:52.618] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:17:52.618] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:17:52.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:17:52.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:17:52.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:17:52.624] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:17:52.624] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:17:52.624] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:17:52.624] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 30
[2024-09-30 14:17:52.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:17:52.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:17:52.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:17:52.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:17:52.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:17:52.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:17:52.624] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:17:52.624] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:17:52.624] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:17:52.624] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:17:52.624] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:17:52.624] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:17:52.624] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:17:52.624] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:17:52.802] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:17:52.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:17:52.802] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:17:52.803] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:17:52.803] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:17:52.803] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:17:52.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:17:52.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:17:52.805] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:17:52.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:17:52.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:17:52.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:17:52.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:17:52.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:17:52.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:17:52.978] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:17:52.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:17:52.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:17:52.979] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:17:52.979] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:17:52.979] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:17:59.391] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:17:59.391] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:17:59.391] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1569191.43 ms
[2024-09-30 14:17:59.391] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.95 ms /    21 runs   (    0.24 ms per token,  4241.57 tokens per second)
[2024-09-30 14:17:59.391] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2910.76 ms /    24 tokens (  121.28 ms per token,     8.25 tokens per second)
[2024-09-30 14:17:59.391] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3494.55 ms /    20 runs   (  174.73 ms per token,     5.72 tokens per second)
[2024-09-30 14:17:59.391] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1572691.95 ms /    44 tokens
[2024-09-30 14:17:59.393] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:17:59.393] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 74
[2024-09-30 14:17:59.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:17:59.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:17:59.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:17:59.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:17:59.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:17:59.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:17:59.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:17:59.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:17:59.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:17:59.394] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:17:59.394] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:17:59.394] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:17:59.395] [info] rag_api_server in src/main.rs:517: response_body_size: 378
[2024-09-30 14:17:59.395] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:17:59.395] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:18:49.423] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36486, local_addr: 0.0.0.0:8080
[2024-09-30 14:18:49.424] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:18:49.424] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:18:49.424] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:18:49.424] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:18:49.424] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-3c737f7d-4f19-44c0-9bd7-f7318beddcdf
[2024-09-30 14:18:49.424] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:18:49.424] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:18:49.424] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:18:49.424] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:18:49.424] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:18:49.424] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:18:49.424] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:18:49.425] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:18:49.425] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:18:49.425] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:18:49.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:18:49.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:18:49.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:18:49.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:18:49.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:18:49.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:18:49.426] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:18:49.426] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:18:49.426] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:18:49.428] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:18:49.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:18:49.431] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:18:49.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:18:49.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:18:49.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:18:49.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:18:49.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:18:49.456] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:18:49.456] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1622651.63 ms
[2024-09-30 14:18:49.456] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:18:49.456] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.09 ms /     6 tokens (    3.85 ms per token,   259.82 tokens per second)
[2024-09-30 14:18:49.456] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:18:49.456] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1622651.56 ms /     7 tokens
[2024-09-30 14:18:49.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:18:49.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:18:49.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:18:49.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:18:49.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:18:49.463] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:18:49.463] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:18:49.463] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:18:49.463] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:18:49.463] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:18:49.463] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:18:49.463] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:18:49.463] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:18:49.480] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:18:49.480] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:18:49.480] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:18:49.480] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-3c737f7d-4f19-44c0-9bd7-f7318beddcdf
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:18:49.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:18:49.480] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:18:49.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:18:49.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:18:49.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:18:49.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:18:49.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:18:49.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:18:49.942] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:18:49.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:18:49.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:18:49.945] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:18:49.945] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:18:49.945] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:18:49.948] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:18:49.948] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:18:49.948] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:18:49.948] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:18:49.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:18:49.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:18:49.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:18:49.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:18:49.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:18:49.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:18:49.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:18:49.948] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:18:49.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:18:49.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:18:49.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:18:49.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:18:49.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:18:49.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:18:50.125] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:18:50.125] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:18:50.125] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:18:50.125] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:18:50.125] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:18:50.125] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:18:50.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:18:50.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:18:50.128] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:18:50.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:18:50.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:18:50.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:18:50.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:18:50.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:18:50.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:18:50.298] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:18:50.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:18:50.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:18:50.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:18:50.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:18:50.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:18:56.389] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:18:56.389] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:18:56.389] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1626317.41 ms
[2024-09-30 14:18:56.389] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.82 ms /    22 runs   (    0.22 ms per token,  4560.53 tokens per second)
[2024-09-30 14:18:56.389] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2716.86 ms /    24 tokens (  113.20 ms per token,     8.83 tokens per second)
[2024-09-30 14:18:56.389] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3367.37 ms /    21 runs   (  160.35 ms per token,     6.24 tokens per second)
[2024-09-30 14:18:56.389] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1629690.95 ms /    45 tokens
[2024-09-30 14:18:56.392] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:18:56.392] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 83
[2024-09-30 14:18:56.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I'm an AI developed to interact with and assist you. How can I help?<|end|>
[2024-09-30 14:18:56.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:18:56.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I'm an AI developed to interact with and assist you. How can I help?
[2024-09-30 14:18:56.392] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:18:56.392] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:18:56.392] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:18:56.392] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 22
[2024-09-30 14:18:56.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 22
[2024-09-30 14:18:56.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:18:56.393] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:18:56.393] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:18:56.393] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:18:56.393] [info] rag_api_server in src/main.rs:517: response_body_size: 387
[2024-09-30 14:18:56.393] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:18:56.393] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:19:46.418] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36878, local_addr: 0.0.0.0:8080
[2024-09-30 14:19:46.419] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:19:46.419] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:19:46.419] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:19:46.419] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:19:46.420] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-ca251e98-eb5c-4a09-af05-9cc97ef75395
[2024-09-30 14:19:46.420] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:19:46.420] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:19:46.420] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:19:46.420] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:19:46.420] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:19:46.420] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:19:46.420] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:19:46.420] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:19:46.420] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:19:46.420] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:19:46.420] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:19:46.420] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:19:46.420] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:19:46.420] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:19:46.420] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:19:46.420] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:19:46.421] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:19:46.421] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:19:46.421] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:19:46.423] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:19:46.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:19:46.426] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:19:46.426] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:19:46.426] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:19:46.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:19:46.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:19:46.428] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:19:46.451] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:19:46.451] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1679646.84 ms
[2024-09-30 14:19:46.451] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:19:46.451] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.98 ms /     6 tokens (    3.66 ms per token,   272.94 tokens per second)
[2024-09-30 14:19:46.451] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:19:46.451] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1679646.56 ms /     7 tokens
[2024-09-30 14:19:46.452] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:19:46.452] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:19:46.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:19:46.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:19:46.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:19:46.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:19:46.457] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:19:46.457] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:19:46.457] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:19:46.457] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:19:46.457] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:19:46.457] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:19:46.457] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:19:46.484] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:19:46.484] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:19:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:19:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:19:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:19:46.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:19:46.484] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:19:46.484] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:19:46.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-ca251e98-eb5c-4a09-af05-9cc97ef75395
[2024-09-30 14:19:46.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:19:46.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:19:46.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:19:46.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:19:46.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:19:46.485] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:19:46.485] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:19:46.485] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:19:46.485] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:19:46.485] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:19:46.485] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:19:46.485] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:19:46.946] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:19:46.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:19:46.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:19:46.949] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:19:46.949] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:19:46.949] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:19:46.952] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:19:46.952] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:19:46.952] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:19:46.952] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 22
[2024-09-30 14:19:46.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:19:46.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:19:46.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:19:46.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:19:46.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:19:46.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:19:46.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:19:46.952] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:19:46.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:19:46.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:19:46.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:19:46.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:19:46.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:19:46.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:19:47.123] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:19:47.123] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:19:47.123] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:19:47.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:19:47.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:19:47.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:19:47.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:19:47.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:19:47.128] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:19:47.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:19:47.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:19:47.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:19:47.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:19:47.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:19:47.128] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:19:47.305] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:19:47.305] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:19:47.305] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:19:47.307] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:19:47.307] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:19:47.307] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:19:55.263] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:19:55.263] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:19:55.263] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1683300.32 ms
[2024-09-30 14:19:55.263] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       7.68 ms /    35 runs   (    0.22 ms per token,  4556.11 tokens per second)
[2024-09-30 14:19:55.263] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2691.26 ms /    24 tokens (  112.14 ms per token,     8.92 tokens per second)
[2024-09-30 14:19:55.263] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    5255.08 ms /    34 runs   (  154.56 ms per token,     6.47 tokens per second)
[2024-09-30 14:19:55.263] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1688564.95 ms /    58 tokens
[2024-09-30 14:19:55.268] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:19:55.268] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 170
[2024-09-30 14:19:55.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I'm Phi, an artificial intelligence designed to interact with users and assist them by providing information or completing tasks. How can I help you today?<|end|>
[2024-09-30 14:19:55.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:19:55.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I'm Phi, an artificial intelligence designed to interact with users and assist them by providing information or completing tasks. How can I help you today?
[2024-09-30 14:19:55.268] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:19:55.268] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:19:55.268] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:19:55.268] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 35
[2024-09-30 14:19:55.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 35
[2024-09-30 14:19:55.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:19:55.268] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:19:55.269] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:19:55.269] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:19:55.269] [info] rag_api_server in src/main.rs:517: response_body_size: 474
[2024-09-30 14:19:55.269] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:19:55.269] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:20:45.296] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:43560, local_addr: 0.0.0.0:8080
[2024-09-30 14:20:45.296] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:20:45.296] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:20:45.296] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:20:45.296] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:20:45.297] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-00399753-cad6-41ad-b1d4-14bba82cc4da
[2024-09-30 14:20:45.297] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:20:45.297] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:20:45.297] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:20:45.297] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:20:45.297] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:20:45.297] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:20:45.297] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:20:45.297] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:20:45.297] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:20:45.297] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:20:45.297] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:20:45.297] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:20:45.297] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:20:45.297] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:20:45.297] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:20:45.297] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:20:45.298] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:20:45.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:20:45.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:20:45.300] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:20:45.300] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:20:45.300] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:20:45.301] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:20:45.301] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:20:45.301] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:20:45.301] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:20:45.301] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:20:45.301] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:20:45.301] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:20:45.306] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:20:45.306] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:20:45.306] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:20:45.308] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:20:45.308] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:20:45.308] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:20:45.341] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:20:45.341] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1738536.72 ms
[2024-09-30 14:20:45.341] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:20:45.341] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      32.56 ms /     6 tokens (    5.43 ms per token,   184.28 tokens per second)
[2024-09-30 14:20:45.341] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:20:45.341] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1738536.56 ms /     7 tokens
[2024-09-30 14:20:45.342] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:20:45.342] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:20:45.347] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:20:45.347] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:20:45.347] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:20:45.347] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:20:45.347] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:20:45.347] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:20:45.347] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:20:45.347] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:20:45.347] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:20:45.347] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:20:45.348] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:20:45.367] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:20:45.367] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:20:45.367] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:20:45.367] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:20:45.367] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:20:45.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:20:45.368] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:20:45.368] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:20:45.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-00399753-cad6-41ad-b1d4-14bba82cc4da
[2024-09-30 14:20:45.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:20:45.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:20:45.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:20:45.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:20:45.368] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:20:45.368] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:20:45.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:20:45.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:20:45.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:20:45.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:20:45.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:20:45.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:20:45.862] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:20:45.862] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:20:45.862] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:20:45.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:20:45.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:20:45.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:20:45.868] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:20:45.868] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:20:45.868] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:20:45.868] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 35
[2024-09-30 14:20:45.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:20:45.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:20:45.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:20:45.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:20:45.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:20:45.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:20:45.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:20:45.869] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:20:45.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:20:45.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:20:45.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:20:45.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:20:45.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:20:45.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:20:46.126] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:20:46.126] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:20:46.126] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:20:46.126] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:20:46.126] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:20:46.126] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:20:46.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:20:46.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:20:46.130] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:20:46.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:20:46.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:20:46.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:20:46.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:20:46.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:20:46.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:20:46.301] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:20:46.301] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:20:46.301] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:20:46.302] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:20:46.302] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:20:46.302] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:20:52.391] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:20:52.391] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:20:52.391] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1742505.24 ms
[2024-09-30 14:20:52.391] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.07 ms /    21 runs   (    0.19 ms per token,  5159.71 tokens per second)
[2024-09-30 14:20:52.391] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2900.90 ms /    24 tokens (  120.87 ms per token,     8.27 tokens per second)
[2024-09-30 14:20:52.391] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3182.38 ms /    20 runs   (  159.12 ms per token,     6.28 tokens per second)
[2024-09-30 14:20:52.391] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1745692.95 ms /    44 tokens
[2024-09-30 14:20:52.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:20:52.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 84
[2024-09-30 14:20:52.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft to assist with a wide range of tasks.<|end|>
[2024-09-30 14:20:52.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:20:52.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft to assist with a wide range of tasks.
[2024-09-30 14:20:52.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:20:52.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:20:52.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:20:52.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:20:52.394] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:20:52.395] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:20:52.395] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:20:52.395] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:20:52.395] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:20:52.395] [info] rag_api_server in src/main.rs:517: response_body_size: 388
[2024-09-30 14:20:52.395] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:20:52.395] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:21:42.421] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:50026, local_addr: 0.0.0.0:8080
[2024-09-30 14:21:42.422] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:21:42.422] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:21:42.422] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:21:42.422] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:21:42.423] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-84a9b3bf-4866-4092-bab8-9620be7f836d
[2024-09-30 14:21:42.423] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:21:42.423] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:21:42.423] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:21:42.423] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:21:42.423] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:21:42.423] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:21:42.423] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:21:42.423] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:21:42.423] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:21:42.423] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:21:42.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:21:42.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:21:42.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:21:42.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:21:42.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:21:42.423] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:21:42.424] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:21:42.424] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:21:42.424] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:21:42.426] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:21:42.426] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:21:42.426] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:21:42.427] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:21:42.427] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:21:42.427] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:21:42.427] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:21:42.427] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:21:42.427] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:21:42.427] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:21:42.429] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:21:42.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:21:42.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:21:42.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:21:42.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:21:42.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:21:42.455] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:21:42.455] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1795650.33 ms
[2024-09-30 14:21:42.455] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:21:42.455] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.11 ms /     6 tokens (    3.85 ms per token,   259.63 tokens per second)
[2024-09-30 14:21:42.455] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:21:42.455] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1795649.56 ms /     7 tokens
[2024-09-30 14:21:42.455] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:21:42.455] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:21:42.460] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:21:42.460] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:21:42.460] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:21:42.460] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:21:42.460] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:21:42.460] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:21:42.461] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:21:42.461] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:21:42.461] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:21:42.461] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:21:42.461] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:21:42.479] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:21:42.479] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:21:42.480] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:21:42.480] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-84a9b3bf-4866-4092-bab8-9620be7f836d
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:21:42.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:21:42.480] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:21:42.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:21:42.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:21:42.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:21:42.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:21:42.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:21:42.480] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:21:42.994] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:21:42.994] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:21:42.994] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:21:42.996] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:21:42.997] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:21:42.997] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:21:42.999] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:21:42.999] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:21:42.999] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:21:42.999] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:21:42.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:21:42.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:21:42.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:21:42.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:21:42.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:21:42.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:21:43.000] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:21:43.000] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:21:43.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:21:43.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:21:43.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:21:43.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:21:43.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:21:43.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:21:43.176] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:21:43.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:21:43.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:21:43.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:21:43.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:21:43.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:21:43.180] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:21:43.180] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:21:43.180] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:21:43.180] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:21:43.180] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:21:43.180] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:21:43.180] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:21:43.180] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:21:43.180] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:21:43.344] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:21:43.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:21:43.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:21:43.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:21:43.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:21:43.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:21:51.196] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:21:51.196] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:21:51.196] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1799281.81 ms
[2024-09-30 14:21:51.196] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.97 ms /    35 runs   (    0.20 ms per token,  5020.08 tokens per second)
[2024-09-30 14:21:51.196] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2635.43 ms /    24 tokens (  109.81 ms per token,     9.11 tokens per second)
[2024-09-30 14:21:51.196] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    5206.78 ms /    34 runs   (  153.14 ms per token,     6.53 tokens per second)
[2024-09-30 14:21:51.196] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1804496.95 ms /    58 tokens
[2024-09-30 14:21:51.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:21:51.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 171
[2024-09-30 14:21:51.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am an AI digital assistant designed to engage in conversation and provide information or assistance with a wide range of topics. How can I help you today?<|end|>
[2024-09-30 14:21:51.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:21:51.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am an AI digital assistant designed to engage in conversation and provide information or assistance with a wide range of topics. How can I help you today?
[2024-09-30 14:21:51.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:21:51.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:21:51.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:21:51.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 35
[2024-09-30 14:21:51.199] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 35
[2024-09-30 14:21:51.199] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:21:51.199] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:21:51.199] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:21:51.199] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:21:51.199] [info] rag_api_server in src/main.rs:517: response_body_size: 475
[2024-09-30 14:21:51.199] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:21:51.199] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:22:41.223] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:55474, local_addr: 0.0.0.0:8080
[2024-09-30 14:22:41.224] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:22:41.224] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:22:41.224] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:22:41.224] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:22:41.224] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-97a4bbcf-0991-442c-bb38-b198ae1d3b6d
[2024-09-30 14:22:41.224] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:22:41.224] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:22:41.224] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:22:41.224] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:22:41.224] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:22:41.225] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:22:41.225] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:22:41.225] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:22:41.225] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:22:41.225] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:22:41.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:22:41.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:22:41.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:22:41.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:22:41.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:22:41.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:22:41.226] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:22:41.226] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:22:41.226] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:22:41.229] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:22:41.229] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:22:41.232] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:22:41.232] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:22:41.232] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:22:41.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:22:41.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:22:41.234] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:22:41.256] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:22:41.256] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1854451.58 ms
[2024-09-30 14:22:41.256] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:22:41.256] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.59 ms /     6 tokens (    3.60 ms per token,   277.85 tokens per second)
[2024-09-30 14:22:41.256] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:22:41.256] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1854451.56 ms /     7 tokens
[2024-09-30 14:22:41.256] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:22:41.256] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:22:41.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:22:41.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:22:41.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:22:41.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:22:41.263] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:22:41.263] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:22:41.263] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:22:41.263] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:22:41.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:22:41.264] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:22:41.264] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:22:41.281] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:22:41.282] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:22:41.282] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:22:41.282] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-97a4bbcf-0991-442c-bb38-b198ae1d3b6d
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:22:41.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:22:41.282] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:22:41.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:22:41.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:22:41.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:22:41.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:22:41.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:22:41.282] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:22:41.745] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:22:41.745] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:22:41.745] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:22:41.749] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:22:41.749] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:22:41.749] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:22:41.754] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:22:41.754] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:22:41.754] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:22:41.754] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 35
[2024-09-30 14:22:41.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:22:41.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:22:41.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:22:41.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:22:41.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:22:41.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:22:41.755] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:22:41.755] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:22:41.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:22:41.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:22:41.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:22:41.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:22:41.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:22:41.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:22:41.947] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:22:41.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:22:41.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:22:41.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:22:41.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:22:41.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:22:41.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:22:41.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:22:41.952] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:22:41.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:22:41.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:22:41.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:22:41.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:22:41.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:22:41.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:22:42.256] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:22:42.256] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:22:42.256] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:22:42.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:22:42.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:22:42.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:22:48.052] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:22:48.052] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:22:48.052] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1858287.33 ms
[2024-09-30 14:22:48.052] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.67 ms /    21 runs   (    0.22 ms per token,  4492.94 tokens per second)
[2024-09-30 14:22:48.052] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2728.24 ms /    24 tokens (  113.68 ms per token,     8.80 tokens per second)
[2024-09-30 14:22:48.052] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3060.47 ms /    20 runs   (  153.02 ms per token,     6.53 tokens per second)
[2024-09-30 14:22:48.052] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1861352.95 ms /    44 tokens
[2024-09-30 14:22:48.055] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:22:48.055] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 112
[2024-09-30 14:22:48.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am an AI digital assistant designed to interact with you and provide information or assistance.<|end|>
[2024-09-30 14:22:48.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:22:48.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am an AI digital assistant designed to interact with you and provide information or assistance.
[2024-09-30 14:22:48.055] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:22:48.055] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:22:48.055] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:22:48.055] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:22:48.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:22:48.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:22:48.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:22:48.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:22:48.056] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:22:48.056] [info] rag_api_server in src/main.rs:517: response_body_size: 416
[2024-09-30 14:22:48.056] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:22:48.056] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:23:38.083] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36994, local_addr: 0.0.0.0:8080
[2024-09-30 14:23:38.084] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:23:38.084] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:23:38.084] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:23:38.084] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:23:38.084] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-ca23aa99-83eb-47d9-b63a-2254dd8cbe9f
[2024-09-30 14:23:38.084] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:23:38.084] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:23:38.084] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:23:38.084] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:23:38.084] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:23:38.085] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:23:38.085] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:23:38.085] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:23:38.085] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:23:38.085] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:23:38.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:23:38.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:23:38.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:23:38.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:23:38.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:23:38.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:23:38.088] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:23:38.088] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:23:38.088] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:23:38.090] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:23:38.090] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:23:38.093] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:23:38.093] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:23:38.093] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:23:38.095] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:23:38.095] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:23:38.095] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:23:38.122] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:23:38.122] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1911317.72 ms
[2024-09-30 14:23:38.122] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:23:38.122] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      25.44 ms /     6 tokens (    4.24 ms per token,   235.87 tokens per second)
[2024-09-30 14:23:38.122] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:23:38.122] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1911317.56 ms /     7 tokens
[2024-09-30 14:23:38.123] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:23:38.123] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:23:38.128] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:23:38.128] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:23:38.128] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:23:38.128] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:23:38.128] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:23:38.129] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:23:38.129] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:23:38.129] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:23:38.129] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:23:38.129] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:23:38.129] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:23:38.147] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:23:38.147] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:23:38.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:23:38.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:23:38.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:23:38.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:23:38.147] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:23:38.147] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:23:38.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-ca23aa99-83eb-47d9-b63a-2254dd8cbe9f
[2024-09-30 14:23:38.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:23:38.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:23:38.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:23:38.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:23:38.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:23:38.148] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:23:38.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:23:38.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:23:38.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:23:38.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:23:38.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:23:38.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:23:38.667] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:23:38.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:23:38.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:23:38.669] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:23:38.669] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:23:38.669] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:23:38.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:23:38.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:23:38.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:23:38.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:23:38.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:23:38.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:23:38.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:23:38.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:23:38.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:23:38.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:23:38.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:23:38.673] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:23:38.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:23:38.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:23:38.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:23:38.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:23:38.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:23:38.673] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:23:38.844] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:23:38.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:23:38.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:23:38.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:23:38.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:23:38.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:23:38.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:23:38.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:23:38.847] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:23:38.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:23:38.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:23:38.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:23:38.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:23:38.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:23:38.847] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:23:39.019] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:23:39.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:23:39.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:23:39.020] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:23:39.020] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:23:39.020] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:23:44.666] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:23:44.666] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:23:44.666] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1915010.94 ms
[2024-09-30 14:23:44.666] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.73 ms /    20 runs   (    0.24 ms per token,  4226.54 tokens per second)
[2024-09-30 14:23:44.666] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2688.62 ms /    24 tokens (  112.03 ms per token,     8.93 tokens per second)
[2024-09-30 14:23:44.666] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2950.73 ms /    19 runs   (  155.30 ms per token,     6.44 tokens per second)
[2024-09-30 14:23:44.666] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1917966.95 ms /    43 tokens
[2024-09-30 14:23:44.668] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:23:44.669] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 111
[2024-09-30 14:23:44.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence developed by Microsoft to assist users with information and tasks.<|end|>
[2024-09-30 14:23:44.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:23:44.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence developed by Microsoft to assist users with information and tasks.
[2024-09-30 14:23:44.669] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:23:44.669] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:23:44.669] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:23:44.669] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:23:44.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:23:44.669] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:23:44.669] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:23:44.669] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:23:44.670] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:23:44.670] [info] rag_api_server in src/main.rs:517: response_body_size: 415
[2024-09-30 14:23:44.670] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:23:44.670] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:24:34.694] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:41330, local_addr: 0.0.0.0:8080
[2024-09-30 14:24:34.695] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:24:34.695] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:24:34.695] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:24:34.695] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:24:34.696] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-6d100e2a-62b4-4b79-bccd-4805ff1dcb93
[2024-09-30 14:24:34.696] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:24:34.696] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:24:34.696] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:24:34.696] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:24:34.696] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:24:34.696] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:24:34.696] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:24:34.696] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:24:34.696] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:24:34.696] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:24:34.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:24:34.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:24:34.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:24:34.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:24:34.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:24:34.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:24:34.697] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:24:34.697] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:24:34.697] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:24:34.699] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:24:34.699] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:24:34.702] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:24:34.702] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:24:34.702] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:24:34.704] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:24:34.704] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:24:34.704] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:24:34.727] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:24:34.727] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1967922.34 ms
[2024-09-30 14:24:34.727] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:24:34.727] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.37 ms /     6 tokens (    3.73 ms per token,   268.19 tokens per second)
[2024-09-30 14:24:34.727] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:24:34.727] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1967921.56 ms /     7 tokens
[2024-09-30 14:24:34.727] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:24:34.727] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:24:34.732] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:24:34.732] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:24:34.732] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:24:34.732] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:24:34.732] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:24:34.732] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:24:34.732] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:24:34.732] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:24:34.733] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:24:34.733] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:24:34.733] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:24:34.750] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:24:34.750] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:24:34.750] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:24:34.750] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-6d100e2a-62b4-4b79-bccd-4805ff1dcb93
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:24:34.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:24:34.751] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:24:34.751] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:24:34.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:24:34.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:24:34.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:24:34.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:24:34.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:24:34.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:24:35.193] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:24:35.193] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:24:35.193] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:24:35.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:24:35.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:24:35.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:24:35.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:24:35.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:24:35.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:24:35.198] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:24:35.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:24:35.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:24:35.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:24:35.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:24:35.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:24:35.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:24:35.199] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:24:35.199] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:24:35.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:24:35.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:24:35.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:24:35.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:24:35.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:24:35.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:24:35.367] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:24:35.367] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:24:35.367] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:24:35.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:24:35.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:24:35.368] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:24:35.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:24:35.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:24:35.371] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:24:35.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:24:35.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:24:35.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:24:35.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:24:35.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:24:35.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:24:35.536] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:24:35.536] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:24:35.536] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:24:35.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:24:35.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:24:35.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:24:41.045] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:24:41.046] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:24:41.046] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1971433.52 ms
[2024-09-30 14:24:41.046] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.27 ms /    20 runs   (    0.21 ms per token,  4682.74 tokens per second)
[2024-09-30 14:24:41.046] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2594.38 ms /    24 tokens (  108.10 ms per token,     9.25 tokens per second)
[2024-09-30 14:24:41.046] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2908.44 ms /    19 runs   (  153.08 ms per token,     6.53 tokens per second)
[2024-09-30 14:24:41.046] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1974346.95 ms /    43 tokens
[2024-09-30 14:24:41.048] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:24:41.048] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 14:24:41.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:24:41.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:24:41.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:24:41.048] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:24:41.048] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:24:41.048] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:24:41.048] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:24:41.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:24:41.049] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:24:41.049] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:24:41.049] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:24:41.049] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:24:41.049] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 14:24:41.049] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:24:41.049] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:25:31.077] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:60084, local_addr: 0.0.0.0:8080
[2024-09-30 14:25:31.078] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:25:31.078] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:25:31.078] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:25:31.078] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:25:31.078] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-6f322aee-1f5d-4071-8e3f-322ea4ba945b
[2024-09-30 14:25:31.078] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:25:31.078] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:25:31.078] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:25:31.078] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:25:31.078] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:25:31.078] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:25:31.078] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:25:31.078] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:25:31.078] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:25:31.078] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:25:31.079] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:25:31.079] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:25:31.079] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:25:31.079] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:25:31.079] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:25:31.079] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:25:31.080] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:25:31.080] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:25:31.080] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:25:31.082] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:25:31.082] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:25:31.085] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:25:31.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:25:31.085] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:25:31.087] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:25:31.087] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:25:31.087] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:25:31.110] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:25:31.110] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2024305.08 ms
[2024-09-30 14:25:31.110] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:25:31.110] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.71 ms /     6 tokens (    3.62 ms per token,   276.42 tokens per second)
[2024-09-30 14:25:31.110] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:25:31.110] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2024304.56 ms /     7 tokens
[2024-09-30 14:25:31.110] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:25:31.110] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:25:31.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:25:31.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:25:31.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:25:31.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:25:31.116] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:25:31.116] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:25:31.116] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:25:31.116] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:25:31.116] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:25:31.116] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:25:31.116] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:25:31.137] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:25:31.137] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:25:31.137] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:25:31.137] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-6f322aee-1f5d-4071-8e3f-322ea4ba945b
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:25:31.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:25:31.137] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:25:31.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:25:31.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:25:31.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:25:31.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:25:31.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:25:31.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:25:31.553] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:25:31.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:25:31.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:25:31.555] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:25:31.555] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:25:31.555] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:25:31.558] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:25:31.558] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:25:31.558] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:25:31.558] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:25:31.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:25:31.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:25:31.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:25:31.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:25:31.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:25:31.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:25:31.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:25:31.559] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:25:31.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:25:31.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:25:31.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:25:31.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:25:31.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:25:31.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:25:31.713] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:25:31.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:25:31.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:25:31.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:25:31.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:25:31.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:25:31.717] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:25:31.717] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:25:31.717] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:25:31.717] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:25:31.717] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:25:31.717] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:25:31.717] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:25:31.717] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:25:31.717] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:25:31.888] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:25:31.888] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:25:31.888] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:25:31.889] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:25:31.889] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:25:31.889] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:25:40.024] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:25:40.024] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:25:40.024] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2027842.36 ms
[2024-09-30 14:25:40.024] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.42 ms /    25 runs   (    0.22 ms per token,  4610.84 tokens per second)
[2024-09-30 14:25:40.024] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2651.82 ms /    24 tokens (  110.49 ms per token,     9.05 tokens per second)
[2024-09-30 14:25:40.024] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    5476.50 ms /    24 runs   (  228.19 ms per token,     4.38 tokens per second)
[2024-09-30 14:25:40.024] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2033324.95 ms /    48 tokens
[2024-09-30 14:25:40.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:25:40.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 98
[2024-09-30 14:25:40.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, an AI designed to interact with you and provide assistance. How can I help today?<|end|>
[2024-09-30 14:25:40.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:25:40.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, an AI designed to interact with you and provide assistance. How can I help today?
[2024-09-30 14:25:40.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:25:40.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:25:40.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:25:40.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:25:40.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:25:40.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:25:40.027] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:25:40.028] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:25:40.028] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:25:40.028] [info] rag_api_server in src/main.rs:517: response_body_size: 402
[2024-09-30 14:25:40.028] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:25:40.028] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:26:30.053] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:56408, local_addr: 0.0.0.0:8080
[2024-09-30 14:26:30.054] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:26:30.054] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:26:30.054] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:26:30.054] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:26:30.054] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-412681a8-6655-4203-bb7d-ac84a4269a87
[2024-09-30 14:26:30.054] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:26:30.054] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:26:30.054] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:26:30.054] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:26:30.054] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:26:30.054] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:26:30.054] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:26:30.054] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:26:30.054] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:26:30.055] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:26:30.055] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:26:30.055] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:26:30.055] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:26:30.055] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:26:30.055] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:26:30.055] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:26:30.056] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:26:30.056] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:26:30.056] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:26:30.058] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:26:30.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:26:30.060] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:26:30.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:26:30.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:26:30.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:26:30.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:26:30.062] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:26:30.084] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:26:30.084] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2083279.21 ms
[2024-09-30 14:26:30.084] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:26:30.084] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      20.84 ms /     6 tokens (    3.47 ms per token,   287.87 tokens per second)
[2024-09-30 14:26:30.084] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:26:30.084] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2083278.56 ms /     7 tokens
[2024-09-30 14:26:30.084] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:26:30.085] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:26:30.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:26:30.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:26:30.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:26:30.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:26:30.090] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:26:30.090] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:26:30.090] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:26:30.090] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:26:30.090] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:26:30.090] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:26:30.090] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:26:30.107] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:26:30.107] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:26:30.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:26:30.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:26:30.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:26:30.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:26:30.108] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:26:30.108] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:26:30.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-412681a8-6655-4203-bb7d-ac84a4269a87
[2024-09-30 14:26:30.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:26:30.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:26:30.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:26:30.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:26:30.108] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:26:30.108] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:26:30.108] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:26:30.108] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:26:30.108] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:26:30.108] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:26:30.108] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:26:30.108] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:26:30.576] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:26:30.576] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:26:30.576] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:26:30.579] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:26:30.579] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:26:30.579] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:26:30.582] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:26:30.582] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:26:30.582] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:26:30.582] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:26:30.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:26:30.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:26:30.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:26:30.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:26:30.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:26:30.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:26:30.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:26:30.583] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:26:30.583] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:26:30.583] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:26:30.583] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:26:30.583] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:26:30.583] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:26:30.583] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:26:30.761] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:26:30.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:26:30.761] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:26:30.762] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:26:30.762] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:26:30.762] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:26:30.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:26:30.765] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:26:30.765] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:26:30.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:26:30.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:26:30.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:26:30.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:26:30.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:26:30.765] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:26:30.920] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:26:30.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:26:30.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:26:30.921] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:26:30.921] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:26:30.921] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:26:35.472] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:26:35.472] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:26:35.472] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2086874.57 ms
[2024-09-30 14:26:35.472] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       2.57 ms /    13 runs   (    0.20 ms per token,  5050.51 tokens per second)
[2024-09-30 14:26:35.472] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2651.59 ms /    24 tokens (  110.48 ms per token,     9.05 tokens per second)
[2024-09-30 14:26:35.472] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    1896.17 ms /    12 runs   (  158.01 ms per token,     6.33 tokens per second)
[2024-09-30 14:26:35.472] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2088773.95 ms /    36 tokens
[2024-09-30 14:26:35.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:26:35.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 47
[2024-09-30 14:26:35.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft.<|end|>
[2024-09-30 14:26:35.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:26:35.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft.
[2024-09-30 14:26:35.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:26:35.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:26:35.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:26:35.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:26:35.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:26:35.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:26:35.476] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:26:35.476] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:26:35.476] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:26:35.476] [info] rag_api_server in src/main.rs:517: response_body_size: 351
[2024-09-30 14:26:35.476] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:26:35.476] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:27:25.501] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:34876, local_addr: 0.0.0.0:8080
[2024-09-30 14:27:25.502] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:27:25.502] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:27:25.502] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:27:25.502] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:27:25.503] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-e8ea2fb3-4da8-4076-96a1-f005abf932db
[2024-09-30 14:27:25.503] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:27:25.503] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:27:25.503] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:27:25.503] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:27:25.503] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:27:25.503] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:27:25.503] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:27:25.503] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:27:25.503] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:27:25.503] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:27:25.503] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:27:25.503] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:27:25.503] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:27:25.503] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:27:25.503] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:27:25.503] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:27:25.504] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:27:25.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:27:25.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:27:25.506] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:27:25.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:27:25.509] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:27:25.509] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:27:25.509] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:27:25.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:27:25.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:27:25.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:27:25.534] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:27:25.534] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2138729.86 ms
[2024-09-30 14:27:25.534] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:27:25.534] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.56 ms /     6 tokens (    3.76 ms per token,   265.99 tokens per second)
[2024-09-30 14:27:25.534] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:27:25.534] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2138729.56 ms /     7 tokens
[2024-09-30 14:27:25.535] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:27:25.535] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:27:25.540] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:27:25.540] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:27:25.540] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:27:25.540] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:27:25.540] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:27:25.540] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:27:25.540] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:27:25.540] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:27:25.540] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:27:25.540] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:27:25.540] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:27:25.559] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:27:25.559] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:27:25.559] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:27:25.559] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-e8ea2fb3-4da8-4076-96a1-f005abf932db
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:27:25.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:27:25.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:27:25.560] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:27:25.560] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:27:25.560] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:27:25.560] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:27:25.560] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:27:25.560] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:27:25.560] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:27:26.005] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:27:26.005] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:27:26.005] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:27:26.008] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:27:26.008] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:27:26.008] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:27:26.011] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:27:26.011] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:27:26.011] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:27:26.012] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:27:26.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:27:26.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:27:26.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:27:26.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:27:26.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:27:26.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:27:26.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:27:26.012] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:27:26.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:27:26.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:27:26.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:27:26.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:27:26.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:27:26.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:27:26.191] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:27:26.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:27:26.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:27:26.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:27:26.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:27:26.191] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:27:26.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:27:26.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:27:26.195] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:27:26.195] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:27:26.195] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:27:26.195] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:27:26.195] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:27:26.195] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:27:26.195] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:27:26.355] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:27:26.355] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:27:26.355] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:27:26.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:27:26.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:27:26.356] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:27:31.944] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:27:31.944] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:27:31.944] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2142354.35 ms
[2024-09-30 14:27:31.944] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.09 ms /    20 runs   (    0.20 ms per token,  4886.39 tokens per second)
[2024-09-30 14:27:31.944] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2696.69 ms /    24 tokens (  112.36 ms per token,     8.90 tokens per second)
[2024-09-30 14:27:31.944] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2886.20 ms /    19 runs   (  151.91 ms per token,     6.58 tokens per second)
[2024-09-30 14:27:31.944] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2145244.95 ms /    43 tokens
[2024-09-30 14:27:31.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:27:31.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 14:27:31.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:27:31.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:27:31.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:27:31.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:27:31.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:27:31.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:27:31.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:27:31.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:27:31.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:27:31.947] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:27:31.948] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:27:31.948] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:27:31.948] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 14:27:31.948] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:27:31.948] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:28:21.973] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:60274, local_addr: 0.0.0.0:8080
[2024-09-30 14:28:21.974] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:28:21.974] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:28:21.974] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:28:21.974] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:28:21.974] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-56f94b44-d262-4745-a09b-a47c18a09bfe
[2024-09-30 14:28:21.974] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:28:21.974] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:28:21.974] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:28:21.974] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:28:21.974] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:28:21.974] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:28:21.974] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:28:21.974] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:28:21.974] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:28:21.974] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:28:21.974] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:28:21.974] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:28:21.974] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:28:21.974] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:28:21.974] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:28:21.974] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:28:21.976] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:28:21.976] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:28:21.976] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:28:21.978] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:28:21.978] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:28:21.982] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:28:21.982] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:28:21.982] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:28:21.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:28:21.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:28:21.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:28:22.008] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:28:22.008] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2195203.57 ms
[2024-09-30 14:28:22.008] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:28:22.008] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.28 ms /     6 tokens (    3.71 ms per token,   269.28 tokens per second)
[2024-09-30 14:28:22.008] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:28:22.008] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2195203.56 ms /     7 tokens
[2024-09-30 14:28:22.008] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:28:22.008] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:28:22.013] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:28:22.013] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:28:22.013] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:28:22.013] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:28:22.013] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:28:22.013] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:28:22.014] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:28:22.014] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:28:22.014] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:28:22.014] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:28:22.014] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:28:22.033] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:28:22.034] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:28:22.034] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:28:22.034] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-56f94b44-d262-4745-a09b-a47c18a09bfe
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:28:22.034] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:28:22.034] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:28:22.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:28:22.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:28:22.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:28:22.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:28:22.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:28:22.034] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:28:22.529] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:28:22.529] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:28:22.529] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:28:22.532] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:28:22.532] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:28:22.532] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:28:22.535] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:28:22.535] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:28:22.535] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:28:22.536] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:28:22.536] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:28:22.536] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:28:22.536] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:28:22.536] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:28:22.536] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:28:22.536] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:28:22.536] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:28:22.536] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:28:22.536] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:28:22.536] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:28:22.536] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:28:22.536] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:28:22.536] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:28:22.536] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:28:22.705] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:28:22.705] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:28:22.705] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:28:22.706] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:28:22.706] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:28:22.706] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:28:22.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:28:22.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:28:22.708] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:28:22.708] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:28:22.708] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:28:22.708] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:28:22.708] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:28:22.708] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:28:22.708] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:28:22.868] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:28:22.868] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:28:22.868] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:28:22.870] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:28:22.870] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:28:22.870] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:28:28.471] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:28:28.471] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:28:28.471] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2198887.17 ms
[2024-09-30 14:28:28.471] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.12 ms /    20 runs   (    0.21 ms per token,  4854.37 tokens per second)
[2024-09-30 14:28:28.471] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2715.64 ms /    24 tokens (  113.15 ms per token,     8.84 tokens per second)
[2024-09-30 14:28:28.471] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2880.70 ms /    19 runs   (  151.62 ms per token,     6.60 tokens per second)
[2024-09-30 14:28:28.471] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2201772.95 ms /    43 tokens
[2024-09-30 14:28:28.474] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:28:28.474] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 14:28:28.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:28:28.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:28:28.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:28:28.474] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:28:28.474] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:28:28.474] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:28:28.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:28:28.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:28:28.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:28:28.475] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:28:28.475] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:28:28.475] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:28:28.475] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 14:28:28.475] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:28:28.475] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:29:18.501] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:46400, local_addr: 0.0.0.0:8080
[2024-09-30 14:29:18.503] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:29:18.503] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:29:18.503] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:29:18.503] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:29:18.503] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-8cbecc5d-4258-494d-bb64-ccb29b98a702
[2024-09-30 14:29:18.503] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:29:18.503] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:29:18.504] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:29:18.504] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:29:18.504] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:29:18.504] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:29:18.504] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:29:18.504] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:29:18.504] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:29:18.504] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:29:18.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:29:18.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:29:18.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:29:18.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:29:18.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:29:18.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:29:18.506] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:29:18.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:29:18.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:29:18.507] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:29:18.507] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:29:18.507] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:29:18.508] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:29:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:29:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:29:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:29:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:29:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:29:18.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:29:18.511] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:29:18.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:29:18.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:29:18.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:29:18.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:29:18.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:29:18.536] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:29:18.536] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2251731.01 ms
[2024-09-30 14:29:18.536] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:29:18.536] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.10 ms /     6 tokens (    3.68 ms per token,   271.46 tokens per second)
[2024-09-30 14:29:18.536] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:29:18.536] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2251730.56 ms /     7 tokens
[2024-09-30 14:29:18.536] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:29:18.536] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:29:18.541] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:29:18.541] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:29:18.541] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:29:18.541] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:29:18.541] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:29:18.541] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:29:18.541] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:29:18.541] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:29:18.541] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:29:18.541] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:29:18.541] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:29:18.558] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:29:18.558] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:29:18.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:29:18.559] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:29:18.559] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-8cbecc5d-4258-494d-bb64-ccb29b98a702
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:29:18.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:29:18.559] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:29:18.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:29:18.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:29:18.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:29:18.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:29:18.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:29:18.559] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:29:19.004] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:29:19.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:29:19.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:29:19.007] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:29:19.007] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:29:19.007] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:29:19.011] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:29:19.011] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:29:19.011] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:29:19.011] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:29:19.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:29:19.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:29:19.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:29:19.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:29:19.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:29:19.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:29:19.012] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:29:19.012] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:29:19.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:29:19.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:29:19.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:29:19.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:29:19.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:29:19.012] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:29:19.174] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:29:19.175] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:29:19.175] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:29:19.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:29:19.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:29:19.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:29:19.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:29:19.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:29:19.179] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:29:19.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:29:19.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:29:19.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:29:19.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:29:19.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:29:19.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:29:19.346] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:29:19.346] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:29:19.346] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:29:19.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:29:19.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:29:19.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:29:24.028] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:29:24.028] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:29:24.028] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2255351.84 ms
[2024-09-30 14:29:24.028] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       2.76 ms /    13 runs   (    0.21 ms per token,  4710.14 tokens per second)
[2024-09-30 14:29:24.028] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2703.07 ms /    24 tokens (  112.63 ms per token,     8.88 tokens per second)
[2024-09-30 14:29:24.028] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    1974.30 ms /    12 runs   (  164.53 ms per token,     6.08 tokens per second)
[2024-09-30 14:29:24.028] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2257328.95 ms /    36 tokens
[2024-09-30 14:29:24.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:29:24.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 47
[2024-09-30 14:29:24.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft.<|end|>
[2024-09-30 14:29:24.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:29:24.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft.
[2024-09-30 14:29:24.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:29:24.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:29:24.031] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:29:24.031] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:29:24.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:29:24.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:29:24.031] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:29:24.031] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:29:24.031] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:29:24.031] [info] rag_api_server in src/main.rs:517: response_body_size: 351
[2024-09-30 14:29:24.031] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:29:24.031] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:30:14.055] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:39630, local_addr: 0.0.0.0:8080
[2024-09-30 14:30:14.056] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:30:14.056] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:30:14.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:30:14.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:30:14.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-fe2a68c2-9d07-4e4f-b4e7-86eef9ff9416
[2024-09-30 14:30:14.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:30:14.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:30:14.056] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:30:14.056] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:30:14.056] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:30:14.057] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:30:14.057] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:30:14.057] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:30:14.057] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:30:14.057] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:30:14.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:30:14.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:30:14.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:30:14.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:30:14.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:30:14.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:30:14.058] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:30:14.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:30:14.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:30:14.059] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:30:14.059] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:30:14.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:30:14.060] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:30:14.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:30:14.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:30:14.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:30:14.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:30:14.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:30:14.060] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:30:14.063] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:30:14.063] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:30:14.063] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:30:14.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:30:14.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:30:14.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:30:14.088] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:30:14.088] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2307283.62 ms
[2024-09-30 14:30:14.088] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:30:14.088] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.92 ms /     6 tokens (    3.65 ms per token,   273.74 tokens per second)
[2024-09-30 14:30:14.088] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:30:14.088] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2307283.56 ms /     7 tokens
[2024-09-30 14:30:14.088] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:30:14.089] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:30:14.093] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:30:14.094] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:30:14.094] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:30:14.094] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:30:14.094] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:30:14.094] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:30:14.094] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:30:14.095] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:30:14.095] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:30:14.095] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:30:14.095] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:30:14.113] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:30:14.114] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:30:14.114] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:30:14.114] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-fe2a68c2-9d07-4e4f-b4e7-86eef9ff9416
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:30:14.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:30:14.114] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:30:14.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:30:14.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:30:14.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:30:14.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:30:14.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:30:14.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:30:14.581] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:30:14.581] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:30:14.581] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:30:14.584] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:30:14.584] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:30:14.584] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:30:14.587] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:30:14.587] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:30:14.587] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:30:14.588] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 14:30:14.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:30:14.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:30:14.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:30:14.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:30:14.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:30:14.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:30:14.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:30:14.589] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:30:14.589] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:30:14.589] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:30:14.589] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:30:14.589] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:30:14.589] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:30:14.589] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:30:14.755] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:30:14.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:30:14.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:30:14.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:30:14.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:30:14.755] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:30:14.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:30:14.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:30:14.758] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:30:14.758] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:30:14.758] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:30:14.758] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:30:14.758] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:30:14.758] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:30:14.758] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:30:14.964] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:30:14.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:30:14.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:30:14.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:30:14.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:30:14.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:30:20.741] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:30:20.741] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:30:20.741] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2310857.28 ms
[2024-09-30 14:30:20.741] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.51 ms /    21 runs   (    0.21 ms per token,  4652.19 tokens per second)
[2024-09-30 14:30:20.741] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2590.08 ms /    24 tokens (  107.92 ms per token,     9.27 tokens per second)
[2024-09-30 14:30:20.741] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3179.87 ms /    20 runs   (  158.99 ms per token,     6.29 tokens per second)
[2024-09-30 14:30:20.741] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2314042.95 ms /    44 tokens
[2024-09-30 14:30:20.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:30:20.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 71
[2024-09-30 14:30:20.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am Phi, an AI developed to assist you. How may I help?<|end|>
[2024-09-30 14:30:20.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:30:20.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am Phi, an AI developed to assist you. How may I help?
[2024-09-30 14:30:20.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:30:20.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:30:20.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:30:20.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:30:20.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:30:20.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:30:20.745] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:30:20.745] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:30:20.745] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:30:20.745] [info] rag_api_server in src/main.rs:517: response_body_size: 375
[2024-09-30 14:30:20.745] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:30:20.745] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:31:10.769] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36874, local_addr: 0.0.0.0:8080
[2024-09-30 14:31:10.770] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:31:10.770] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:31:10.770] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:31:10.770] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:31:10.771] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-42ed3b63-5353-4f69-ad35-f5705e27dd45
[2024-09-30 14:31:10.771] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:31:10.771] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:31:10.771] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:31:10.771] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:31:10.771] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:31:10.771] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:31:10.771] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:31:10.772] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:31:10.772] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:31:10.772] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:31:10.772] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:31:10.772] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:31:10.772] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:31:10.772] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:31:10.772] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:31:10.772] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:31:10.775] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:31:10.775] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:31:10.775] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:31:10.777] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:31:10.777] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:31:10.777] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:31:10.778] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:31:10.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:31:10.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:31:10.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:31:10.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:31:10.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:31:10.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:31:10.783] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:31:10.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:31:10.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:31:10.785] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:31:10.785] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:31:10.785] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:31:10.818] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:31:10.818] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2364013.56 ms
[2024-09-30 14:31:10.818] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:31:10.818] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      31.49 ms /     6 tokens (    5.25 ms per token,   190.54 tokens per second)
[2024-09-30 14:31:10.818] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:31:10.818] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2364013.56 ms /     7 tokens
[2024-09-30 14:31:10.818] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:31:10.819] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:31:10.824] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:31:10.824] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:31:10.824] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:31:10.824] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:31:10.825] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:31:10.825] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:31:10.825] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:31:10.825] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:31:10.825] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:31:10.825] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:31:10.825] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:31:10.842] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:31:10.843] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:31:10.843] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:31:10.843] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-42ed3b63-5353-4f69-ad35-f5705e27dd45
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:31:10.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:31:10.843] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:31:10.843] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:31:10.843] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:31:10.843] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:31:10.843] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:31:10.843] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:31:10.843] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:31:11.568] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:31:11.569] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:31:11.569] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:31:11.573] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:31:11.573] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:31:11.573] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:31:11.579] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:31:11.579] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:31:11.579] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:31:11.580] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:31:11.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:31:11.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:31:11.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:31:11.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:31:11.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:31:11.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:31:11.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:31:11.580] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:31:11.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:31:11.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:31:11.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:31:11.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:31:11.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:31:11.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:31:11.832] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:31:11.832] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:31:11.832] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:31:11.834] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:31:11.834] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:31:11.834] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:31:11.839] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:31:11.839] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:31:11.839] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:31:11.839] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:31:11.839] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:31:11.839] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:31:11.839] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:31:11.839] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:31:11.839] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:31:12.100] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:31:12.100] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:31:12.100] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:31:12.101] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:31:12.101] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:31:12.101] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:31:18.703] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:31:18.703] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:31:18.703] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2368410.28 ms
[2024-09-30 14:31:18.703] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.10 ms /    21 runs   (    0.24 ms per token,  4117.65 tokens per second)
[2024-09-30 14:31:18.703] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3007.55 ms /    24 tokens (  125.31 ms per token,     7.98 tokens per second)
[2024-09-30 14:31:18.703] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3588.30 ms /    20 runs   (  179.41 ms per token,     5.57 tokens per second)
[2024-09-30 14:31:18.703] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2372004.95 ms /    44 tokens
[2024-09-30 14:31:18.705] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:31:18.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 72
[2024-09-30 14:31:18.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:31:18.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:31:18.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:31:18.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:31:18.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:31:18.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:31:18.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:31:18.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:31:18.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:31:18.706] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:31:18.706] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:31:18.706] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:31:18.706] [info] rag_api_server in src/main.rs:517: response_body_size: 376
[2024-09-30 14:31:18.706] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:31:18.707] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:32:08.731] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:59050, local_addr: 0.0.0.0:8080
[2024-09-30 14:32:08.732] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:32:08.732] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:32:08.732] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:32:08.732] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:32:08.732] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-288b0c36-f90d-4eed-8db5-118ceddb7543
[2024-09-30 14:32:08.732] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:32:08.732] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:32:08.732] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:32:08.732] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:32:08.732] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:32:08.732] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:32:08.732] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:32:08.732] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:32:08.732] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:32:08.732] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:32:08.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:32:08.733] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:32:08.733] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:32:08.733] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:32:08.733] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:32:08.733] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:32:08.734] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:32:08.734] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:32:08.734] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:32:08.736] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:32:08.736] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:32:08.740] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:32:08.740] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:32:08.740] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:32:08.742] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:32:08.742] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:32:08.742] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:32:08.766] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:32:08.766] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2421961.14 ms
[2024-09-30 14:32:08.766] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:32:08.766] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.91 ms /     6 tokens (    3.65 ms per token,   273.79 tokens per second)
[2024-09-30 14:32:08.766] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:32:08.766] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2421960.56 ms /     7 tokens
[2024-09-30 14:32:08.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:32:08.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:32:08.773] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:32:08.773] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:32:08.773] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:32:08.773] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:32:08.773] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:32:08.773] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:32:08.773] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:32:08.774] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:32:08.774] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:32:08.774] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:32:08.774] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:32:08.792] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:32:08.792] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:32:08.792] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:32:08.792] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-288b0c36-f90d-4eed-8db5-118ceddb7543
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:32:08.792] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:32:08.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:32:08.793] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:32:08.793] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:32:08.793] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:32:08.793] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:32:08.793] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:32:08.793] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:32:08.793] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:32:09.272] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:32:09.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:32:09.272] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:32:09.275] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:32:09.275] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:32:09.275] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:32:09.277] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:32:09.278] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:32:09.278] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:32:09.278] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:32:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:32:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:32:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:32:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:32:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:32:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:32:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:32:09.278] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:32:09.278] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:32:09.278] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:32:09.278] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:32:09.278] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:32:09.278] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:32:09.278] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:32:09.446] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:32:09.446] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:32:09.446] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:32:09.447] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:32:09.447] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:32:09.447] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:32:09.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:32:09.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:32:09.450] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:32:09.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:32:09.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:32:09.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:32:09.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:32:09.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:32:09.450] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:32:09.607] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:32:09.607] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:32:09.607] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:32:09.609] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:32:09.609] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:32:09.609] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:32:16.819] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:32:16.819] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:32:16.819] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2425667.45 ms
[2024-09-30 14:32:16.819] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.45 ms /    29 runs   (    0.22 ms per token,  4495.43 tokens per second)
[2024-09-30 14:32:16.819] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2756.91 ms /    24 tokens (  114.87 ms per token,     8.71 tokens per second)
[2024-09-30 14:32:16.819] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4445.59 ms /    28 runs   (  158.77 ms per token,     6.30 tokens per second)
[2024-09-30 14:32:16.819] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2430120.95 ms /    52 tokens
[2024-09-30 14:32:16.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:32:16.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 156
[2024-09-30 14:32:16.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence created by Microsoft designed to assist and interact with users through conversation. How can I help you today?<|end|>
[2024-09-30 14:32:16.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:32:16.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence created by Microsoft designed to assist and interact with users through conversation. How can I help you today?
[2024-09-30 14:32:16.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:32:16.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:32:16.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:32:16.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 29
[2024-09-30 14:32:16.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 29
[2024-09-30 14:32:16.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:32:16.823] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:32:16.823] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:32:16.823] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:32:16.823] [info] rag_api_server in src/main.rs:517: response_body_size: 460
[2024-09-30 14:32:16.823] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:32:16.823] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:33:06.850] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:55116, local_addr: 0.0.0.0:8080
[2024-09-30 14:33:06.850] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:33:06.850] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:33:06.851] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:33:06.851] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:33:06.851] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-c1ea807e-a8a3-4f2d-ab57-eaac26e0fc01
[2024-09-30 14:33:06.851] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:33:06.851] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:33:06.851] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:33:06.851] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:33:06.851] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:33:06.851] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:33:06.851] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:33:06.851] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:33:06.851] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:33:06.851] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:33:06.851] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:33:06.851] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:33:06.851] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:33:06.851] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:33:06.851] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:33:06.852] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:33:06.853] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:33:06.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:33:06.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:33:06.855] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:33:06.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:33:06.859] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:33:06.859] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:33:06.859] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:33:06.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:33:06.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:33:06.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:33:06.885] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:33:06.885] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2480080.65 ms
[2024-09-30 14:33:06.885] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:33:06.885] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.48 ms /     6 tokens (    3.91 ms per token,   255.55 tokens per second)
[2024-09-30 14:33:06.885] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:33:06.885] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2480080.56 ms /     7 tokens
[2024-09-30 14:33:06.886] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:33:06.886] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:33:06.892] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:33:06.892] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:33:06.892] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:33:06.892] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:33:06.892] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:33:06.892] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:33:06.892] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:33:06.892] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:33:06.892] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:33:06.892] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:33:06.892] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:33:06.909] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:33:06.909] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:33:06.909] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:33:06.909] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-c1ea807e-a8a3-4f2d-ab57-eaac26e0fc01
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:33:06.909] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:33:06.910] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:33:06.910] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:33:06.910] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:33:06.910] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:33:06.910] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:33:06.910] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:33:06.910] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:33:06.910] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:33:07.331] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:33:07.331] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:33:07.331] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:33:07.334] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:33:07.334] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:33:07.334] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:33:07.336] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:33:07.336] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:33:07.336] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:33:07.336] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 29
[2024-09-30 14:33:07.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:33:07.336] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:33:07.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:33:07.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:33:07.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:33:07.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:33:07.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:33:07.337] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:33:07.337] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:33:07.337] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:33:07.337] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:33:07.337] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:33:07.337] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:33:07.337] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:33:07.499] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:33:07.499] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:33:07.499] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:33:07.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:33:07.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:33:07.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:33:07.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:33:07.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:33:07.505] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:33:07.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:33:07.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:33:07.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:33:07.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:33:07.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:33:07.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:33:07.685] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:33:07.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:33:07.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:33:07.687] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:33:07.687] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:33:07.687] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:33:13.517] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:33:13.517] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:33:13.517] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2483705.54 ms
[2024-09-30 14:33:13.517] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.29 ms /    21 runs   (    0.20 ms per token,  4897.39 tokens per second)
[2024-09-30 14:33:13.517] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2716.35 ms /    24 tokens (  113.18 ms per token,     8.84 tokens per second)
[2024-09-30 14:33:13.517] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3108.17 ms /    20 runs   (  155.41 ms per token,     6.43 tokens per second)
[2024-09-30 14:33:13.517] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2486818.95 ms /    44 tokens
[2024-09-30 14:33:13.521] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:33:13.521] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 74
[2024-09-30 14:33:13.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:33:13.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:33:13.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:33:13.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:33:13.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:33:13.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:33:13.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:33:13.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:33:13.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:33:13.522] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:33:13.522] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:33:13.522] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:33:13.522] [info] rag_api_server in src/main.rs:517: response_body_size: 378
[2024-09-30 14:33:13.522] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:33:13.522] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:34:03.585] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:54392, local_addr: 0.0.0.0:8080
[2024-09-30 14:34:03.586] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:34:03.586] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:34:03.586] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:34:03.586] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:34:03.586] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-b9eee8fd-2171-49f4-a911-988adc379172
[2024-09-30 14:34:03.586] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:34:03.586] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:34:03.586] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:34:03.586] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:34:03.586] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:34:03.586] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:34:03.586] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:34:03.586] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:34:03.586] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:34:03.586] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:34:03.587] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:34:03.587] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:34:03.587] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:34:03.587] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:34:03.587] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:34:03.587] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:34:03.588] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:34:03.588] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:34:03.588] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:34:03.591] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:34:03.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:34:03.597] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:34:03.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:34:03.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:34:03.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:34:03.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:34:03.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:34:03.633] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:34:03.633] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2536828.79 ms
[2024-09-30 14:34:03.633] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:34:03.633] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      33.15 ms /     6 tokens (    5.52 ms per token,   181.00 tokens per second)
[2024-09-30 14:34:03.633] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:34:03.633] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2536828.56 ms /     7 tokens
[2024-09-30 14:34:03.634] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:34:03.634] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:34:03.640] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:34:03.640] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:34:03.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:34:03.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:34:03.641] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:34:03.641] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:34:03.641] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:34:03.641] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:34:03.641] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:34:03.641] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:34:03.641] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:34:03.660] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:34:03.661] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:34:03.661] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:34:03.661] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-b9eee8fd-2171-49f4-a911-988adc379172
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:34:03.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:34:03.661] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:34:03.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:34:03.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:34:03.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:34:03.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:34:03.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:34:03.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:34:04.110] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:34:04.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:34:04.110] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:34:04.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:34:04.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:34:04.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:34:04.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:34:04.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:34:04.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:34:04.117] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:34:04.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:34:04.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:34:04.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:34:04.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:34:04.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:34:04.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:34:04.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:34:04.117] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:34:04.117] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:34:04.117] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:34:04.117] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:34:04.117] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:34:04.117] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:34:04.117] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:34:04.284] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:34:04.284] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:34:04.284] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:34:04.284] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:34:04.284] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:34:04.284] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:34:04.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:34:04.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:34:04.289] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:34:04.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:34:04.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:34:04.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:34:04.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:34:04.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:34:04.289] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:34:04.457] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:34:04.457] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:34:04.457] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:34:04.457] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:34:04.457] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:34:04.457] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:34:10.130] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:34:10.130] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:34:10.130] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2540371.88 ms
[2024-09-30 14:34:10.130] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.04 ms /    20 runs   (    0.20 ms per token,  4952.95 tokens per second)
[2024-09-30 14:34:10.130] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2612.56 ms /    24 tokens (  108.86 ms per token,     9.19 tokens per second)
[2024-09-30 14:34:10.130] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3055.15 ms /    19 runs   (  160.80 ms per token,     6.22 tokens per second)
[2024-09-30 14:34:10.130] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2543431.95 ms /    43 tokens
[2024-09-30 14:34:10.133] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:34:10.133] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 14:34:10.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:34:10.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:34:10.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:34:10.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:34:10.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:34:10.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:34:10.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:34:10.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:34:10.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:34:10.134] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:34:10.134] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:34:10.134] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:34:10.134] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 14:34:10.134] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:34:10.135] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:35:00.158] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:44752, local_addr: 0.0.0.0:8080
[2024-09-30 14:35:00.159] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:35:00.159] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:35:00.159] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:35:00.159] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:35:00.159] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-dab1d5cd-2ed2-40db-a1dd-8e1dee2f0b07
[2024-09-30 14:35:00.159] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:35:00.159] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:35:00.159] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:35:00.159] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:35:00.159] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:35:00.159] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:35:00.159] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:35:00.159] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:35:00.159] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:35:00.159] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:35:00.159] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:35:00.159] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:35:00.159] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:35:00.159] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:00.159] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:35:00.159] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:00.161] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:35:00.161] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:35:00.161] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:00.163] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:35:00.163] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:00.167] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:35:00.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:35:00.167] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:35:00.169] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:35:00.169] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:35:00.169] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:00.193] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:35:00.193] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2593388.77 ms
[2024-09-30 14:35:00.193] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:35:00.193] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.64 ms /     6 tokens (    3.94 ms per token,   253.79 tokens per second)
[2024-09-30 14:35:00.193] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:35:00.193] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2593388.56 ms /     7 tokens
[2024-09-30 14:35:00.194] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:35:00.194] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:35:00.200] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:35:00.200] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:35:00.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:35:00.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:35:00.201] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:35:00.201] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:35:00.201] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:35:00.201] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:35:00.201] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:35:00.201] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:35:00.201] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:35:00.220] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:35:00.220] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:35:00.220] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:35:00.220] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-dab1d5cd-2ed2-40db-a1dd-8e1dee2f0b07
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:35:00.220] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:00.221] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:35:00.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:35:00.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:35:00.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:35:00.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:00.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:35:00.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:00.681] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:35:00.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:35:00.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:35:00.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:35:00.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:35:00.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:00.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:00.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:35:00.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:35:00.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:35:00.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:35:00.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:35:00.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:35:00.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:35:00.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:35:00.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:35:00.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:00.688] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:35:00.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:35:00.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:35:00.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:35:00.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:00.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:35:00.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:00.857] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:35:00.857] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:35:00.857] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:35:00.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:35:00.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:35:00.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:00.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:35:00.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:00.861] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:35:00.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:35:00.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:35:00.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:35:00.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:00.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:35:00.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:01.020] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:35:01.020] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:35:01.020] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:35:01.021] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:35:01.021] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:35:01.021] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:07.025] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:35:07.025] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:35:07.025] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2596895.56 ms
[2024-09-30 14:35:07.025] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.78 ms /    24 runs   (    0.20 ms per token,  5018.82 tokens per second)
[2024-09-30 14:35:07.025] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2572.46 ms /    24 tokens (  107.19 ms per token,     9.33 tokens per second)
[2024-09-30 14:35:07.025] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3425.33 ms /    23 runs   (  148.93 ms per token,     6.71 tokens per second)
[2024-09-30 14:35:07.025] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2600326.95 ms /    47 tokens
[2024-09-30 14:35:07.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:35:07.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 86
[2024-09-30 14:35:07.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, a sophisticated AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:35:07.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:35:07.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, a sophisticated AI developed by Microsoft. How can I help you today?
[2024-09-30 14:35:07.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:07.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:35:07.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:35:07.028] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 14:35:07.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 24
[2024-09-30 14:35:07.029] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:35:07.029] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:35:07.029] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:35:07.029] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:35:07.029] [info] rag_api_server in src/main.rs:517: response_body_size: 390
[2024-09-30 14:35:07.030] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:35:07.030] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:35:57.055] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:47912, local_addr: 0.0.0.0:8080
[2024-09-30 14:35:57.056] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:35:57.056] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:35:57.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:35:57.056] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:35:57.057] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-43a44ca2-0254-48b3-b25c-5c2713aa39ef
[2024-09-30 14:35:57.057] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:35:57.057] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:35:57.057] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:35:57.057] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:35:57.057] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:35:57.057] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:35:57.057] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:35:57.057] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:35:57.057] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:35:57.057] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:35:57.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:35:57.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:35:57.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:35:57.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:57.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:35:57.058] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:57.061] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:35:57.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:35:57.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:35:57.063] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:35:57.063] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:35:57.063] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:57.064] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:35:57.064] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:35:57.064] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:35:57.064] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:35:57.064] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:57.064] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:35:57.064] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:57.073] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:35:57.073] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:35:57.073] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:35:57.075] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:35:57.075] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:35:57.075] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:57.097] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:35:57.097] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2650292.96 ms
[2024-09-30 14:35:57.098] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:35:57.098] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.79 ms /     6 tokens (    3.63 ms per token,   275.37 tokens per second)
[2024-09-30 14:35:57.098] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:35:57.098] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2650292.56 ms /     7 tokens
[2024-09-30 14:35:57.098] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:35:57.098] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:35:57.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:35:57.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:35:57.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:35:57.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:35:57.103] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:35:57.103] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:35:57.104] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:35:57.104] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:35:57.104] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:35:57.104] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:35:57.104] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:35:57.123] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:35:57.123] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:35:57.124] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:35:57.124] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-43a44ca2-0254-48b3-b25c-5c2713aa39ef
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:35:57.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:57.124] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:35:57.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:35:57.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:35:57.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:35:57.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:57.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:35:57.124] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:57.684] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:35:57.684] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:35:57.684] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:35:57.687] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:35:57.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:35:57.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:57.691] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:57.691] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:35:57.691] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:35:57.691] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 14:35:57.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:35:57.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:35:57.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:35:57.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:35:57.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:35:57.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:35:57.692] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:57.692] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:35:57.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:35:57.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:35:57.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:35:57.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:57.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:35:57.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:57.947] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:35:57.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:35:57.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:35:57.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:35:57.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:35:57.948] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:35:57.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:35:57.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:35:57.951] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:35:57.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:35:57.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:35:57.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:35:57.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:35:57.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:35:57.951] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:35:58.177] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:35:58.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:35:58.177] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:35:58.178] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:35:58.178] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:35:58.178] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:36:04.855] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:36:04.855] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:36:04.855] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2654494.76 ms
[2024-09-30 14:36:04.855] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.16 ms /    23 runs   (    0.22 ms per token,  4454.77 tokens per second)
[2024-09-30 14:36:04.855] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3014.88 ms /    24 tokens (  125.62 ms per token,     7.96 tokens per second)
[2024-09-30 14:36:04.855] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3655.85 ms /    22 runs   (  166.17 ms per token,     6.02 tokens per second)
[2024-09-30 14:36:04.855] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2658156.95 ms /    46 tokens
[2024-09-30 14:36:04.859] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:36:04.859] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 81
[2024-09-30 14:36:04.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I'm Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:36:04.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:36:04.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I'm Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:36:04.860] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:36:04.860] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:36:04.860] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:36:04.860] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 23
[2024-09-30 14:36:04.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 23
[2024-09-30 14:36:04.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:36:04.860] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:36:04.860] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:36:04.860] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:36:04.860] [info] rag_api_server in src/main.rs:517: response_body_size: 385
[2024-09-30 14:36:04.860] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:36:04.860] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:36:54.884] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:43600, local_addr: 0.0.0.0:8080
[2024-09-30 14:36:54.885] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:36:54.885] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:36:54.885] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:36:54.885] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:36:54.885] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-7113d599-6957-4e78-b3aa-e4d02c33988a
[2024-09-30 14:36:54.885] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:36:54.885] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:36:54.885] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:36:54.885] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:36:54.885] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:36:54.886] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:36:54.886] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:36:54.886] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:36:54.886] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:36:54.886] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:36:54.886] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:36:54.886] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:36:54.886] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:36:54.886] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:36:54.886] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:36:54.886] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:36:54.888] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:36:54.888] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:36:54.888] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:36:54.891] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:36:54.891] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:36:54.895] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:36:54.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:36:54.895] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:36:54.897] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:36:54.897] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:36:54.897] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:36:54.922] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:36:54.922] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2708117.09 ms
[2024-09-30 14:36:54.922] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:36:54.922] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.88 ms /     6 tokens (    3.98 ms per token,   251.28 tokens per second)
[2024-09-30 14:36:54.922] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:36:54.922] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2708116.56 ms /     7 tokens
[2024-09-30 14:36:54.922] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:36:54.922] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:36:54.928] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:36:54.928] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:36:54.928] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:36:54.928] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:36:54.929] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:36:54.929] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:36:54.929] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:36:54.929] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:36:54.929] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:36:54.929] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:36:54.929] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:36:54.945] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:36:54.945] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:36:54.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:36:54.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:36:54.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:36:54.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:36:54.945] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:36:54.945] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:36:54.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-7113d599-6957-4e78-b3aa-e4d02c33988a
[2024-09-30 14:36:54.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:36:54.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:36:54.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:36:54.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:36:54.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:36:54.946] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:36:54.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:36:54.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:36:54.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:36:54.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:36:54.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:36:54.946] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:36:55.401] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:36:55.401] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:36:55.401] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:36:55.404] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:36:55.404] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:36:55.404] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:36:55.407] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:36:55.407] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:36:55.407] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:36:55.407] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 23
[2024-09-30 14:36:55.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:36:55.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:36:55.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:36:55.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:36:55.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:36:55.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:36:55.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:36:55.407] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:36:55.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:36:55.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:36:55.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:36:55.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:36:55.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:36:55.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:36:55.598] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:36:55.598] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:36:55.598] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:36:55.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:36:55.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:36:55.599] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:36:55.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:36:55.602] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:36:55.602] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:36:55.602] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:36:55.602] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:36:55.602] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:36:55.602] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:36:55.602] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:36:55.602] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:36:55.779] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:36:55.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:36:55.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:36:55.780] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:36:55.780] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:36:55.780] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:37:01.464] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:37:01.464] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:37:01.464] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2711704.37 ms
[2024-09-30 14:37:01.464] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.17 ms /    20 runs   (    0.21 ms per token,  4791.57 tokens per second)
[2024-09-30 14:37:01.464] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2622.53 ms /    24 tokens (  109.27 ms per token,     9.15 tokens per second)
[2024-09-30 14:37:01.464] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3055.98 ms /    19 runs   (  160.84 ms per token,     6.22 tokens per second)
[2024-09-30 14:37:01.464] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2714764.95 ms /    43 tokens
[2024-09-30 14:37:01.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:37:01.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 14:37:01.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:37:01.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:37:01.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:37:01.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:37:01.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:37:01.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:37:01.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:37:01.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:37:01.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:37:01.470] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:37:01.470] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:37:01.470] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:37:01.470] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 14:37:01.470] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:37:01.470] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:37:51.496] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:51580, local_addr: 0.0.0.0:8080
[2024-09-30 14:37:51.496] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:37:51.496] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:37:51.496] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:37:51.496] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:37:51.496] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-ccffd544-297a-4bab-9a9f-a124b8542540
[2024-09-30 14:37:51.497] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:37:51.497] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:37:51.497] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:37:51.497] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:37:51.497] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:37:51.497] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:37:51.497] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:37:51.497] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:37:51.497] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:37:51.497] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:37:51.497] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:37:51.497] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:37:51.497] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:37:51.497] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:37:51.497] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:37:51.497] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:37:51.499] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:37:51.499] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:37:51.499] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:37:51.501] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:37:51.501] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:37:51.501] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:37:51.502] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:37:51.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:37:51.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:37:51.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:37:51.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:37:51.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:37:51.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:37:51.506] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:37:51.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:37:51.506] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:37:51.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:37:51.509] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:37:51.509] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:37:51.533] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:37:51.533] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2764728.94 ms
[2024-09-30 14:37:51.533] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:37:51.533] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      24.26 ms /     6 tokens (    4.04 ms per token,   247.33 tokens per second)
[2024-09-30 14:37:51.533] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:37:51.533] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2764728.56 ms /     7 tokens
[2024-09-30 14:37:51.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:37:51.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:37:51.539] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:37:51.539] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:37:51.539] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:37:51.539] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:37:51.539] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:37:51.539] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:37:51.539] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:37:51.539] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:37:51.539] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:37:51.539] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:37:51.539] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:37:51.557] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:37:51.557] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:37:51.557] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:37:51.557] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-ccffd544-297a-4bab-9a9f-a124b8542540
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:37:51.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:37:51.557] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:37:51.557] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:37:51.557] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:37:51.557] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:37:51.557] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:37:51.557] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:37:51.557] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:37:52.051] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:37:52.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:37:52.051] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:37:52.054] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:37:52.054] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:37:52.054] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:37:52.057] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:37:52.057] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:37:52.057] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:37:52.057] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:37:52.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:37:52.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:37:52.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:37:52.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:37:52.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:37:52.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:37:52.057] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:37:52.057] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:37:52.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:37:52.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:37:52.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:37:52.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:37:52.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:37:52.057] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:37:52.221] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:37:52.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:37:52.221] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:37:52.222] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:37:52.222] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:37:52.222] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:37:52.225] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:37:52.225] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:37:52.225] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:37:52.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:37:52.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:37:52.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:37:52.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:37:52.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:37:52.225] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:37:52.396] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:37:52.396] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:37:52.396] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:37:52.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:37:52.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:37:52.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:37:59.527] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:37:59.527] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:37:59.527] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2768659.58 ms
[2024-09-30 14:37:59.527] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.44 ms /    25 runs   (    0.22 ms per token,  4594.74 tokens per second)
[2024-09-30 14:37:59.527] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2960.40 ms /    24 tokens (  123.35 ms per token,     8.11 tokens per second)
[2024-09-30 14:37:59.527] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4162.49 ms /    24 runs   (  173.44 ms per token,     5.77 tokens per second)
[2024-09-30 14:37:59.527] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2772828.95 ms /    48 tokens
[2024-09-30 14:37:59.530] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:37:59.530] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 107
[2024-09-30 14:37:59.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft to assist and provide information. How can I help you today?<|end|>
[2024-09-30 14:37:59.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:37:59.530] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft to assist and provide information. How can I help you today?
[2024-09-30 14:37:59.530] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:37:59.530] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:37:59.530] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:37:59.531] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:37:59.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:37:59.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:37:59.531] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:37:59.531] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:37:59.531] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:37:59.531] [info] rag_api_server in src/main.rs:517: response_body_size: 411
[2024-09-30 14:37:59.531] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:37:59.531] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:38:49.561] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:56958, local_addr: 0.0.0.0:8080
[2024-09-30 14:38:49.562] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:38:49.562] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:38:49.562] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:38:49.562] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:38:49.562] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-748a6e79-cc98-4e2a-84c7-e57fee044515
[2024-09-30 14:38:49.562] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:38:49.562] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:38:49.562] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:38:49.563] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:38:49.563] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:38:49.563] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:38:49.563] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:38:49.563] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:38:49.563] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:38:49.563] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:38:49.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:38:49.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:38:49.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:38:49.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:38:49.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:38:49.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:38:49.565] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:38:49.565] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:38:49.565] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:38:49.566] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:38:49.566] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:38:49.566] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:38:49.567] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:38:49.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:38:49.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:38:49.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:38:49.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:38:49.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:38:49.567] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:38:49.572] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:38:49.572] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:38:49.572] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:38:49.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:38:49.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:38:49.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:38:49.597] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:38:49.597] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2822792.27 ms
[2024-09-30 14:38:49.597] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:38:49.597] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.97 ms /     6 tokens (    3.66 ms per token,   273.15 tokens per second)
[2024-09-30 14:38:49.597] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:38:49.597] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2822791.56 ms /     7 tokens
[2024-09-30 14:38:49.597] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:38:49.597] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:38:49.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:38:49.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:38:49.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:38:49.604] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:38:49.604] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:38:49.604] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:38:49.604] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:38:49.604] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:38:49.604] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:38:49.604] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:38:49.605] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:38:49.622] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:38:49.622] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:38:49.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:38:49.623] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:38:49.623] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-748a6e79-cc98-4e2a-84c7-e57fee044515
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:38:49.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:38:49.623] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:38:49.623] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:38:49.623] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:38:49.623] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:38:49.623] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:38:49.623] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:38:49.623] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:38:50.084] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:38:50.084] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:38:50.084] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:38:50.087] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:38:50.087] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:38:50.087] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:38:50.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:38:50.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:38:50.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:38:50.090] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 14:38:50.090] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:38:50.090] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:38:50.090] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:38:50.090] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:38:50.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:38:50.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:38:50.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:38:50.091] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:38:50.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:38:50.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:38:50.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:38:50.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:38:50.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:38:50.091] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:38:50.252] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:38:50.252] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:38:50.252] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:38:50.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:38:50.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:38:50.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:38:50.257] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:38:50.257] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:38:50.257] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:38:50.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:38:50.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:38:50.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:38:50.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:38:50.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:38:50.257] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:38:50.434] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:38:50.434] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:38:50.434] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:38:50.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:38:50.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:38:50.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:38:56.886] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:38:56.886] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:38:56.886] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2826333.34 ms
[2024-09-30 14:38:56.886] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.87 ms /    26 runs   (    0.23 ms per token,  4426.29 tokens per second)
[2024-09-30 14:38:56.886] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2596.70 ms /    24 tokens (  108.20 ms per token,     9.24 tokens per second)
[2024-09-30 14:38:56.886] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3847.29 ms /    25 runs   (  153.89 ms per token,     6.50 tokens per second)
[2024-09-30 14:38:56.886] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2830187.95 ms /    49 tokens
[2024-09-30 14:38:56.890] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:38:56.890] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 110
[2024-09-30 14:38:56.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft to assist with information and tasks. How can I help you today?<|end|>
[2024-09-30 14:38:56.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:38:56.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft to assist with information and tasks. How can I help you today?
[2024-09-30 14:38:56.891] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:38:56.891] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:38:56.891] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:38:56.891] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 26
[2024-09-30 14:38:56.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 26
[2024-09-30 14:38:56.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:38:56.891] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:38:56.892] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:38:56.892] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:38:56.892] [info] rag_api_server in src/main.rs:517: response_body_size: 414
[2024-09-30 14:38:56.892] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:38:56.892] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:39:46.917] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:54206, local_addr: 0.0.0.0:8080
[2024-09-30 14:39:46.918] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:39:46.918] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:39:46.918] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:39:46.918] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:39:46.918] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-07e0c594-593d-4bbf-ad19-f12674942679
[2024-09-30 14:39:46.918] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:39:46.918] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:39:46.918] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:39:46.918] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:39:46.918] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:39:46.919] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:39:46.919] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:39:46.919] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:39:46.919] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:39:46.919] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:39:46.919] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:39:46.919] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:39:46.919] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:39:46.919] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:39:46.919] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:39:46.919] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:39:46.920] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:39:46.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:39:46.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:39:46.922] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:39:46.922] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:39:46.925] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:39:46.925] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:39:46.925] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:39:46.926] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:39:46.926] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:39:46.926] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:39:46.962] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:39:46.962] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2880157.60 ms
[2024-09-30 14:39:46.962] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:39:46.962] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      34.68 ms /     6 tokens (    5.78 ms per token,   173.01 tokens per second)
[2024-09-30 14:39:46.962] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:39:46.962] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2880157.56 ms /     7 tokens
[2024-09-30 14:39:46.963] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:39:46.963] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:39:46.967] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:39:46.968] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:39:46.968] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:39:46.968] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:39:46.968] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:39:46.968] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:39:46.968] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:39:46.968] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:39:46.968] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:39:46.968] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:39:46.968] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:39:46.988] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:39:46.988] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:39:46.988] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:39:46.988] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-07e0c594-593d-4bbf-ad19-f12674942679
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:39:46.988] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:39:46.988] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:39:46.988] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:39:46.988] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:39:46.988] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:39:46.988] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:39:46.988] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:39:46.988] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:39:47.484] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:39:47.484] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:39:47.484] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:39:47.488] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:39:47.488] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:39:47.488] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:39:47.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:39:47.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:39:47.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:39:47.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 26
[2024-09-30 14:39:47.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:39:47.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:39:47.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:39:47.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:39:47.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:39:47.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:39:47.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:39:47.491] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:39:47.491] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:39:47.491] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:39:47.491] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:39:47.491] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:39:47.491] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:39:47.491] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:39:47.707] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:39:47.707] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:39:47.707] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:39:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:39:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:39:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:39:47.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:39:47.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:39:47.713] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:39:47.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:39:47.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:39:47.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:39:47.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:39:47.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:39:47.713] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:39:47.927] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:39:47.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:39:47.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:39:47.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:39:47.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:39:47.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:39:54.835] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:39:54.835] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:39:54.835] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2884362.81 ms
[2024-09-30 14:39:54.835] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.36 ms /    22 runs   (    0.20 ms per token,  5042.40 tokens per second)
[2024-09-30 14:39:54.835] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3133.42 ms /    24 tokens (  130.56 ms per token,     7.66 tokens per second)
[2024-09-30 14:39:54.835] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3768.50 ms /    21 runs   (  179.45 ms per token,     5.57 tokens per second)
[2024-09-30 14:39:54.835] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2888136.95 ms /    45 tokens
[2024-09-30 14:39:54.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:39:54.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 80
[2024-09-30 14:39:54.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:39:54.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:39:54.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:39:54.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:39:54.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:39:54.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:39:54.838] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 22
[2024-09-30 14:39:54.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 22
[2024-09-30 14:39:54.838] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:39:54.838] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:39:54.839] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:39:54.839] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:39:54.839] [info] rag_api_server in src/main.rs:517: response_body_size: 384
[2024-09-30 14:39:54.839] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:39:54.839] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:40:44.865] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:58680, local_addr: 0.0.0.0:8080
[2024-09-30 14:40:44.866] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:40:44.866] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:40:44.866] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:40:44.866] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:40:44.866] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-e2fa7c41-d907-45a1-929b-7e986ed06337
[2024-09-30 14:40:44.866] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:40:44.866] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:40:44.866] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:40:44.866] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:40:44.866] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:40:44.866] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:40:44.866] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:40:44.866] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:40:44.866] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:40:44.866] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:40:44.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:40:44.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:40:44.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:40:44.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:40:44.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:40:44.866] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:40:44.868] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:40:44.868] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:40:44.868] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:40:44.872] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:40:44.872] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:40:44.875] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:40:44.875] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:40:44.875] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:40:44.877] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:40:44.877] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:40:44.877] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:40:44.900] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:40:44.900] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2938095.15 ms
[2024-09-30 14:40:44.900] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:40:44.900] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.74 ms /     6 tokens (    3.62 ms per token,   276.04 tokens per second)
[2024-09-30 14:40:44.900] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:40:44.900] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2938094.56 ms /     7 tokens
[2024-09-30 14:40:44.901] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:40:44.901] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:40:44.909] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:40:44.909] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:40:44.909] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:40:44.909] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:40:44.909] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:40:44.909] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:40:44.909] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:40:44.909] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:40:44.909] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:40:44.909] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:40:44.910] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:40:44.926] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:40:44.926] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:40:44.927] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:40:44.927] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-e2fa7c41-d907-45a1-929b-7e986ed06337
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:40:44.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:40:44.927] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:40:44.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:40:44.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:40:44.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:40:44.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:40:44.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:40:44.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:40:45.371] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:40:45.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:40:45.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:40:45.374] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:40:45.374] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:40:45.374] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:40:45.377] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:40:45.377] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:40:45.377] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:40:45.377] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 22
[2024-09-30 14:40:45.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:40:45.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:40:45.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:40:45.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:40:45.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:40:45.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:40:45.378] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:40:45.378] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:40:45.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:40:45.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:40:45.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:40:45.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:40:45.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:40:45.378] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:40:45.550] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:40:45.551] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:40:45.551] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:40:45.552] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:40:45.552] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:40:45.552] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:40:45.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:40:45.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:40:45.556] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:40:45.556] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:40:45.556] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:40:45.556] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:40:45.556] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:40:45.556] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:40:45.556] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:40:45.709] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:40:45.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:40:45.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:40:45.711] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:40:45.711] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:40:45.711] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:40:53.746] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:40:53.746] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:40:53.746] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2942135.70 ms
[2024-09-30 14:40:53.746] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.85 ms /    27 runs   (    0.22 ms per token,  4615.38 tokens per second)
[2024-09-30 14:40:53.746] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3122.72 ms /    24 tokens (  130.11 ms per token,     7.69 tokens per second)
[2024-09-30 14:40:53.746] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4904.58 ms /    26 runs   (  188.64 ms per token,     5.30 tokens per second)
[2024-09-30 14:40:53.746] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2947046.95 ms /    50 tokens
[2024-09-30 14:40:53.748] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:40:53.748] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 109
[2024-09-30 14:40:53.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am Phi, an AI assistant designed to help you with various tasks. How can I assist you today?<|end|>
[2024-09-30 14:40:53.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:40:53.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am Phi, an AI assistant designed to help you with various tasks. How can I assist you today?
[2024-09-30 14:40:53.749] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:40:53.749] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:40:53.749] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:40:53.749] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 27
[2024-09-30 14:40:53.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 27
[2024-09-30 14:40:53.749] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:40:53.750] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:40:53.750] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:40:53.750] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:40:53.750] [info] rag_api_server in src/main.rs:517: response_body_size: 413
[2024-09-30 14:40:53.750] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:40:53.750] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:41:43.777] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:49550, local_addr: 0.0.0.0:8080
[2024-09-30 14:41:43.778] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:41:43.778] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:41:43.778] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:41:43.778] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:41:43.778] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-92f3ed0b-8b8f-46a2-8273-a57bc29032c2
[2024-09-30 14:41:43.779] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:41:43.779] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:41:43.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:41:43.779] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:41:43.779] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:41:43.779] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:41:43.779] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:41:43.779] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:41:43.779] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:41:43.779] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:41:43.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:41:43.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:41:43.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:41:43.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:41:43.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:41:43.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:41:43.781] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:41:43.781] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:41:43.781] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:41:43.783] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:41:43.783] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:41:43.786] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:41:43.786] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:41:43.786] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:41:43.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:41:43.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:41:43.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:41:43.822] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:41:43.822] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 2997017.35 ms
[2024-09-30 14:41:43.822] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:41:43.822] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      33.48 ms /     6 tokens (    5.58 ms per token,   179.23 tokens per second)
[2024-09-30 14:41:43.822] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:41:43.822] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 2997016.56 ms /     7 tokens
[2024-09-30 14:41:43.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:41:43.822] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:41:43.827] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:41:43.827] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:41:43.828] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:41:43.828] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:41:43.828] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:41:43.828] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:41:43.828] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:41:43.828] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:41:43.828] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:41:43.828] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:41:43.828] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:41:43.847] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:41:43.847] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:41:43.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:41:43.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:41:43.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:41:43.847] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:41:43.847] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:41:43.847] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:41:43.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-92f3ed0b-8b8f-46a2-8273-a57bc29032c2
[2024-09-30 14:41:43.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:41:43.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:41:43.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:41:43.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:41:43.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:41:43.848] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:41:43.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:41:43.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:41:43.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:41:43.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:41:43.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:41:43.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:41:44.389] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:41:44.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:41:44.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:41:44.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:41:44.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:41:44.393] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:41:44.396] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:41:44.396] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:41:44.396] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:41:44.396] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 27
[2024-09-30 14:41:44.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:41:44.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:41:44.396] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:41:44.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:41:44.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:41:44.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:41:44.397] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:41:44.397] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:41:44.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:41:44.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:41:44.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:41:44.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:41:44.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:41:44.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:41:44.591] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:41:44.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:41:44.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:41:44.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:41:44.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:41:44.592] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:41:44.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:41:44.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:41:44.595] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:41:44.595] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:41:44.595] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:41:44.595] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:41:44.595] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:41:44.595] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:41:44.595] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:41:44.798] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:41:44.798] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:41:44.798] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:41:44.799] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:41:44.799] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:41:44.799] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:41:51.924] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:41:51.924] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:41:51.924] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3001297.63 ms
[2024-09-30 14:41:51.924] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =      14.52 ms /    20 runs   (    0.73 ms per token,  1377.51 tokens per second)
[2024-09-30 14:41:51.924] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3197.07 ms /    24 tokens (  133.21 ms per token,     7.51 tokens per second)
[2024-09-30 14:41:51.924] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3912.55 ms /    19 runs   (  205.92 ms per token,     4.86 tokens per second)
[2024-09-30 14:41:51.924] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3005224.95 ms /    43 tokens
[2024-09-30 14:41:51.927] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:41:51.927] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 14:41:51.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:41:51.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:41:51.927] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:41:51.927] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:41:51.927] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:41:51.927] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:41:51.927] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:41:51.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:41:51.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:41:51.928] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:41:51.928] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:41:51.928] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:41:51.928] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 14:41:51.928] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:41:51.928] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:42:41.951] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:41532, local_addr: 0.0.0.0:8080
[2024-09-30 14:42:41.952] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:42:41.952] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:42:41.952] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:42:41.952] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:42:41.952] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-0e760a8a-be8b-4cdc-9689-375a0b2f2dbb
[2024-09-30 14:42:41.952] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:42:41.952] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:42:41.952] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:42:41.952] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:42:41.952] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:42:41.952] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:42:41.952] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:42:41.952] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:42:41.952] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:42:41.952] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:42:41.952] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:42:41.953] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:42:41.953] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:42:41.953] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:42:41.953] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:42:41.953] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:42:41.954] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:42:41.954] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:42:41.954] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:42:41.956] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:42:41.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:42:41.959] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:42:41.959] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:42:41.959] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:42:41.961] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:42:41.961] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:42:41.961] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:42:41.987] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:42:41.987] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3055182.05 ms
[2024-09-30 14:42:41.987] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:42:41.987] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      24.98 ms /     6 tokens (    4.16 ms per token,   240.21 tokens per second)
[2024-09-30 14:42:41.987] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:42:41.987] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3055181.56 ms /     7 tokens
[2024-09-30 14:42:41.987] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:42:41.987] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:42:41.992] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:42:41.992] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:42:41.992] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:42:41.993] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:42:41.993] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:42:41.993] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:42:41.993] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:42:41.993] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:42:41.993] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:42:41.993] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:42:41.993] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:42:42.013] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:42:42.013] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:42:42.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:42:42.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:42:42.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:42:42.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:42:42.014] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:42:42.014] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:42:42.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-0e760a8a-be8b-4cdc-9689-375a0b2f2dbb
[2024-09-30 14:42:42.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:42:42.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:42:42.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:42:42.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:42:42.014] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:42:42.014] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:42:42.014] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:42:42.014] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:42:42.014] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:42:42.014] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:42:42.014] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:42:42.014] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:42:42.473] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:42:42.473] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:42:42.473] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:42:42.475] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:42:42.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:42:42.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:42:42.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:42:42.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:42:42.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:42:42.479] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:42:42.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:42:42.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:42:42.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:42:42.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:42:42.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:42:42.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:42:42.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:42:42.479] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:42:42.479] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:42:42.479] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:42:42.479] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:42:42.479] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:42:42.479] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:42:42.479] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:42:42.649] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:42:42.650] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:42:42.650] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:42:42.651] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:42:42.651] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:42:42.651] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:42:42.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:42:42.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:42:42.654] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:42:42.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:42:42.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:42:42.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:42:42.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:42:42.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:42:42.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:42:42.803] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:42:42.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:42:42.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:42:42.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:42:42.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:42:42.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:42:48.931] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:42:48.931] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:42:48.931] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3059003.90 ms
[2024-09-30 14:42:48.931] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.05 ms /    21 runs   (    0.19 ms per token,  5190.31 tokens per second)
[2024-09-30 14:42:48.931] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2897.64 ms /    24 tokens (  120.74 ms per token,     8.28 tokens per second)
[2024-09-30 14:42:48.931] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3223.49 ms /    20 runs   (  161.17 ms per token,     6.20 tokens per second)
[2024-09-30 14:42:48.931] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3062231.95 ms /    44 tokens
[2024-09-30 14:42:48.933] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:42:48.933] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 92
[2024-09-30 14:42:48.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, a large language model developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:42:48.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:42:48.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, a large language model developed by Microsoft. How can I assist you today?
[2024-09-30 14:42:48.933] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:42:48.933] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:42:48.934] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:42:48.934] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:42:48.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:42:48.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:42:48.934] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:42:48.934] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:42:48.934] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:42:48.934] [info] rag_api_server in src/main.rs:517: response_body_size: 396
[2024-09-30 14:42:48.934] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:42:48.934] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:43:38.962] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:50432, local_addr: 0.0.0.0:8080
[2024-09-30 14:43:38.963] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:43:38.963] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:43:38.963] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:43:38.963] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:43:38.963] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-65cb8a1f-c4ff-407c-bf6a-08500130680b
[2024-09-30 14:43:38.963] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:43:38.963] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:43:38.963] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:43:38.963] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:43:38.963] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:43:38.963] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:43:38.963] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:43:38.963] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:43:38.963] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:43:38.963] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:43:38.963] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:43:38.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:43:38.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:43:38.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:43:38.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:43:38.964] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:43:38.965] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:43:38.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:43:38.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:43:38.969] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:43:38.969] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:43:38.971] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:43:38.971] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:43:38.971] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:43:38.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:43:38.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:43:38.973] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:43:38.994] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:43:38.994] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3112189.97 ms
[2024-09-30 14:43:38.994] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:43:38.995] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      20.80 ms /     6 tokens (    3.47 ms per token,   288.41 tokens per second)
[2024-09-30 14:43:38.995] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:43:38.995] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3112189.56 ms /     7 tokens
[2024-09-30 14:43:38.995] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:43:38.995] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:43:39.001] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:43:39.001] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:43:39.001] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:43:39.002] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:43:39.002] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:43:39.002] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:43:39.002] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:43:39.002] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:43:39.002] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:43:39.002] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:43:39.002] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:43:39.019] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:43:39.019] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:43:39.019] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:43:39.019] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-65cb8a1f-c4ff-407c-bf6a-08500130680b
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:43:39.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:43:39.019] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:43:39.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:43:39.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:43:39.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:43:39.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:43:39.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:43:39.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:43:39.451] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:43:39.451] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:43:39.451] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:43:39.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:43:39.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:43:39.454] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:43:39.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:43:39.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:43:39.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:43:39.457] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 14:43:39.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:43:39.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:43:39.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:43:39.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:43:39.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:43:39.457] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:43:39.458] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:43:39.458] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:43:39.458] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:43:39.458] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:43:39.458] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:43:39.458] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:43:39.458] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:43:39.458] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:43:39.617] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:43:39.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:43:39.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:43:39.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:43:39.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:43:39.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:43:39.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:43:39.621] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:43:39.621] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:43:39.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:43:39.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:43:39.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:43:39.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:43:39.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:43:39.621] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:43:39.779] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:43:39.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:43:39.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:43:39.781] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:43:39.781] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:43:39.781] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:43:45.167] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:43:45.167] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:43:45.167] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3115751.67 ms
[2024-09-30 14:43:45.167] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       3.73 ms /    19 runs   (    0.20 ms per token,  5096.57 tokens per second)
[2024-09-30 14:43:45.167] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2668.61 ms /    24 tokens (  111.19 ms per token,     8.99 tokens per second)
[2024-09-30 14:43:45.167] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2711.90 ms /    18 runs   (  150.66 ms per token,     6.64 tokens per second)
[2024-09-30 14:43:45.167] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3118467.95 ms /    42 tokens
[2024-09-30 14:43:45.171] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:43:45.171] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 90
[2024-09-30 14:43:45.171] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence developed by Microsoft. How can I assist you?<|end|>
[2024-09-30 14:43:45.171] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:43:45.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence developed by Microsoft. How can I assist you?
[2024-09-30 14:43:45.172] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:43:45.172] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:43:45.172] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:43:45.172] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 19
[2024-09-30 14:43:45.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 19
[2024-09-30 14:43:45.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:43:45.173] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:43:45.173] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:43:45.173] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:43:45.173] [info] rag_api_server in src/main.rs:517: response_body_size: 394
[2024-09-30 14:43:45.173] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:43:45.173] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:44:35.200] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:47652, local_addr: 0.0.0.0:8080
[2024-09-30 14:44:35.201] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:44:35.201] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:44:35.201] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:44:35.201] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:44:35.201] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-9721e4d1-230d-489d-982d-ec01b49f3203
[2024-09-30 14:44:35.201] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:44:35.201] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:44:35.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:44:35.201] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:44:35.201] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:44:35.201] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:44:35.201] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:44:35.201] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:44:35.201] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:44:35.202] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:44:35.202] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:44:35.202] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:44:35.202] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:44:35.202] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:44:35.202] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:44:35.202] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:44:35.203] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:44:35.203] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:44:35.203] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:44:35.205] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:44:35.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:44:35.208] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:44:35.208] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:44:35.208] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:44:35.210] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:44:35.210] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:44:35.210] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:44:35.234] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:44:35.234] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3168429.56 ms
[2024-09-30 14:44:35.234] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:44:35.234] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.62 ms /     6 tokens (    3.77 ms per token,   265.28 tokens per second)
[2024-09-30 14:44:35.234] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:44:35.234] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3168429.56 ms /     7 tokens
[2024-09-30 14:44:35.234] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:44:35.235] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:44:35.239] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:44:35.239] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:44:35.239] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:44:35.239] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:44:35.240] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:44:35.240] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:44:35.240] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:44:35.240] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:44:35.240] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:44:35.240] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:44:35.240] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:44:35.260] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:44:35.260] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:44:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:44:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:44:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:44:35.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:44:35.260] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:44:35.261] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:44:35.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-9721e4d1-230d-489d-982d-ec01b49f3203
[2024-09-30 14:44:35.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:44:35.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:44:35.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:44:35.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:44:35.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:44:35.261] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:44:35.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:44:35.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:44:35.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:44:35.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:44:35.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:44:35.261] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:44:35.706] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:44:35.706] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:44:35.706] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:44:35.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:44:35.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:44:35.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:44:35.713] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:44:35.713] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:44:35.713] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:44:35.713] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 19
[2024-09-30 14:44:35.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:44:35.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:44:35.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:44:35.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:44:35.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:44:35.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:44:35.714] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:44:35.714] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:44:35.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:44:35.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:44:35.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:44:35.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:44:35.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:44:35.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:44:35.935] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:44:35.935] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:44:35.935] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:44:35.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:44:35.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:44:35.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:44:35.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:44:35.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:44:35.939] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:44:35.939] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:44:35.939] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:44:35.939] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:44:35.939] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:44:35.939] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:44:35.939] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:44:36.148] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:44:36.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:44:36.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:44:36.149] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:44:36.149] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:44:36.149] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:44:43.544] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:44:43.544] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:44:43.544] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3172884.87 ms
[2024-09-30 14:44:43.544] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.97 ms /    20 runs   (    0.25 ms per token,  4024.14 tokens per second)
[2024-09-30 14:44:43.544] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3433.68 ms /    24 tokens (  143.07 ms per token,     6.99 tokens per second)
[2024-09-30 14:44:43.544] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3955.12 ms /    19 runs   (  208.16 ms per token,     4.80 tokens per second)
[2024-09-30 14:44:43.544] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3176845.95 ms /    43 tokens
[2024-09-30 14:44:43.547] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:44:43.547] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 14:44:43.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 14:44:43.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:44:43.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 14:44:43.547] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:44:43.547] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:44:43.547] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:44:43.548] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:44:43.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:44:43.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:44:43.548] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:44:43.548] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:44:43.548] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:44:43.548] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 14:44:43.548] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:44:43.548] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:45:33.575] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:43400, local_addr: 0.0.0.0:8080
[2024-09-30 14:45:33.576] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:45:33.576] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:45:33.576] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:45:33.576] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:45:33.576] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-b28a67ed-e227-4ca7-967d-d33820e8d6ba
[2024-09-30 14:45:33.576] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:45:33.577] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:45:33.577] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:45:33.577] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:45:33.577] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:45:33.577] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:45:33.577] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:45:33.577] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:45:33.577] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:45:33.577] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:45:33.577] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:45:33.577] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:45:33.577] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:45:33.577] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:45:33.577] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:45:33.577] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:45:33.578] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:45:33.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:45:33.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:45:33.580] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:45:33.580] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:45:33.584] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:45:33.584] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:45:33.584] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:45:33.586] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:45:33.586] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:45:33.586] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:45:33.610] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:45:33.610] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3226805.06 ms
[2024-09-30 14:45:33.610] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:45:33.610] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.54 ms /     6 tokens (    3.76 ms per token,   266.15 tokens per second)
[2024-09-30 14:45:33.610] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:45:33.610] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3226804.56 ms /     7 tokens
[2024-09-30 14:45:33.610] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:45:33.610] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:45:33.615] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:45:33.615] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:45:33.615] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:45:33.615] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:45:33.615] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:45:33.615] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:45:33.615] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:45:33.616] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:45:33.616] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:45:33.616] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:45:33.616] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:45:33.635] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:45:33.636] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:45:33.636] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:45:33.636] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-b28a67ed-e227-4ca7-967d-d33820e8d6ba
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:45:33.636] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:45:33.636] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:45:33.636] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:45:33.636] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:45:33.636] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:45:33.636] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:45:33.636] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:45:33.636] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:45:34.105] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:45:34.105] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:45:34.105] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:45:34.109] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:45:34.109] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:45:34.109] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:45:34.113] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:45:34.113] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:45:34.113] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:45:34.114] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 14:45:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:45:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:45:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:45:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:45:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:45:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:45:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:45:34.114] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:45:34.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:45:34.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:45:34.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:45:34.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:45:34.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:45:34.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:45:34.295] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:45:34.295] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:45:34.295] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:45:34.296] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:45:34.296] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:45:34.296] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:45:34.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:45:34.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:45:34.298] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:45:34.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:45:34.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:45:34.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:45:34.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:45:34.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:45:34.298] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:45:34.476] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:45:34.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:45:34.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:45:34.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:45:34.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:45:34.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:45:40.852] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:45:40.852] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:45:40.852] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3230448.38 ms
[2024-09-30 14:45:40.852] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.37 ms /    24 runs   (    0.22 ms per token,  4468.44 tokens per second)
[2024-09-30 14:45:40.852] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2668.61 ms /    24 tokens (  111.19 ms per token,     8.99 tokens per second)
[2024-09-30 14:45:40.852] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3698.77 ms /    23 runs   (  160.82 ms per token,     6.22 tokens per second)
[2024-09-30 14:45:40.852] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3234153.95 ms /    47 tokens
[2024-09-30 14:45:40.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:45:40.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 127
[2024-09-30 14:45:40.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence developed to provide assistance and engage in meaningful conversations with users.<|end|>
[2024-09-30 14:45:40.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:45:40.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence developed to provide assistance and engage in meaningful conversations with users.
[2024-09-30 14:45:40.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:45:40.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:45:40.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:45:40.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 14:45:40.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 24
[2024-09-30 14:45:40.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:45:40.856] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:45:40.856] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:45:40.856] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:45:40.856] [info] rag_api_server in src/main.rs:517: response_body_size: 431
[2024-09-30 14:45:40.856] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:45:40.856] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 14:46:30.879] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:35550, local_addr: 0.0.0.0:8080
[2024-09-30 14:46:30.879] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 14:46:30.880] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 14:46:30.880] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 14:46:30.880] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 14:46:30.880] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-c120bacf-49c6-45db-9b86-528a4f7d79c5
[2024-09-30 14:46:30.880] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 14:46:30.880] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 14:46:30.880] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 14:46:30.880] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 14:46:30.880] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:46:30.880] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:46:30.880] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 14:46:30.880] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:46:30.880] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:46:30.880] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 14:46:30.880] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:46:30.880] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:46:30.880] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:46:30.880] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:46:30.880] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:46:30.880] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:46:30.882] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:46:30.882] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:46:30.882] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:46:30.883] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:46:30.883] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:46:30.883] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:46:30.884] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 14:46:30.884] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 14:46:30.884] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 14:46:30.884] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 14:46:30.884] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:46:30.884] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 14:46:30.884] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:46:30.887] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 14:46:30.887] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 14:46:30.887] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 14:46:30.889] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 14:46:30.889] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 14:46:30.889] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:46:30.914] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:46:30.914] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3284109.11 ms
[2024-09-30 14:46:30.914] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:46:30.914] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.33 ms /     6 tokens (    3.89 ms per token,   257.18 tokens per second)
[2024-09-30 14:46:30.914] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 14:46:30.914] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3284108.56 ms /     7 tokens
[2024-09-30 14:46:30.914] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:46:30.914] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 14:46:30.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 14:46:30.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 14:46:30.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 14:46:30.919] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 14:46:30.919] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 14:46:30.919] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 14:46:30.919] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 14:46:30.919] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 14:46:30.919] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:46:30.919] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:46:30.919] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 14:46:30.936] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 14:46:30.936] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 14:46:30.937] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 14:46:30.937] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-c120bacf-49c6-45db-9b86-528a4f7d79c5
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 14:46:30.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:46:30.937] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:46:30.937] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:46:30.937] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:46:30.937] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:46:30.937] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:46:30.937] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:46:30.937] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:46:31.399] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:46:31.399] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:46:31.399] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:46:31.402] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:46:31.402] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:46:31.402] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:46:31.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:46:31.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:46:31.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:46:31.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 14:46:31.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 14:46:31.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 14:46:31.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 14:46:31.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 14:46:31.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 14:46:31.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 14:46:31.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 14:46:31.406] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:46:31.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:46:31.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:46:31.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:46:31.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:46:31.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:46:31.406] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:46:31.605] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:46:31.605] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:46:31.605] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:46:31.605] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:46:31.605] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:46:31.605] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:46:31.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 14:46:31.608] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:46:31.608] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 14:46:31.608] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 14:46:31.608] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 14:46:31.608] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 14:46:31.608] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 14:46:31.608] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 14:46:31.608] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 14:46:31.832] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 14:46:31.832] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 14:46:31.832] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 14:46:31.833] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 14:46:31.833] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 14:46:31.833] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 14:46:39.123] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 14:46:39.123] [info] [WASI-NN] llama.cpp: 
[2024-09-30 14:46:39.123] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 3288196.84 ms
[2024-09-30 14:46:39.123] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.98 ms /    23 runs   (    0.22 ms per token,  4623.12 tokens per second)
[2024-09-30 14:46:39.123] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3062.16 ms /    24 tokens (  127.59 ms per token,     7.84 tokens per second)
[2024-09-30 14:46:39.123] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4221.02 ms /    22 runs   (  191.86 ms per token,     5.21 tokens per second)
[2024-09-30 14:46:39.123] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 3292423.95 ms /    46 tokens
[2024-09-30 14:46:39.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:46:39.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 81
[2024-09-30 14:46:39.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I'm Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 14:46:39.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 14:46:39.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I'm Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 14:46:39.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 14:46:39.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 14:46:39.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 14:46:39.127] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 23
[2024-09-30 14:46:39.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 23
[2024-09-30 14:46:39.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 14:46:39.127] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 14:46:39.127] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 14:46:39.127] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 14:46:39.128] [info] rag_api_server in src/main.rs:517: response_body_size: 385
[2024-09-30 14:46:39.128] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 14:46:39.128] [info] rag_api_server in src/main.rs:521: response_is_success: true
